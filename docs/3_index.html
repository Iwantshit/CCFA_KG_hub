<html>
    <head>
        <meta charset="utf-8">
        
            <script>function neighbourhoodHighlight(params) {
  // console.log("in nieghbourhoodhighlight");
  allNodes = nodes.get({ returnType: "Object" });
  // originalNodes = JSON.parse(JSON.stringify(allNodes));
  // if something is selected:
  if (params.nodes.length > 0) {
    highlightActive = true;
    var i, j;
    var selectedNode = params.nodes[0];
    var degrees = 2;

    // mark all nodes as hard to read.
    for (let nodeId in allNodes) {
      // nodeColors[nodeId] = allNodes[nodeId].color;
      allNodes[nodeId].color = "rgba(200,200,200,0.5)";
      if (allNodes[nodeId].hiddenLabel === undefined) {
        allNodes[nodeId].hiddenLabel = allNodes[nodeId].label;
        allNodes[nodeId].label = undefined;
      }
    }
    var connectedNodes = network.getConnectedNodes(selectedNode);
    var allConnectedNodes = [];

    // get the second degree nodes
    for (i = 1; i < degrees; i++) {
      for (j = 0; j < connectedNodes.length; j++) {
        allConnectedNodes = allConnectedNodes.concat(
          network.getConnectedNodes(connectedNodes[j])
        );
      }
    }

    // all second degree nodes get a different color and their label back
    for (i = 0; i < allConnectedNodes.length; i++) {
      // allNodes[allConnectedNodes[i]].color = "pink";
      allNodes[allConnectedNodes[i]].color = "rgba(150,150,150,0.75)";
      if (allNodes[allConnectedNodes[i]].hiddenLabel !== undefined) {
        allNodes[allConnectedNodes[i]].label =
          allNodes[allConnectedNodes[i]].hiddenLabel;
        allNodes[allConnectedNodes[i]].hiddenLabel = undefined;
      }
    }

    // all first degree nodes get their own color and their label back
    for (i = 0; i < connectedNodes.length; i++) {
      // allNodes[connectedNodes[i]].color = undefined;
      allNodes[connectedNodes[i]].color = nodeColors[connectedNodes[i]];
      if (allNodes[connectedNodes[i]].hiddenLabel !== undefined) {
        allNodes[connectedNodes[i]].label =
          allNodes[connectedNodes[i]].hiddenLabel;
        allNodes[connectedNodes[i]].hiddenLabel = undefined;
      }
    }

    // the main node gets its own color and its label back.
    // allNodes[selectedNode].color = undefined;
    allNodes[selectedNode].color = nodeColors[selectedNode];
    if (allNodes[selectedNode].hiddenLabel !== undefined) {
      allNodes[selectedNode].label = allNodes[selectedNode].hiddenLabel;
      allNodes[selectedNode].hiddenLabel = undefined;
    }
  } else if (highlightActive === true) {
    // console.log("highlightActive was true");
    // reset all nodes
    for (let nodeId in allNodes) {
      // allNodes[nodeId].color = "purple";
      allNodes[nodeId].color = nodeColors[nodeId];
      // delete allNodes[nodeId].color;
      if (allNodes[nodeId].hiddenLabel !== undefined) {
        allNodes[nodeId].label = allNodes[nodeId].hiddenLabel;
        allNodes[nodeId].hiddenLabel = undefined;
      }
    }
    highlightActive = false;
  }

  // transform the object into an array
  var updateArray = [];
  if (params.nodes.length > 0) {
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        // console.log(allNodes[nodeId]);
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  } else {
    // console.log("Nothing was selected");
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        // console.log(allNodes[nodeId]);
        // allNodes[nodeId].color = {};
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  }
}

function filterHighlight(params) {
  allNodes = nodes.get({ returnType: "Object" });
  // if something is selected:
  if (params.nodes.length > 0) {
    filterActive = true;
    let selectedNodes = params.nodes;

    // hiding all nodes and saving the label
    for (let nodeId in allNodes) {
      allNodes[nodeId].hidden = true;
      if (allNodes[nodeId].savedLabel === undefined) {
        allNodes[nodeId].savedLabel = allNodes[nodeId].label;
        allNodes[nodeId].label = undefined;
      }
    }

    for (let i=0; i < selectedNodes.length; i++) {
      allNodes[selectedNodes[i]].hidden = false;
      if (allNodes[selectedNodes[i]].savedLabel !== undefined) {
        allNodes[selectedNodes[i]].label = allNodes[selectedNodes[i]].savedLabel;
        allNodes[selectedNodes[i]].savedLabel = undefined;
      }
    }

  } else if (filterActive === true) {
    // reset all nodes
    for (let nodeId in allNodes) {
      allNodes[nodeId].hidden = false;
      if (allNodes[nodeId].savedLabel !== undefined) {
        allNodes[nodeId].label = allNodes[nodeId].savedLabel;
        allNodes[nodeId].savedLabel = undefined;
      }
    }
    filterActive = false;
  }

  // transform the object into an array
  var updateArray = [];
  if (params.nodes.length > 0) {
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  } else {
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  }
}

function selectNode(nodes) {
  network.selectNodes(nodes);
  neighbourhoodHighlight({ nodes: nodes });
  return nodes;
}

function selectNodes(nodes) {
  network.selectNodes(nodes);
  filterHighlight({nodes: nodes});
  return nodes;
}

function highlightFilter(filter) {
  let selectedNodes = []
  let selectedProp = filter['property']
  if (filter['item'] === 'node') {
    let allNodes = nodes.get({ returnType: "Object" });
    for (let nodeId in allNodes) {
      if (allNodes[nodeId][selectedProp] && filter['value'].includes((allNodes[nodeId][selectedProp]).toString())) {
        selectedNodes.push(nodeId)
      }
    }
  }
  else if (filter['item'] === 'edge'){
    let allEdges = edges.get({returnType: 'object'});
    // check if the selected property exists for selected edge and select the nodes connected to the edge
    for (let edge in allEdges) {
      if (allEdges[edge][selectedProp] && filter['value'].includes((allEdges[edge][selectedProp]).toString())) {
        selectedNodes.push(allEdges[edge]['from'])
        selectedNodes.push(allEdges[edge]['to'])
      }
    }
  }
  selectNodes(selectedNodes)
}</script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
            
            
            
            
            
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tom-select/2.0.0-rc.4/css/tom-select.min.css" integrity="sha512-43fHB3GLgZfz8QXl1RPQ8O66oIgv3po9cJ5erMt1c4QISq9dYb195T3vr5ImnJPXuVroKcGBPXBFKETW8jrPNQ==" crossorigin="anonymous" referrerpolicy="no-referrer" />
                <script src="https://cdnjs.cloudflare.com/ajax/libs/tom-select/2.0.0-rc.4/js/tom-select.complete.js" integrity="sha512-jeF9CfnvzDiw9G9xiksVjxR2lib44Gnovvkv+3CgCG6NXCD4gqlA5nDAVW5WjpA+i+/zKsUWV5xNEbW1X/HH0Q==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            

        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 900px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             
             #loadingBar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width: 100%;
                 height: 900px;
                 background-color:rgba(200,200,200,0.8);
                 -webkit-transition: all 0.5s ease;
                 -moz-transition: all 0.5s ease;
                 -ms-transition: all 0.5s ease;
                 -o-transition: all 0.5s ease;
                 transition: all 0.5s ease;
                 opacity:1;
             }

             #bar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width:20px;
                 height:20px;
                 margin:auto auto auto auto;
                 border-radius:11px;
                 border:2px solid rgba(30,30,30,0.05);
                 background: rgb(0, 173, 246); /* Old browsers */
                 box-shadow: 2px 0px 4px rgba(0,0,0,0.4);
             }

             #border {
                 position:absolute;
                 top:10px;
                 left:10px;
                 width:500px;
                 height:23px;
                 margin:auto auto auto auto;
                 box-shadow: 0px 0px 4px rgba(0,0,0,0.2);
                 border-radius:10px;
             }

             #text {
                 position:absolute;
                 top:8px;
                 left:530px;
                 width:30px;
                 height:50px;
                 margin:auto auto auto auto;
                 font-size:22px;
                 color: #000000;
             }

             div.outerBorder {
                 position:relative;
                 top:400px;
                 width:600px;
                 height:44px;
                 margin:auto auto auto auto;
                 border:8px solid rgba(0,0,0,0.1);
                 background: rgb(252,252,252); /* Old browsers */
                 background: -moz-linear-gradient(top,  rgba(252,252,252,1) 0%, rgba(237,237,237,1) 100%); /* FF3.6+ */
                 background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,rgba(252,252,252,1)), color-stop(100%,rgba(237,237,237,1))); /* Chrome,Safari4+ */
                 background: -webkit-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Chrome10+,Safari5.1+ */
                 background: -o-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Opera 11.10+ */
                 background: -ms-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* IE10+ */
                 background: linear-gradient(to bottom,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* W3C */
                 filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#fcfcfc', endColorstr='#ededed',GradientType=0 ); /* IE6-9 */
                 border-radius:72px;
                 box-shadow: 0px 0px 10px rgba(0,0,0,0.2);
             }
             

             
             #config {
                 float: left;
                 width: 400px;
                 height: 600px;
             }
             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
                <div id="select-menu" class="card-header">
                    <div class="row no-gutters">
                        <div class="col-10 pb-2">
                            <select
                            class="form-select"
                            aria-label="Default select example"
                            onchange="selectNode([value]);"
                            id="select-node"
                            placeholder="Select node..."
                            >
                                <option selected>Select a Node by ID</option>
                                
                                    <option value="2d videos">2d videos</option>
                                
                                    <option value="agent">agent</option>
                                
                                    <option value="colour">colour</option>
                                
                                    <option value="features">features</option>
                                
                                    <option value="frames">frames</option>
                                
                                    <option value="graphs">graphs</option>
                                
                                    <option value="model">model</option>
                                
                                    <option value="position">position</option>
                                
                                    <option value="transformer">transformer</option>
                                
                                    <option value="type">type</option>
                                
                                    <option value="baby intuitions benchmark">baby intuitions benchmark</option>
                                
                                    <option value="a. bulling">a. bulling</option>
                                
                                    <option value="irene">irene</option>
                                
                                    <option value="m. bortoletto">m. bortoletto</option>
                                
                                    <option value="abductive commonsense reasoning">abductive commonsense reasoning</option>
                                
                                    <option value="learning representations">learning representations</option>
                                
                                    <option value="ablation studies">ablation studies</option>
                                
                                    <option value="relational graphs">relational graphs</option>
                                
                                    <option value="actions">actions</option>
                                
                                    <option value="context embedding">context embedding</option>
                                
                                    <option value="context encoder">context encoder</option>
                                
                                    <option value="ctx token">ctx token</option>
                                
                                    <option value="embedding vector">embedding vector</option>
                                
                                    <option value="encoded states">encoded states</option>
                                
                                    <option value="grid-world environment">grid-world environment</option>
                                
                                    <option value="objects">objects</option>
                                
                                    <option value="state encoder">state encoder</option>
                                
                                    <option value="sub-tasks">sub-tasks</option>
                                
                                    <option value="tasks">tasks</option>
                                
                                    <option value="trials">trials</option>
                                
                                    <option value="adjacent and aligned">adjacent and aligned</option>
                                
                                    <option value="agent's past trajectories">agent's past trajectories</option>
                                
                                    <option value="feature fusion module">feature fusion module</option>
                                
                                    <option value="gnn">gnn</option>
                                
                                    <option value="local directional relations">local directional relations</option>
                                
                                    <option value="node embeddings">node embeddings</option>
                                
                                    <option value="node features">node features</option>
                                
                                    <option value="prediction net">prediction net</option>
                                
                                    <option value="remote directional relations">remote directional relations</option>
                                
                                    <option value="type, position, color, shape">type, position, color, shape</option>
                                
                                    <option value="architecture">architecture</option>
                                
                                    <option value="bayesian theory of mind">bayesian theory of mind</option>
                                
                                    <option value="bib">bib</option>
                                
                                    <option value="bipack">bipack</option>
                                
                                    <option value="common-sense reasoning tasks">common-sense reasoning tasks</option>
                                
                                    <option value="context embeddings">context embeddings</option>
                                
                                    <option value="core psychological reasoning">core psychological reasoning</option>
                                
                                    <option value="edges">edges</option>
                                
                                    <option value="efficient action">efficient action</option>
                                
                                    <option value="evaluation">evaluation</option>
                                
                                    <option value="gandhi et al. 2021">gandhi et al. 2021</option>
                                
                                    <option value="graph">graph</option>
                                
                                    <option value="ground truth">ground truth</option>
                                
                                    <option value="hbtom">hbtom</option>
                                
                                    <option value="instrumental action">instrumental action</option>
                                
                                    <option value="lack of accessible and well-maintained benchmarks">lack of accessible and well-maintained benchmarks</option>
                                
                                    <option value="message passing">message passing</option>
                                
                                    <option value="normalised between [-1, 1]">normalised between [-1, 1]</option>
                                
                                    <option value="normalised between [0, 1]">normalised between [0, 1]</option>
                                
                                    <option value="one-hot vectors">one-hot vectors</option>
                                
                                    <option value="shape">shape</option>
                                
                                    <option value="spatial relations">spatial relations</option>
                                
                                    <option value="states">states</option>
                                
                                    <option value="training and evaluation on the baby intuitions benchmark">training and evaluation on the baby intuitions benchmark</option>
                                
                                    <option value="trajectories">trajectories</option>
                                
                                    <option value="transformer context encoder">transformer context encoder</option>
                                
                                    <option value="violation of expectation paradigm">violation of expectation paradigm</option>
                                
                                    <option value="weights">weights</option>
                                
                                    <option value="agent and world state encoding">agent and world state encoding</option>
                                
                                    <option value="rational agents">rational agents</option>
                                
                                    <option value="training tasks">training tasks</option>
                                
                                    <option value="graphsage">graphsage</option>
                                
                                    <option value="agent tests">agent tests</option>
                                
                                    <option value="benchmarks">benchmarks</option>
                                
                                    <option value="research">research</option>
                                
                                    <option value="agent-blocked instrumental action">agent-blocked instrumental action</option>
                                
                                    <option value="evaluation trials">evaluation trials</option>
                                
                                    <option value="frame">frame</option>
                                
                                    <option value="graph generation">graph generation</option>
                                
                                    <option value="instrumental blocking barrier">instrumental blocking barrier</option>
                                
                                    <option value="json file">json file</option>
                                
                                    <option value="navigation preference">navigation preference</option>
                                
                                    <option value="single-object multiple-agent">single-object multiple-agent</option>
                                
                                    <option value="training trials">training trials</option>
                                
                                    <option value="agents">agents</option>
                                
                                    <option value="existing models">existing models</option>
                                
                                    <option value="graph neural network (gnn)">graph neural network (gnn)</option>
                                
                                    <option value="intuitive reasoning network (irene)">intuitive reasoning network (irene)</option>
                                
                                    <option value="reasoning tasks">reasoning tasks</option>
                                
                                    <option value="video transformer">video transformer</option>
                                
                                    <option value="vt">vt</option>
                                
                                    <option value="agents’ goals">agents’ goals</option>
                                
                                    <option value="agents’ preferences">agents’ preferences</option>
                                
                                    <option value="ai agents">ai agents</option>
                                
                                    <option value="ai systems">ai systems</option>
                                
                                    <option value="developmental disorders">developmental disorders</option>
                                
                                    <option value="human-machine collaboration">human-machine collaboration</option>
                                
                                    <option value="humans">humans</option>
                                
                                    <option value="infants">infants</option>
                                
                                    <option value="machine common-sense reasoning">machine common-sense reasoning</option>
                                
                                    <option value="robotics">robotics</option>
                                
                                    <option value="ai capabilities">ai capabilities</option>
                                
                                    <option value="baby intuitions benchmark (bib)">baby intuitions benchmark (bib)</option>
                                
                                    <option value="test trials">test trials</option>
                                
                                    <option value="voe paradigm">voe paradigm</option>
                                
                                    <option value="ann-sophia">ann-sophia</option>
                                
                                    <option value="denisov">denisov</option>
                                
                                    <option value="ekta sood">ekta sood</option>
                                
                                    <option value="hsiu-yu yang">hsiu-yu yang</option>
                                
                                    <option value="manuel mager">manuel mager</option>
                                
                                    <option value="attention heads">attention heads</option>
                                
                                    <option value="gandhi et al.">gandhi et al.</option>
                                
                                    <option value="mlp policy">mlp policy</option>
                                
                                    <option value="results">results</option>
                                
                                    <option value="transformer encoder">transformer encoder</option>
                                
                                    <option value="attention is all you need">attention is all you need</option>
                                
                                    <option value="advances in neural information processing systems, 30">advances in neural information processing systems, 30</option>
                                
                                    <option value="discerning the goals, preferences, and actions of others">discerning the goals, preferences, and actions of others</option>
                                
                                    <option value="initial models">initial models</option>
                                
                                    <option value="machine theory of mind network">machine theory of mind network</option>
                                
                                    <option value="meta-learning problem">meta-learning problem</option>
                                
                                    <option value="models’ ability to predict future actions">models’ ability to predict future actions</option>
                                
                                    <option value="most recent model (hein and diepold 2022, vt)">most recent model (hein and diepold 2022, vt)</option>
                                
                                    <option value="observer model">observer model</option>
                                
                                    <option value="training and evaluation tasks">training and evaluation tasks</option>
                                
                                    <option value="video">video</option>
                                
                                    <option value="bail-largeon 1987">bail-largeon 1987</option>
                                
                                    <option value="common-sense reasoning">common-sense reasoning</option>
                                
                                    <option value="expected events or situations">expected events or situations</option>
                                
                                    <option value="intuitive physics">intuitive physics</option>
                                
                                    <option value="intuitive psychology">intuitive psychology</option>
                                
                                    <option value="piloto et al.">piloto et al.</option>
                                
                                    <option value="riochet et al.">riochet et al.</option>
                                
                                    <option value="shu et al.">shu et al.</option>
                                
                                    <option value="bc-mlp">bc-mlp</option>
                                
                                    <option value="video-rnn">video-rnn</option>
                                
                                    <option value="bc-rnn">bc-rnn</option>
                                
                                    <option value="behaviour">behaviour</option>
                                
                                    <option value="mindblindness">mindblindness</option>
                                
                                    <option value="benchmarking progress to infant-level physical reasoning in ai">benchmarking progress to infant-level physical reasoning in ai</option>
                                
                                    <option value="transactions on machine learning research">transactions on machine learning research</option>
                                
                                    <option value="collaborative agents">collaborative agents</option>
                                
                                    <option value="evaluating language processing">evaluating language processing</option>
                                
                                    <option value="general ability of ai systems to reason about unexpected events or situations">general ability of ai systems to reason about unexpected events or situations</option>
                                
                                    <option value="infants expect other agents to have goals, preferences and engage in instrumental actions">infants expect other agents to have goals, preferences and engage in instrumental actions</option>
                                
                                    <option value="intelligent">intelligent</option>
                                
                                    <option value="models’ ability to reason about other agents">models’ ability to reason about other agents</option>
                                
                                    <option value="visual scene understanding">visual scene understanding</option>
                                
                                    <option value="bhagavatula, c.">bhagavatula, c.</option>
                                
                                    <option value="choi, y.">choi, y.</option>
                                
                                    <option value="expectedness">expectedness</option>
                                
                                    <option value="hein et al.">hein et al.</option>
                                
                                    <option value="improving performance">improving performance</option>
                                
                                    <option value="logistic regression classifiers">logistic regression classifiers</option>
                                
                                    <option value="lstm">lstm</option>
                                
                                    <option value="state-action pairs">state-action pairs</option>
                                
                                    <option value="test trial">test trial</option>
                                
                                    <option value="video frames">video frames</option>
                                
                                    <option value="bib task">bib task</option>
                                
                                    <option value="blocking barriers">blocking barriers</option>
                                
                                    <option value="blocking obstacles">blocking obstacles</option>
                                
                                    <option value="clevrer: collision events for video representation and reasoning">clevrer: collision events for video representation and reasoning</option>
                                
                                    <option value="international conference on learning representations">international conference on learning representations</option>
                                
                                    <option value="cognitron">cognitron</option>
                                
                                    <option value="self-organizing multi-layered neural network">self-organizing multi-layered neural network</option>
                                
                                    <option value="dasgupta et al.">dasgupta et al.</option>
                                
                                    <option value="weihs et al.">weihs et al.</option>
                                
                                    <option value="developmental cognitive science">developmental cognitive science</option>
                                
                                    <option value="common-sense reasoning benchmarks">common-sense reasoning benchmarks</option>
                                
                                    <option value="performance">performance</option>
                                
                                    <option value="commonsense psychology">commonsense psychology</option>
                                
                                    <option value="human infants and machines">human infants and machines</option>
                                
                                    <option value="comparing intuitions">comparing intuitions</option>
                                
                                    <option value="agents’ goals, preferences and actions in human infants">agents’ goals, preferences and actions in human infants</option>
                                
                                    <option value="complementary basic concepts">complementary basic concepts</option>
                                
                                    <option value="complex tasks">complex tasks</option>
                                
                                    <option value="models">models</option>
                                
                                    <option value="constantin ruhdorfer">constantin ruhdorfer</option>
                                
                                    <option value="m¨uller">m¨uller</option>
                                
                                    <option value="edges eij">edges eij</option>
                                
                                    <option value="encoder">encoder</option>
                                
                                    <option value="graph-sage layers">graph-sage layers</option>
                                
                                    <option value="lstm aggregation">lstm aggregation</option>
                                
                                    <option value="mlp">mlp</option>
                                
                                    <option value="relationships">relationships</option>
                                
                                    <option value="rnn">rnn</option>
                                
                                    <option value="spatial relationships">spatial relationships</option>
                                
                                    <option value="state embedding">state embedding</option>
                                
                                    <option value="test frame graph">test frame graph</option>
                                
                                    <option value="test state">test state</option>
                                
                                    <option value="trial representations">trial representations</option>
                                
                                    <option value="deep network learning">deep network learning</option>
                                
                                    <option value="exponential linear units (elus)">exponential linear units (elus)</option>
                                
                                    <option value="designing and creating new benchmarks">designing and creating new benchmarks</option>
                                
                                    <option value="general neural common-sense reasoners">general neural common-sense reasoners</option>
                                
                                    <option value="deutsche forschungsgemeinschaft">deutsche forschungsgemeinschaft</option>
                                
                                    <option value="l. shi">l. shi</option>
                                
                                    <option value="remote relations">remote relations</option>
                                
                                    <option value="efficiency irrational agent">efficiency irrational agent</option>
                                
                                    <option value="efficient">efficient</option>
                                
                                    <option value="efficient action task">efficient action task</option>
                                
                                    <option value="errors">errors</option>
                                
                                    <option value="evaluation performance">evaluation performance</option>
                                
                                    <option value="evaluation set">evaluation set</option>
                                
                                    <option value="preference task">preference task</option>
                                
                                    <option value="evaluation tasks">evaluation tasks</option>
                                
                                    <option value="voe accuracy scores">voe accuracy scores</option>
                                
                                    <option value="generalisation">generalisation</option>
                                
                                    <option value="infants’ responses">infants’ responses</option>
                                
                                    <option value="neural network">neural network</option>
                                
                                    <option value="theory of mind network">theory of mind network</option>
                                
                                    <option value="existing models (gandhi et al. 2021; hein and diepold 2022)">existing models (gandhi et al. 2021; hein and diepold 2022)</option>
                                
                                    <option value="expected outcome">expected outcome</option>
                                
                                    <option value="expected trials">expected trials</option>
                                
                                    <option value="familiarisation">familiarisation</option>
                                
                                    <option value="fast and accurate deep network learning">fast and accurate deep network learning</option>
                                
                                    <option value="exponential linear units">exponential linear units</option>
                                
                                    <option value="graphsage layers">graphsage layers</option>
                                
                                    <option value="hidden dimension 96">hidden dimension 96</option>
                                
                                    <option value="hidden dimensions 256, 128 and 256">hidden dimensions 256, 128 and 256</option>
                                
                                    <option value="mean prediction error">mean prediction error</option>
                                
                                    <option value="output dimension two">output dimension two</option>
                                
                                    <option value="stack of six layers">stack of six layers</option>
                                
                                    <option value="vt model">vt model</option>
                                
                                    <option value="final scores">final scores</option>
                                
                                    <option value="gandhi et al. (2021)">gandhi et al. (2021)</option>
                                
                                    <option value="gcn">gcn</option>
                                
                                    <option value="graph convolutional networks">graph convolutional networks</option>
                                
                                    <option value="relational data">relational data</option>
                                
                                    <option value="graph neural network">graph neural network</option>
                                
                                    <option value="graph relations">graph relations</option>
                                
                                    <option value="obstacles">obstacles</option>
                                
                                    <option value="preferences">preferences</option>
                                
                                    <option value="hein, a.">hein, a.</option>
                                
                                    <option value="diepold, k.">diepold, k.</option>
                                
                                    <option value="hellaswag: can a machine really finish your sentence?">hellaswag: can a machine really finish your sentence?</option>
                                
                                    <option value="proceedings of the 57th annual meeting of the association for computational linguistics">proceedings of the 57th annual meeting of the association for computational linguistics</option>
                                
                                    <option value="hendrycks, d.">hendrycks, d.</option>
                                
                                    <option value="gimpel, k.">gimpel, k.</option>
                                
                                    <option value="heuristics">heuristics</option>
                                
                                    <option value="huang, l.">huang, l.</option>
                                
                                    <option value="le bras, r.">le bras, r.</option>
                                
                                    <option value="human-robot collaboration">human-robot collaboration</option>
                                
                                    <option value="commonsense reasoning">commonsense reasoning</option>
                                
                                    <option value="inaccessible goal">inaccessible goal</option>
                                
                                    <option value="inaccessible goal task">inaccessible goal task</option>
                                
                                    <option value="inconsequential barrier">inconsequential barrier</option>
                                
                                    <option value="multi-agent">multi-agent</option>
                                
                                    <option value="no barrier">no barrier</option>
                                
                                    <option value="p">p</option>
                                
                                    <option value="s">s</option>
                                
                                    <option value="instrumental action task">instrumental action task</option>
                                
                                    <option value="m">m</option>
                                
                                    <option value="multi-agent task">multi-agent task</option>
                                
                                    <option value="inductive representation learning">inductive representation learning</option>
                                
                                    <option value="large graphs">large graphs</option>
                                
                                    <option value="our model’s expectations">our model’s expectations</option>
                                
                                    <option value="infants' looking times">infants' looking times</option>
                                
                                    <option value="stojni´c et al. (2023)">stojni´c et al. (2023)</option>
                                
                                    <option value="input">input</option>
                                
                                    <option value="remote">remote</option>
                                
                                    <option value="instrumental action tasks">instrumental action tasks</option>
                                
                                    <option value="intentional stance">intentional stance</option>
                                
                                    <option value="12 months of age">12 months of age</option>
                                
                                    <option value="international conference on machine learning">international conference on machine learning</option>
                                
                                    <option value="video transformer network">video transformer network</option>
                                
                                    <option value="machine theory of mind">machine theory of mind</option>
                                
                                    <option value="rectified linear units improve restricted boltzmann machines">rectified linear units improve restricted boltzmann machines</option>
                                
                                    <option value="intphys: a framework and benchmark for visual intuitive physics reasoning">intphys: a framework and benchmark for visual intuitive physics reasoning</option>
                                
                                    <option value="ieee transactions on pattern analysis and machine intelligence">ieee transactions on pattern analysis and machine intelligence</option>
                                
                                    <option value="intuitions about support in 4.5-month-old infants">intuitions about support in 4.5-month-old infants</option>
                                
                                    <option value="cognition, 47(2): 121–148">cognition, 47(2): 121–148</option>
                                
                                    <option value="coarse probabilistic object representations">coarse probabilistic object representations</option>
                                
                                    <option value="intuitive physics learning in a deep-learning model inspired by developmental psychology">intuitive physics learning in a deep-learning model inspired by developmental psychology</option>
                                
                                    <option value="nature human behaviour">nature human behaviour</option>
                                
                                    <option value="neural model">neural model</option>
                                
                                    <option value="irrational agent">irrational agent</option>
                                
                                    <option value="irrational agents">irrational agents</option>
                                
                                    <option value="knowledge">knowledge</option>
                                
                                    <option value="knowledge gained during training">knowledge gained during training</option>
                                
                                    <option value="local">local</option>
                                
                                    <option value="lstm limitations">lstm limitations</option>
                                
                                    <option value="novel model">novel model</option>
                                
                                    <option value="path control">path control</option>
                                
                                    <option value="path control and time control sub-tasks">path control and time control sub-tasks</option>
                                
                                    <option value="performance scores">performance scores</option>
                                
                                    <option value="preference">preference</option>
                                
                                    <option value="reasoning performance">reasoning performance</option>
                                
                                    <option value="reward hacking">reward hacking</option>
                                
                                    <option value="state and context representations">state and context representations</option>
                                
                                    <option value="t-tests">t-tests</option>
                                
                                    <option value="table 2">table 2</option>
                                
                                    <option value="the state of the art">the state of the art</option>
                                
                                    <option value="time control">time control</option>
                                
                                    <option value="unseen evaluation tasks">unseen evaluation tasks</option>
                                
                                    <option value="jiang, m.">jiang, m.</option>
                                
                                    <option value="rockt¨aschel, t.">rockt¨aschel, t.</option>
                                
                                    <option value="jiang, z.">jiang, z.</option>
                                
                                    <option value="minervini, p.">minervini, p.</option>
                                
                                    <option value="jones, l.">jones, l.</option>
                                
                                    <option value="gomez, a. n.">gomez, a. n.</option>
                                
                                    <option value="kaiser, ł.">kaiser, ł.</option>
                                
                                    <option value="polosukhin, i.">polosukhin, i.</option>
                                
                                    <option value="learning in graph domains">learning in graph domains</option>
                                
                                    <option value="model for learning">model for learning</option>
                                
                                    <option value="local relations">local relations</option>
                                
                                    <option value="nodes">nodes</option>
                                
                                    <option value="resulting model">resulting model</option>
                                
                                    <option value="worse scores">worse scores</option>
                                
                                    <option value="modeling violation-of-expectation">modeling violation-of-expectation</option>
                                
                                    <option value="physical reasoning">physical reasoning</option>
                                
                                    <option value="mota, t.">mota, t.</option>
                                
                                    <option value="sridharan, m.">sridharan, m.</option>
                                
                                    <option value="mse">mse</option>
                                
                                    <option value="prediction error">prediction error</option>
                                
                                    <option value="natural language processing">natural language processing</option>
                                
                                    <option value="international joint conference on natural language processing">international joint conference on natural language processing</option>
                                
                                    <option value="observer">observer</option>
                                
                                    <option value="parmar, n.">parmar, n.</option>
                                
                                    <option value="uszkoreit, j.">uszkoreit, j.</option>
                                
                                    <option value="piqa">piqa</option>
                                
                                    <option value="physical commonsense in natural language">physical commonsense in natural language</option>
                                
                                    <option value="training set">training set</option>
                                
                                    <option value="unexpected trials">unexpected trials</option>
                                
                                    <option value="probing physics knowledge using tools from developmental psychology">probing physics knowledge using tools from developmental psychology</option>
                                
                                    <option value="arxiv preprint arxiv:1804.01128">arxiv preprint arxiv:1804.01128</option>
                                
                                    <option value="simulation">simulation</option>
                                
                                    <option value="physical scene understanding">physical scene understanding</option>
                                
                                    <option value="social iqa: commonsense reasoning about social interactions">social iqa: commonsense reasoning about social interactions</option>
                                
                                    <option value="proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language pro">proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language pro</option>
                                
                                    <option value="solving the baby intuitions benchmark with a hierarchically bayesian theory of mind">solving the baby intuitions benchmark with a hierarchically bayesian theory of mind</option>
                                
                                    <option value="robotics: science and systems workshop on social intelligence in humans and robots">robotics: science and systems workshop on social intelligence in humans and robots</option>
                                
                                    <option value="teachers’ extent of the use of particular task types in mathematics">teachers’ extent of the use of particular task types in mathematics</option>
                                
                                    <option value="mathematics education research group of australasia">mathematics education research group of australasia</option>
                                
                                    <option value="vaswani, a.">vaswani, a.</option>
                                
                                    <option value="shazeer, n.">shazeer, n.</option>
                                
                                    <option value="ieee/cvf international conference on computer vision">ieee/cvf international conference on computer vision</option>
                                
                                    <option value="proceedings of the">proceedings of the</option>
                                
                                    <option value="winogrande: an adversarial winograd schema challenge at scale">winogrande: an adversarial winograd schema challenge at scale</option>
                                
                                    <option value="communications of the acm">communications of the acm</option>
                                
                            </select>
                        </div>
                        <div class="col-2 pb-2">
                            <button type="button" class="btn btn-primary btn-block" onclick="neighbourhoodHighlight({nodes: []});">Reset Selection</button>
                        </div>
                    </div>
                </div>
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
            <div id="loadingBar">
              <div class="outerBorder">
                <div id="text">0%</div>
                <div id="border">
                  <div id="bar"></div>
                </div>
              </div>
            </div>
        
        
            <div id="config"></div>
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              
                  new TomSelect("#select-node",{
                      create: false,
                      sortField: {
                          field: "text",
                          direction: "asc"
                      }
                  });
              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"group": 2, "id": "2d videos", "label": "2d videos", "shape": "dot", "size": 10}, {"group": 2, "id": "agent", "label": "agent", "shape": "dot", "size": 52}, {"group": 2, "id": "colour", "label": "colour", "shape": "dot", "size": 29}, {"group": 2, "id": "features", "label": "features", "shape": "dot", "size": 29}, {"group": 2, "id": "frames", "label": "frames", "shape": "dot", "size": 29}, {"group": 2, "id": "graphs", "label": "graphs", "shape": "dot", "size": 33}, {"group": 2, "id": "model", "label": "model", "shape": "dot", "size": 57}, {"group": 2, "id": "position", "label": "position", "shape": "dot", "size": 29}, {"group": 2, "id": "transformer", "label": "transformer", "shape": "dot", "size": 44}, {"group": 2, "id": "type", "label": "type", "shape": "dot", "size": 29}, {"group": 2, "id": "baby intuitions benchmark", "label": "baby intuitions benchmark", "shape": "dot", "size": 12}, {"group": 2, "id": "a. bulling", "label": "a. bulling", "shape": "dot", "size": 2}, {"group": 2, "id": "irene", "label": "irene", "shape": "dot", "size": 129}, {"group": 2, "id": "m. bortoletto", "label": "m. bortoletto", "shape": "dot", "size": 2}, {"group": 3, "id": "abductive commonsense reasoning", "label": "abductive commonsense reasoning", "shape": "dot", "size": 1}, {"group": 3, "id": "learning representations", "label": "learning representations", "shape": "dot", "size": 1}, {"group": 2, "id": "ablation studies", "label": "ablation studies", "shape": "dot", "size": 2}, {"group": 2, "id": "relational graphs", "label": "relational graphs", "shape": "dot", "size": 18}, {"group": 2, "id": "actions", "label": "actions", "shape": "dot", "size": 13}, {"group": 2, "id": "context embedding", "label": "context embedding", "shape": "dot", "size": 31}, {"group": 2, "id": "context encoder", "label": "context encoder", "shape": "dot", "size": 22}, {"group": 2, "id": "ctx token", "label": "ctx token", "shape": "dot", "size": 12}, {"group": 2, "id": "embedding vector", "label": "embedding vector", "shape": "dot", "size": 16}, {"group": 2, "id": "encoded states", "label": "encoded states", "shape": "dot", "size": 21}, {"group": 2, "id": "grid-world environment", "label": "grid-world environment", "shape": "dot", "size": 28}, {"group": 2, "id": "objects", "label": "objects", "shape": "dot", "size": 28}, {"group": 2, "id": "state encoder", "label": "state encoder", "shape": "dot", "size": 48}, {"group": 2, "id": "sub-tasks", "label": "sub-tasks", "shape": "dot", "size": 20}, {"group": 2, "id": "tasks", "label": "tasks", "shape": "dot", "size": 35}, {"group": 2, "id": "trials", "label": "trials", "shape": "dot", "size": 20}, {"group": 2, "id": "adjacent and aligned", "label": "adjacent and aligned", "shape": "dot", "size": 10}, {"group": 2, "id": "agent\u0027s past trajectories", "label": "agent\u0027s past trajectories", "shape": "dot", "size": 5}, {"group": 2, "id": "feature fusion module", "label": "feature fusion module", "shape": "dot", "size": 35}, {"group": 2, "id": "gnn", "label": "gnn", "shape": "dot", "size": 33}, {"group": 2, "id": "local directional relations", "label": "local directional relations", "shape": "dot", "size": 24}, {"group": 2, "id": "node embeddings", "label": "node embeddings", "shape": "dot", "size": 8}, {"group": 2, "id": "node features", "label": "node features", "shape": "dot", "size": 10}, {"group": 2, "id": "prediction net", "label": "prediction net", "shape": "dot", "size": 28}, {"group": 2, "id": "remote directional relations", "label": "remote directional relations", "shape": "dot", "size": 10}, {"group": 2, "id": "type, position, color, shape", "label": "type, position, color, shape", "shape": "dot", "size": 5}, {"group": 2, "id": "architecture", "label": "architecture", "shape": "dot", "size": 9}, {"group": 2, "id": "bayesian theory of mind", "label": "bayesian theory of mind", "shape": "dot", "size": 4}, {"group": 2, "id": "bib", "label": "bib", "shape": "dot", "size": 35}, {"group": 2, "id": "bipack", "label": "bipack", "shape": "dot", "size": 4}, {"group": 2, "id": "common-sense reasoning tasks", "label": "common-sense reasoning tasks", "shape": "dot", "size": 8}, {"group": 2, "id": "context embeddings", "label": "context embeddings", "shape": "dot", "size": 9}, {"group": 2, "id": "core psychological reasoning", "label": "core psychological reasoning", "shape": "dot", "size": 1}, {"group": 2, "id": "edges", "label": "edges", "shape": "dot", "size": 17}, {"group": 2, "id": "efficient action", "label": "efficient action", "shape": "dot", "size": 11}, {"group": 2, "id": "evaluation", "label": "evaluation", "shape": "dot", "size": 10}, {"group": 2, "id": "gandhi et al. 2021", "label": "gandhi et al. 2021", "shape": "dot", "size": 10}, {"group": 2, "id": "graph", "label": "graph", "shape": "dot", "size": 9}, {"group": 2, "id": "ground truth", "label": "ground truth", "shape": "dot", "size": 9}, {"group": 2, "id": "hbtom", "label": "hbtom", "shape": "dot", "size": 14}, {"group": 2, "id": "instrumental action", "label": "instrumental action", "shape": "dot", "size": 18}, {"group": 2, "id": "lack of accessible and well-maintained benchmarks", "label": "lack of accessible and well-maintained benchmarks", "shape": "dot", "size": 7}, {"group": 2, "id": "message passing", "label": "message passing", "shape": "dot", "size": 12}, {"group": 2, "id": "normalised between [-1, 1]", "label": "normalised between [-1, 1]", "shape": "dot", "size": 9}, {"group": 2, "id": "normalised between [0, 1]", "label": "normalised between [0, 1]", "shape": "dot", "size": 9}, {"group": 2, "id": "one-hot vectors", "label": "one-hot vectors", "shape": "dot", "size": 9}, {"group": 2, "id": "shape", "label": "shape", "shape": "dot", "size": 9}, {"group": 2, "id": "spatial relations", "label": "spatial relations", "shape": "dot", "size": 10}, {"group": 2, "id": "states", "label": "states", "shape": "dot", "size": 9}, {"group": 2, "id": "training and evaluation on the baby intuitions benchmark", "label": "training and evaluation on the baby intuitions benchmark", "shape": "dot", "size": 10}, {"group": 2, "id": "trajectories", "label": "trajectories", "shape": "dot", "size": 8}, {"group": 2, "id": "transformer context encoder", "label": "transformer context encoder", "shape": "dot", "size": 9}, {"group": 2, "id": "violation of expectation paradigm", "label": "violation of expectation paradigm", "shape": "dot", "size": 19}, {"group": 2, "id": "weights", "label": "weights", "shape": "dot", "size": 9}, {"group": 2, "id": "agent and world state encoding", "label": "agent and world state encoding", "shape": "dot", "size": 4}, {"group": 2, "id": "rational agents", "label": "rational agents", "shape": "dot", "size": 17}, {"group": 2, "id": "training tasks", "label": "training tasks", "shape": "dot", "size": 30}, {"group": 2, "id": "graphsage", "label": "graphsage", "shape": "dot", "size": 19}, {"group": 2, "id": "agent tests", "label": "agent tests", "shape": "dot", "size": 3}, {"group": 2, "id": "benchmarks", "label": "benchmarks", "shape": "dot", "size": 13}, {"group": 2, "id": "research", "label": "research", "shape": "dot", "size": 13}, {"group": 5, "id": "agent-blocked instrumental action", "label": "agent-blocked instrumental action", "shape": "dot", "size": 8}, {"group": 5, "id": "evaluation trials", "label": "evaluation trials", "shape": "dot", "size": 4}, {"group": 5, "id": "frame", "label": "frame", "shape": "dot", "size": 4}, {"group": 5, "id": "graph generation", "label": "graph generation", "shape": "dot", "size": 8}, {"group": 5, "id": "instrumental blocking barrier", "label": "instrumental blocking barrier", "shape": "dot", "size": 10}, {"group": 5, "id": "json file", "label": "json file", "shape": "dot", "size": 4}, {"group": 5, "id": "navigation preference", "label": "navigation preference", "shape": "dot", "size": 4}, {"group": 5, "id": "single-object multiple-agent", "label": "single-object multiple-agent", "shape": "dot", "size": 8}, {"group": 5, "id": "training trials", "label": "training trials", "shape": "dot", "size": 4}, {"group": 2, "id": "agents", "label": "agents", "shape": "dot", "size": 9}, {"group": 2, "id": "existing models", "label": "existing models", "shape": "dot", "size": 17}, {"group": 2, "id": "graph neural network (gnn)", "label": "graph neural network (gnn)", "shape": "dot", "size": 17}, {"group": 2, "id": "intuitive reasoning network (irene)", "label": "intuitive reasoning network (irene)", "shape": "dot", "size": 23}, {"group": 2, "id": "reasoning tasks", "label": "reasoning tasks", "shape": "dot", "size": 17}, {"group": 2, "id": "video transformer", "label": "video transformer", "shape": "dot", "size": 17}, {"group": 2, "id": "vt", "label": "vt", "shape": "dot", "size": 28}, {"group": 2, "id": "agents\u2019 goals", "label": "agents\u2019 goals", "shape": "dot", "size": 1}, {"group": 2, "id": "agents\u2019 preferences", "label": "agents\u2019 preferences", "shape": "dot", "size": 8}, {"group": 7, "id": "ai agents", "label": "ai agents", "shape": "dot", "size": 9}, {"group": 7, "id": "ai systems", "label": "ai systems", "shape": "dot", "size": 2}, {"group": 7, "id": "developmental disorders", "label": "developmental disorders", "shape": "dot", "size": 2}, {"group": 7, "id": "human-machine collaboration", "label": "human-machine collaboration", "shape": "dot", "size": 1}, {"group": 7, "id": "humans", "label": "humans", "shape": "dot", "size": 1}, {"group": 7, "id": "infants", "label": "infants", "shape": "dot", "size": 4}, {"group": 7, "id": "machine common-sense reasoning", "label": "machine common-sense reasoning", "shape": "dot", "size": 2}, {"group": 7, "id": "robotics", "label": "robotics", "shape": "dot", "size": 1}, {"group": 2, "id": "ai capabilities", "label": "ai capabilities", "shape": "dot", "size": 3}, {"group": 2, "id": "baby intuitions benchmark (bib)", "label": "baby intuitions benchmark (bib)", "shape": "dot", "size": 13}, {"group": 2, "id": "test trials", "label": "test trials", "shape": "dot", "size": 15}, {"group": 2, "id": "voe paradigm", "label": "voe paradigm", "shape": "dot", "size": 19}, {"group": 8, "id": "ann-sophia", "label": "ann-sophia", "shape": "dot", "size": 4}, {"group": 8, "id": "denisov", "label": "denisov", "shape": "dot", "size": 6}, {"group": 8, "id": "ekta sood", "label": "ekta sood", "shape": "dot", "size": 6}, {"group": 8, "id": "hsiu-yu yang", "label": "hsiu-yu yang", "shape": "dot", "size": 6}, {"group": 8, "id": "manuel mager", "label": "manuel mager", "shape": "dot", "size": 6}, {"group": 2, "id": "attention heads", "label": "attention heads", "shape": "dot", "size": 8}, {"group": 2, "id": "gandhi et al.", "label": "gandhi et al.", "shape": "dot", "size": 45}, {"group": 2, "id": "mlp policy", "label": "mlp policy", "shape": "dot", "size": 25}, {"group": 2, "id": "results", "label": "results", "shape": "dot", "size": 35}, {"group": 2, "id": "transformer encoder", "label": "transformer encoder", "shape": "dot", "size": 28}, {"group": 4, "id": "attention is all you need", "label": "attention is all you need", "shape": "dot", "size": 1}, {"group": 4, "id": "advances in neural information processing systems, 30", "label": "advances in neural information processing systems, 30", "shape": "dot", "size": 1}, {"group": 2, "id": "discerning the goals, preferences, and actions of others", "label": "discerning the goals, preferences, and actions of others", "shape": "dot", "size": 1}, {"group": 2, "id": "initial models", "label": "initial models", "shape": "dot", "size": 4}, {"group": 2, "id": "machine theory of mind network", "label": "machine theory of mind network", "shape": "dot", "size": 4}, {"group": 2, "id": "meta-learning problem", "label": "meta-learning problem", "shape": "dot", "size": 3}, {"group": 2, "id": "models\u2019 ability to predict future actions", "label": "models\u2019 ability to predict future actions", "shape": "dot", "size": 3}, {"group": 2, "id": "most recent model (hein and diepold 2022, vt)", "label": "most recent model (hein and diepold 2022, vt)", "shape": "dot", "size": 4}, {"group": 2, "id": "observer model", "label": "observer model", "shape": "dot", "size": 6}, {"group": 2, "id": "training and evaluation tasks", "label": "training and evaluation tasks", "shape": "dot", "size": 3}, {"group": 2, "id": "video", "label": "video", "shape": "dot", "size": 4}, {"group": 2, "id": "bail-largeon 1987", "label": "bail-largeon 1987", "shape": "dot", "size": 9}, {"group": 2, "id": "common-sense reasoning", "label": "common-sense reasoning", "shape": "dot", "size": 14}, {"group": 2, "id": "expected events or situations", "label": "expected events or situations", "shape": "dot", "size": 11}, {"group": 2, "id": "intuitive physics", "label": "intuitive physics", "shape": "dot", "size": 15}, {"group": 2, "id": "intuitive psychology", "label": "intuitive psychology", "shape": "dot", "size": 13}, {"group": 2, "id": "piloto et al.", "label": "piloto et al.", "shape": "dot", "size": 11}, {"group": 2, "id": "riochet et al.", "label": "riochet et al.", "shape": "dot", "size": 11}, {"group": 2, "id": "shu et al.", "label": "shu et al.", "shape": "dot", "size": 11}, {"group": 2, "id": "bc-mlp", "label": "bc-mlp", "shape": "dot", "size": 10}, {"group": 2, "id": "video-rnn", "label": "video-rnn", "shape": "dot", "size": 10}, {"group": 2, "id": "bc-rnn", "label": "bc-rnn", "shape": "dot", "size": 8}, {"group": 10, "id": "behaviour", "label": "behaviour", "shape": "dot", "size": 1}, {"group": 10, "id": "mindblindness", "label": "mindblindness", "shape": "dot", "size": 1}, {"group": 11, "id": "benchmarking progress to infant-level physical reasoning in ai", "label": "benchmarking progress to infant-level physical reasoning in ai", "shape": "dot", "size": 1}, {"group": 11, "id": "transactions on machine learning research", "label": "transactions on machine learning research", "shape": "dot", "size": 1}, {"group": 2, "id": "collaborative agents", "label": "collaborative agents", "shape": "dot", "size": 4}, {"group": 2, "id": "evaluating language processing", "label": "evaluating language processing", "shape": "dot", "size": 3}, {"group": 2, "id": "general ability of ai systems to reason about unexpected events or situations", "label": "general ability of ai systems to reason about unexpected events or situations", "shape": "dot", "size": 3}, {"group": 2, "id": "infants expect other agents to have goals, preferences and engage in instrumental actions", "label": "infants expect other agents to have goals, preferences and engage in instrumental actions", "shape": "dot", "size": 3}, {"group": 2, "id": "intelligent", "label": "intelligent", "shape": "dot", "size": 4}, {"group": 2, "id": "models\u2019 ability to reason about other agents", "label": "models\u2019 ability to reason about other agents", "shape": "dot", "size": 3}, {"group": 2, "id": "visual scene understanding", "label": "visual scene understanding", "shape": "dot", "size": 3}, {"group": 12, "id": "bhagavatula, c.", "label": "bhagavatula, c.", "shape": "dot", "size": 1}, {"group": 12, "id": "choi, y.", "label": "choi, y.", "shape": "dot", "size": 1}, {"group": 2, "id": "expectedness", "label": "expectedness", "shape": "dot", "size": 4}, {"group": 2, "id": "hein et al.", "label": "hein et al.", "shape": "dot", "size": 13}, {"group": 2, "id": "improving performance", "label": "improving performance", "shape": "dot", "size": 3}, {"group": 2, "id": "logistic regression classifiers", "label": "logistic regression classifiers", "shape": "dot", "size": 4}, {"group": 2, "id": "lstm", "label": "lstm", "shape": "dot", "size": 14}, {"group": 2, "id": "state-action pairs", "label": "state-action pairs", "shape": "dot", "size": 5}, {"group": 2, "id": "test trial", "label": "test trial", "shape": "dot", "size": 5}, {"group": 2, "id": "video frames", "label": "video frames", "shape": "dot", "size": 11}, {"group": 2, "id": "bib task", "label": "bib task", "shape": "dot", "size": 3}, {"group": 2, "id": "blocking barriers", "label": "blocking barriers", "shape": "dot", "size": 4}, {"group": 2, "id": "blocking obstacles", "label": "blocking obstacles", "shape": "dot", "size": 2}, {"group": 13, "id": "clevrer: collision events for video representation and reasoning", "label": "clevrer: collision events for video representation and reasoning", "shape": "dot", "size": 1}, {"group": 13, "id": "international conference on learning representations", "label": "international conference on learning representations", "shape": "dot", "size": 1}, {"group": 15, "id": "cognitron", "label": "cognitron", "shape": "dot", "size": 1}, {"group": 15, "id": "self-organizing multi-layered neural network", "label": "self-organizing multi-layered neural network", "shape": "dot", "size": 1}, {"group": 2, "id": "dasgupta et al.", "label": "dasgupta et al.", "shape": "dot", "size": 8}, {"group": 2, "id": "weihs et al.", "label": "weihs et al.", "shape": "dot", "size": 8}, {"group": 2, "id": "developmental cognitive science", "label": "developmental cognitive science", "shape": "dot", "size": 3}, {"group": 2, "id": "common-sense reasoning benchmarks", "label": "common-sense reasoning benchmarks", "shape": "dot", "size": 3}, {"group": 2, "id": "performance", "label": "performance", "shape": "dot", "size": 24}, {"group": 16, "id": "commonsense psychology", "label": "commonsense psychology", "shape": "dot", "size": 1}, {"group": 16, "id": "human infants and machines", "label": "human infants and machines", "shape": "dot", "size": 1}, {"group": 6, "id": "comparing intuitions", "label": "comparing intuitions", "shape": "dot", "size": 1}, {"group": 6, "id": "agents\u2019 goals, preferences and actions in human infants", "label": "agents\u2019 goals, preferences and actions in human infants", "shape": "dot", "size": 1}, {"group": 2, "id": "complementary basic concepts", "label": "complementary basic concepts", "shape": "dot", "size": 2}, {"group": 2, "id": "complex tasks", "label": "complex tasks", "shape": "dot", "size": 2}, {"group": 2, "id": "models", "label": "models", "shape": "dot", "size": 7}, {"group": 8, "id": "constantin ruhdorfer", "label": "constantin ruhdorfer", "shape": "dot", "size": 5}, {"group": 8, "id": "m\u00a8uller", "label": "m\u00a8uller", "shape": "dot", "size": 5}, {"group": 2, "id": "edges eij", "label": "edges eij", "shape": "dot", "size": 3}, {"group": 2, "id": "encoder", "label": "encoder", "shape": "dot", "size": 2}, {"group": 2, "id": "graph-sage layers", "label": "graph-sage layers", "shape": "dot", "size": 3}, {"group": 2, "id": "lstm aggregation", "label": "lstm aggregation", "shape": "dot", "size": 3}, {"group": 2, "id": "mlp", "label": "mlp", "shape": "dot", "size": 3}, {"group": 2, "id": "relationships", "label": "relationships", "shape": "dot", "size": 3}, {"group": 2, "id": "rnn", "label": "rnn", "shape": "dot", "size": 3}, {"group": 2, "id": "spatial relationships", "label": "spatial relationships", "shape": "dot", "size": 3}, {"group": 2, "id": "state embedding", "label": "state embedding", "shape": "dot", "size": 9}, {"group": 2, "id": "test frame graph", "label": "test frame graph", "shape": "dot", "size": 3}, {"group": 2, "id": "test state", "label": "test state", "shape": "dot", "size": 2}, {"group": 2, "id": "trial representations", "label": "trial representations", "shape": "dot", "size": 3}, {"group": 19, "id": "deep network learning", "label": "deep network learning", "shape": "dot", "size": 1}, {"group": 19, "id": "exponential linear units (elus)", "label": "exponential linear units (elus)", "shape": "dot", "size": 1}, {"group": 2, "id": "designing and creating new benchmarks", "label": "designing and creating new benchmarks", "shape": "dot", "size": 3}, {"group": 2, "id": "general neural common-sense reasoners", "label": "general neural common-sense reasoners", "shape": "dot", "size": 3}, {"group": 2, "id": "deutsche forschungsgemeinschaft", "label": "deutsche forschungsgemeinschaft", "shape": "dot", "size": 2}, {"group": 2, "id": "l. shi", "label": "l. shi", "shape": "dot", "size": 2}, {"group": 2, "id": "remote relations", "label": "remote relations", "shape": "dot", "size": 14}, {"group": 2, "id": "efficiency irrational agent", "label": "efficiency irrational agent", "shape": "dot", "size": 2}, {"group": 2, "id": "efficient", "label": "efficient", "shape": "dot", "size": 3}, {"group": 2, "id": "efficient action task", "label": "efficient action task", "shape": "dot", "size": 2}, {"group": 2, "id": "errors", "label": "errors", "shape": "dot", "size": 8}, {"group": 2, "id": "evaluation performance", "label": "evaluation performance", "shape": "dot", "size": 4}, {"group": 2, "id": "evaluation set", "label": "evaluation set", "shape": "dot", "size": 2}, {"group": 2, "id": "preference task", "label": "preference task", "shape": "dot", "size": 25}, {"group": 2, "id": "evaluation tasks", "label": "evaluation tasks", "shape": "dot", "size": 9}, {"group": 2, "id": "voe accuracy scores", "label": "voe accuracy scores", "shape": "dot", "size": 9}, {"group": 2, "id": "generalisation", "label": "generalisation", "shape": "dot", "size": 9}, {"group": 2, "id": "infants\u2019 responses", "label": "infants\u2019 responses", "shape": "dot", "size": 8}, {"group": 2, "id": "neural network", "label": "neural network", "shape": "dot", "size": 8}, {"group": 2, "id": "theory of mind network", "label": "theory of mind network", "shape": "dot", "size": 8}, {"group": 2, "id": "existing models (gandhi et al. 2021; hein and diepold 2022)", "label": "existing models (gandhi et al. 2021; hein and diepold 2022)", "shape": "dot", "size": 1}, {"group": 2, "id": "expected outcome", "label": "expected outcome", "shape": "dot", "size": 3}, {"group": 2, "id": "expected trials", "label": "expected trials", "shape": "dot", "size": 2}, {"group": 2, "id": "familiarisation", "label": "familiarisation", "shape": "dot", "size": 3}, {"group": 21, "id": "fast and accurate deep network learning", "label": "fast and accurate deep network learning", "shape": "dot", "size": 1}, {"group": 21, "id": "exponential linear units", "label": "exponential linear units", "shape": "dot", "size": 1}, {"group": 2, "id": "graphsage layers", "label": "graphsage layers", "shape": "dot", "size": 11}, {"group": 2, "id": "hidden dimension 96", "label": "hidden dimension 96", "shape": "dot", "size": 8}, {"group": 2, "id": "hidden dimensions 256, 128 and 256", "label": "hidden dimensions 256, 128 and 256", "shape": "dot", "size": 8}, {"group": 2, "id": "mean prediction error", "label": "mean prediction error", "shape": "dot", "size": 9}, {"group": 2, "id": "output dimension two", "label": "output dimension two", "shape": "dot", "size": 8}, {"group": 2, "id": "stack of six layers", "label": "stack of six layers", "shape": "dot", "size": 8}, {"group": 2, "id": "vt model", "label": "vt model", "shape": "dot", "size": 9}, {"group": 2, "id": "final scores", "label": "final scores", "shape": "dot", "size": 3}, {"group": 2, "id": "gandhi et al. (2021)", "label": "gandhi et al. (2021)", "shape": "dot", "size": 3}, {"group": 2, "id": "gcn", "label": "gcn", "shape": "dot", "size": 4}, {"group": 24, "id": "graph convolutional networks", "label": "graph convolutional networks", "shape": "dot", "size": 1}, {"group": 24, "id": "relational data", "label": "relational data", "shape": "dot", "size": 1}, {"group": 2, "id": "graph neural network", "label": "graph neural network", "shape": "dot", "size": 2}, {"group": 2, "id": "graph relations", "label": "graph relations", "shape": "dot", "size": 2}, {"group": 2, "id": "obstacles", "label": "obstacles", "shape": "dot", "size": 5}, {"group": 2, "id": "preferences", "label": "preferences", "shape": "dot", "size": 3}, {"group": 20, "id": "hein, a.", "label": "hein, a.", "shape": "dot", "size": 1}, {"group": 20, "id": "diepold, k.", "label": "diepold, k.", "shape": "dot", "size": 1}, {"group": 25, "id": "hellaswag: can a machine really finish your sentence?", "label": "hellaswag: can a machine really finish your sentence?", "shape": "dot", "size": 1}, {"group": 25, "id": "proceedings of the 57th annual meeting of the association for computational linguistics", "label": "proceedings of the 57th annual meeting of the association for computational linguistics", "shape": "dot", "size": 1}, {"group": 22, "id": "hendrycks, d.", "label": "hendrycks, d.", "shape": "dot", "size": 1}, {"group": 22, "id": "gimpel, k.", "label": "gimpel, k.", "shape": "dot", "size": 1}, {"group": 2, "id": "heuristics", "label": "heuristics", "shape": "dot", "size": 2}, {"group": 26, "id": "huang, l.", "label": "huang, l.", "shape": "dot", "size": 1}, {"group": 26, "id": "le bras, r.", "label": "le bras, r.", "shape": "dot", "size": 1}, {"group": 17, "id": "human-robot collaboration", "label": "human-robot collaboration", "shape": "dot", "size": 1}, {"group": 17, "id": "commonsense reasoning", "label": "commonsense reasoning", "shape": "dot", "size": 1}, {"group": 2, "id": "inaccessible goal", "label": "inaccessible goal", "shape": "dot", "size": 8}, {"group": 2, "id": "inaccessible goal task", "label": "inaccessible goal task", "shape": "dot", "size": 12}, {"group": 2, "id": "inconsequential barrier", "label": "inconsequential barrier", "shape": "dot", "size": 12}, {"group": 2, "id": "multi-agent", "label": "multi-agent", "shape": "dot", "size": 13}, {"group": 2, "id": "no barrier", "label": "no barrier", "shape": "dot", "size": 12}, {"group": 2, "id": "p", "label": "p", "shape": "dot", "size": 12}, {"group": 2, "id": "s", "label": "s", "shape": "dot", "size": 12}, {"group": 2, "id": "instrumental action task", "label": "instrumental action task", "shape": "dot", "size": 8}, {"group": 2, "id": "m", "label": "m", "shape": "dot", "size": 9}, {"group": 2, "id": "multi-agent task", "label": "multi-agent task", "shape": "dot", "size": 9}, {"group": 29, "id": "inductive representation learning", "label": "inductive representation learning", "shape": "dot", "size": 1}, {"group": 29, "id": "large graphs", "label": "large graphs", "shape": "dot", "size": 1}, {"group": 7, "id": "our model\u2019s expectations", "label": "our model\u2019s expectations", "shape": "dot", "size": 2}, {"group": 2, "id": "infants\u0027 looking times", "label": "infants\u0027 looking times", "shape": "dot", "size": 2}, {"group": 2, "id": "stojni\u00b4c et al. (2023)", "label": "stojni\u00b4c et al. (2023)", "shape": "dot", "size": 2}, {"group": 2, "id": "input", "label": "input", "shape": "dot", "size": 4}, {"group": 2, "id": "remote", "label": "remote", "shape": "dot", "size": 6}, {"group": 2, "id": "instrumental action tasks", "label": "instrumental action tasks", "shape": "dot", "size": 2}, {"group": 1, "id": "intentional stance", "label": "intentional stance", "shape": "dot", "size": 1}, {"group": 1, "id": "12 months of age", "label": "12 months of age", "shape": "dot", "size": 1}, {"group": 28, "id": "international conference on machine learning", "label": "international conference on machine learning", "shape": "dot", "size": 3}, {"group": 28, "id": "video transformer network", "label": "video transformer network", "shape": "dot", "size": 3}, {"group": 28, "id": "machine theory of mind", "label": "machine theory of mind", "shape": "dot", "size": 1}, {"group": 28, "id": "rectified linear units improve restricted boltzmann machines", "label": "rectified linear units improve restricted boltzmann machines", "shape": "dot", "size": 1}, {"group": 27, "id": "intphys: a framework and benchmark for visual intuitive physics reasoning", "label": "intphys: a framework and benchmark for visual intuitive physics reasoning", "shape": "dot", "size": 1}, {"group": 27, "id": "ieee transactions on pattern analysis and machine intelligence", "label": "ieee transactions on pattern analysis and machine intelligence", "shape": "dot", "size": 1}, {"group": 14, "id": "intuitions about support in 4.5-month-old infants", "label": "intuitions about support in 4.5-month-old infants", "shape": "dot", "size": 1}, {"group": 14, "id": "cognition, 47(2): 121\u2013148", "label": "cognition, 47(2): 121\u2013148", "shape": "dot", "size": 1}, {"group": 2, "id": "coarse probabilistic object representations", "label": "coarse probabilistic object representations", "shape": "dot", "size": 1}, {"group": 31, "id": "intuitive physics learning in a deep-learning model inspired by developmental psychology", "label": "intuitive physics learning in a deep-learning model inspired by developmental psychology", "shape": "dot", "size": 1}, {"group": 31, "id": "nature human behaviour", "label": "nature human behaviour", "shape": "dot", "size": 1}, {"group": 2, "id": "neural model", "label": "neural model", "shape": "dot", "size": 2}, {"group": 2, "id": "irrational agent", "label": "irrational agent", "shape": "dot", "size": 3}, {"group": 2, "id": "irrational agents", "label": "irrational agents", "shape": "dot", "size": 3}, {"group": 2, "id": "knowledge", "label": "knowledge", "shape": "dot", "size": 2}, {"group": 2, "id": "knowledge gained during training", "label": "knowledge gained during training", "shape": "dot", "size": 3}, {"group": 2, "id": "local", "label": "local", "shape": "dot", "size": 5}, {"group": 2, "id": "lstm limitations", "label": "lstm limitations", "shape": "dot", "size": 4}, {"group": 2, "id": "novel model", "label": "novel model", "shape": "dot", "size": 3}, {"group": 2, "id": "path control", "label": "path control", "shape": "dot", "size": 2}, {"group": 2, "id": "path control and time control sub-tasks", "label": "path control and time control sub-tasks", "shape": "dot", "size": 2}, {"group": 2, "id": "performance scores", "label": "performance scores", "shape": "dot", "size": 2}, {"group": 2, "id": "preference", "label": "preference", "shape": "dot", "size": 2}, {"group": 2, "id": "reasoning performance", "label": "reasoning performance", "shape": "dot", "size": 3}, {"group": 2, "id": "reward hacking", "label": "reward hacking", "shape": "dot", "size": 2}, {"group": 2, "id": "state and context representations", "label": "state and context representations", "shape": "dot", "size": 3}, {"group": 2, "id": "t-tests", "label": "t-tests", "shape": "dot", "size": 3}, {"group": 2, "id": "table 2", "label": "table 2", "shape": "dot", "size": 3}, {"group": 2, "id": "the state of the art", "label": "the state of the art", "shape": "dot", "size": 1}, {"group": 2, "id": "time control", "label": "time control", "shape": "dot", "size": 2}, {"group": 2, "id": "unseen evaluation tasks", "label": "unseen evaluation tasks", "shape": "dot", "size": 3}, {"group": 32, "id": "jiang, m.", "label": "jiang, m.", "shape": "dot", "size": 1}, {"group": 32, "id": "rockt\u00a8aschel, t.", "label": "rockt\u00a8aschel, t.", "shape": "dot", "size": 1}, {"group": 33, "id": "jiang, z.", "label": "jiang, z.", "shape": "dot", "size": 1}, {"group": 33, "id": "minervini, p.", "label": "minervini, p.", "shape": "dot", "size": 1}, {"group": 23, "id": "jones, l.", "label": "jones, l.", "shape": "dot", "size": 1}, {"group": 23, "id": "gomez, a. n.", "label": "gomez, a. n.", "shape": "dot", "size": 1}, {"group": 34, "id": "kaiser, \u0142.", "label": "kaiser, \u0142.", "shape": "dot", "size": 1}, {"group": 34, "id": "polosukhin, i.", "label": "polosukhin, i.", "shape": "dot", "size": 1}, {"group": 35, "id": "learning in graph domains", "label": "learning in graph domains", "shape": "dot", "size": 1}, {"group": 35, "id": "model for learning", "label": "model for learning", "shape": "dot", "size": 1}, {"group": 2, "id": "local relations", "label": "local relations", "shape": "dot", "size": 4}, {"group": 2, "id": "nodes", "label": "nodes", "shape": "dot", "size": 4}, {"group": 2, "id": "resulting model", "label": "resulting model", "shape": "dot", "size": 4}, {"group": 2, "id": "worse scores", "label": "worse scores", "shape": "dot", "size": 4}, {"group": 37, "id": "modeling violation-of-expectation", "label": "modeling violation-of-expectation", "shape": "dot", "size": 1}, {"group": 37, "id": "physical reasoning", "label": "physical reasoning", "shape": "dot", "size": 1}, {"group": 38, "id": "mota, t.", "label": "mota, t.", "shape": "dot", "size": 1}, {"group": 38, "id": "sridharan, m.", "label": "sridharan, m.", "shape": "dot", "size": 1}, {"group": 2, "id": "mse", "label": "mse", "shape": "dot", "size": 3}, {"group": 2, "id": "prediction error", "label": "prediction error", "shape": "dot", "size": 3}, {"group": 30, "id": "natural language processing", "label": "natural language processing", "shape": "dot", "size": 1}, {"group": 30, "id": "international joint conference on natural language processing", "label": "international joint conference on natural language processing", "shape": "dot", "size": 1}, {"group": 2, "id": "observer", "label": "observer", "shape": "dot", "size": 3}, {"group": 39, "id": "parmar, n.", "label": "parmar, n.", "shape": "dot", "size": 1}, {"group": 39, "id": "uszkoreit, j.", "label": "uszkoreit, j.", "shape": "dot", "size": 1}, {"group": 40, "id": "piqa", "label": "piqa", "shape": "dot", "size": 1}, {"group": 40, "id": "physical commonsense in natural language", "label": "physical commonsense in natural language", "shape": "dot", "size": 1}, {"group": 2, "id": "training set", "label": "training set", "shape": "dot", "size": 2}, {"group": 2, "id": "unexpected trials", "label": "unexpected trials", "shape": "dot", "size": 2}, {"group": 9, "id": "probing physics knowledge using tools from developmental psychology", "label": "probing physics knowledge using tools from developmental psychology", "shape": "dot", "size": 1}, {"group": 9, "id": "arxiv preprint arxiv:1804.01128", "label": "arxiv preprint arxiv:1804.01128", "shape": "dot", "size": 1}, {"group": 41, "id": "simulation", "label": "simulation", "shape": "dot", "size": 1}, {"group": 41, "id": "physical scene understanding", "label": "physical scene understanding", "shape": "dot", "size": 1}, {"group": 42, "id": "social iqa: commonsense reasoning about social interactions", "label": "social iqa: commonsense reasoning about social interactions", "shape": "dot", "size": 1}, {"group": 42, "id": "proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language pro", "label": "proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language pro", "shape": "dot", "size": 1}, {"group": 43, "id": "solving the baby intuitions benchmark with a hierarchically bayesian theory of mind", "label": "solving the baby intuitions benchmark with a hierarchically bayesian theory of mind", "shape": "dot", "size": 1}, {"group": 43, "id": "robotics: science and systems workshop on social intelligence in humans and robots", "label": "robotics: science and systems workshop on social intelligence in humans and robots", "shape": "dot", "size": 1}, {"group": 36, "id": "teachers\u2019 extent of the use of particular task types in mathematics", "label": "teachers\u2019 extent of the use of particular task types in mathematics", "shape": "dot", "size": 1}, {"group": 36, "id": "mathematics education research group of australasia", "label": "mathematics education research group of australasia", "shape": "dot", "size": 1}, {"group": 44, "id": "vaswani, a.", "label": "vaswani, a.", "shape": "dot", "size": 1}, {"group": 44, "id": "shazeer, n.", "label": "shazeer, n.", "shape": "dot", "size": 1}, {"group": 28, "id": "ieee/cvf international conference on computer vision", "label": "ieee/cvf international conference on computer vision", "shape": "dot", "size": 1}, {"group": 28, "id": "proceedings of the", "label": "proceedings of the", "shape": "dot", "size": 1}, {"group": 18, "id": "winogrande: an adversarial winograd schema challenge at scale", "label": "winogrande: an adversarial winograd schema challenge at scale", "shape": "dot", "size": 1}, {"group": 18, "id": "communications of the acm", "label": "communications of the acm", "shape": "dot", "size": 1}]);
                  edges = new vis.DataSet([{"from": "2d videos", "title": "contextual proximity", "to": "agent", "width": 1.0}, {"from": "2d videos", "title": "contextual proximity", "to": "colour", "width": 0.5}, {"from": "2d videos", "title": "contextual proximity", "to": "features", "width": 1.0}, {"from": "2d videos", "title": "contextual proximity", "to": "frames", "width": 0.5}, {"from": "2d videos", "title": "contextual proximity", "to": "graphs", "width": 0.75}, {"from": "2d videos", "title": "contextual proximity", "to": "model", "width": 0.5}, {"from": "2d videos", "title": "contextual proximity", "to": "position", "width": 0.5}, {"from": "2d videos", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "2d videos", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "2d videos", "title": "The BIB consists of 2D videos", "to": "baby intuitions benchmark", "width": 1.0}, {"from": "a. bulling", "title": "contextual proximity", "to": "irene", "width": 1.5}, {"from": "a. bulling", "title": "M. Bortoletto and A. Bulling were funded by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 801708", "to": "m. bortoletto", "width": 1.0}, {"from": "abductive commonsense reasoning", "title": "The concept of \u0027Abductive Commonsense Reasoning\u0027 is discussed in the context of the paper \u0027Abductive Commonsense Reasoning\u0027 presented at the 8th International Conference on Learning Representations, ICLR 2020.", "to": "learning representations", "width": 1.0}, {"from": "ablation studies", "title": "To investigate how different components of our method contribute to these performance improvements, we performed a series of ablation studies summarised in Table 2,contextual proximity", "to": "irene", "width": 4.25}, {"from": "ablation studies", "title": "contextual proximity", "to": "relational graphs", "width": 0.5}, {"from": "actions", "title": "contextual proximity", "to": "agent", "width": 0.5}, {"from": "actions", "title": "contextual proximity", "to": "context embedding", "width": 1.0}, {"from": "actions", "title": "contextual proximity", "to": "context encoder", "width": 0.5}, {"from": "actions", "title": "contextual proximity", "to": "ctx token", "width": 0.5}, {"from": "actions", "title": "contextual proximity", "to": "embedding vector", "width": 0.5}, {"from": "actions", "title": "The encoded states are concatenated to the corresponding actions, showing a relationship between the encoded states and actions for processing in IRENE.,The encoded states {hij} are concatenated to the corresponding actions {aij} and projected to the transformer input dimension by a linear layer fproj.,contextual proximity", "to": "encoded states", "width": 2.75}, {"from": "actions", "title": "contextual proximity", "to": "grid-world environment", "width": 0.5}, {"from": "actions", "title": "IRENE is for reasoning about agents\u2019 goals, preferences, and actions,contextual proximity", "to": "irene", "width": 2.75}, {"from": "actions", "title": "contextual proximity", "to": "objects", "width": 0.5}, {"from": "actions", "title": "contextual proximity", "to": "state encoder", "width": 0.75}, {"from": "actions", "title": "contextual proximity", "to": "sub-tasks", "width": 0.5}, {"from": "actions", "title": "contextual proximity", "to": "tasks", "width": 0.75}, {"from": "actions", "title": "contextual proximity", "to": "trials", "width": 0.5}, {"from": "adjacent and aligned", "title": "contextual proximity", "to": "agent\u0027s past trajectories", "width": 0.5}, {"from": "adjacent and aligned", "title": "contextual proximity", "to": "context encoder", "width": 1.0}, {"from": "adjacent and aligned", "title": "contextual proximity", "to": "feature fusion module", "width": 0.5}, {"from": "adjacent and aligned", "title": "contextual proximity", "to": "gnn", "width": 0.5}, {"from": "adjacent and aligned", "title": "The context mentions that local directional relations identify the relative position of two adjacent entities, while adjacent and aligned relations do not require adjacency. These are different types of directional relations.,contextual proximity", "to": "local directional relations", "width": 2.0}, {"from": "adjacent and aligned", "title": "contextual proximity", "to": "node embeddings", "width": 0.5}, {"from": "adjacent and aligned", "title": "contextual proximity", "to": "node features", "width": 1.0}, {"from": "adjacent and aligned", "title": "contextual proximity", "to": "prediction net", "width": 0.5}, {"from": "adjacent and aligned", "title": "The context mentions that remote directional relations do not require adjacency, while adjacent and aligned relations do. These are different types of directional relations.,contextual proximity", "to": "remote directional relations", "width": 2.0}, {"from": "adjacent and aligned", "title": "contextual proximity", "to": "type, position, color, shape", "width": 0.5}, {"from": "agent", "title": "contextual proximity", "to": "architecture", "width": 0.5}, {"from": "agent", "title": "contextual proximity", "to": "baby intuitions benchmark", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "bayesian theory of mind", "width": 0.75}, {"from": "agent", "title": "Both BIB and AGENT are models used to test whether agents have object-based goals and act efficiently. AGENT does not evaluate whether models can reason about multiple agents, inaccessible goals, instrumental actions, or distinguish between rational and irrational agents, while BIB provides a single canonical split to maximize the evaluation of models\u2019 generalisability.,contextual proximity", "to": "bib", "width": 3.75}, {"from": "agent", "title": "BIPaCK is evaluated on AGENT.,contextual proximity", "to": "bipack", "width": 2.5}, {"from": "agent", "title": "contextual proximity", "to": "colour", "width": 2.0}, {"from": "agent", "title": "contextual proximity", "to": "common-sense reasoning tasks", "width": 0.5}, {"from": "agent", "title": "contextual proximity", "to": "context embedding", "width": 0.5}, {"from": "agent", "title": "contextual proximity", "to": "context embeddings", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "context encoder", "width": 0.5}, {"from": "agent", "title": "AGENT is a Benchmark for Core Psychological Reasoning", "to": "core psychological reasoning", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "ctx token", "width": 0.5}, {"from": "agent", "title": "contextual proximity", "to": "edges", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "efficient action", "width": 0.5}, {"from": "agent", "title": "contextual proximity", "to": "encoded states", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "evaluation", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "feature fusion module", "width": 0.5}, {"from": "agent", "title": "contextual proximity", "to": "features", "width": 4.0}, {"from": "agent", "title": "contextual proximity", "to": "frames", "width": 2.0}, {"from": "agent", "title": "contextual proximity", "to": "gandhi et al. 2021", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "gnn", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "graph", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "graphs", "width": 3.0}, {"from": "agent", "title": "The agent interacts with different objects in the grid-world environment, representing the key entities in the environment.,contextual proximity", "to": "grid-world environment", "width": 3.0}, {"from": "agent", "title": "contextual proximity", "to": "ground truth", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "hbtom", "width": 0.75}, {"from": "agent", "title": "contextual proximity", "to": "instrumental action", "width": 0.5}, {"from": "agent", "title": "contextual proximity", "to": "irene", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "lack of accessible and well-maintained benchmarks", "width": 0.5}, {"from": "agent", "title": "contextual proximity", "to": "message passing", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "model", "width": 2.0}, {"from": "agent", "title": "contextual proximity", "to": "normalised between [-1, 1]", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "normalised between [0, 1]", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "objects", "width": 2.0}, {"from": "agent", "title": "contextual proximity", "to": "one-hot vectors", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "position", "width": 2.0}, {"from": "agent", "title": "contextual proximity", "to": "shape", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "spatial relations", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "state encoder", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "states", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "sub-tasks", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "tasks", "width": 1.5}, {"from": "agent", "title": "contextual proximity", "to": "training and evaluation on the baby intuitions benchmark", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "trajectories", "width": 0.5}, {"from": "agent", "title": "contextual proximity", "to": "transformer", "width": 2.0}, {"from": "agent", "title": "contextual proximity", "to": "transformer context encoder", "width": 0.5}, {"from": "agent", "title": "contextual proximity", "to": "trials", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "type", "width": 2.0}, {"from": "agent", "title": "contextual proximity", "to": "violation of expectation paradigm", "width": 1.0}, {"from": "agent", "title": "contextual proximity", "to": "weights", "width": 1.0}, {"from": "agent and world state encoding", "title": "contextual proximity", "to": "irene", "width": 0.75}, {"from": "agent and world state encoding", "title": "contextual proximity", "to": "rational agents", "width": 0.75}, {"from": "agent and world state encoding", "title": "contextual proximity", "to": "training tasks", "width": 0.5}, {"from": "agent and world state encoding", "title": "GraphSAGE performs well thanks to inductivity: learning an aggregator allows the model to effectively generate embeddings for nodes", "to": "graphsage", "width": 1.0}, {"from": "agent tests", "title": "contextual proximity", "to": "benchmarks", "width": 0.75}, {"from": "agent tests", "title": "Like the BIB, AGENT tests whether models can predict that agents have object-based goals and act efficiently.,contextual proximity", "to": "bib", "width": 1.5}, {"from": "agent tests", "title": "contextual proximity", "to": "research", "width": 0.5}, {"from": "agent\u0027s past trajectories", "title": "The context encoder parses the agent\u0027s past trajectories into a context vector, indicating a relationship between the two.,contextual proximity", "to": "context encoder", "width": 1.5}, {"from": "agent\u0027s past trajectories", "title": "contextual proximity", "to": "local directional relations", "width": 0.5}, {"from": "agent\u0027s past trajectories", "title": "contextual proximity", "to": "node features", "width": 0.5}, {"from": "agent\u0027s past trajectories", "title": "contextual proximity", "to": "remote directional relations", "width": 0.5}, {"from": "agent-blocked instrumental action", "title": "contextual proximity", "to": "evaluation trials", "width": 0.75}, {"from": "agent-blocked instrumental action", "title": "contextual proximity", "to": "frame", "width": 0.75}, {"from": "agent-blocked instrumental action", "title": "contextual proximity", "to": "graph generation", "width": 1.5}, {"from": "agent-blocked instrumental action", "title": "contextual proximity", "to": "instrumental blocking barrier", "width": 1.5}, {"from": "agent-blocked instrumental action", "title": "contextual proximity", "to": "json file", "width": 0.75}, {"from": "agent-blocked instrumental action", "title": "contextual proximity", "to": "navigation preference", "width": 0.75}, {"from": "agent-blocked instrumental action", "title": "Single-Object Multiple-Agent is related to Agent-Blocked Instrumental Action as models need to combine knowledge of navigation and instrumental actions to solve tasks, as mentioned in the context.,contextual proximity", "to": "single-object multiple-agent", "width": 2.5}, {"from": "agent-blocked instrumental action", "title": "contextual proximity", "to": "training trials", "width": 0.75}, {"from": "agents", "title": "contextual proximity", "to": "existing models", "width": 0.5}, {"from": "agents", "title": "contextual proximity", "to": "graph neural network (gnn)", "width": 0.75}, {"from": "agents", "title": "contextual proximity", "to": "intuitive reasoning network (irene)", "width": 1.25}, {"from": "agents", "title": "IRENE is able to bind preferences to specific agents and to better distinguish between rational and irrational agents.,contextual proximity", "to": "irene", "width": 2.25}, {"from": "agents", "title": "contextual proximity", "to": "model", "width": 0.75}, {"from": "agents", "title": "contextual proximity", "to": "reasoning tasks", "width": 0.5}, {"from": "agents", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "agents", "title": "contextual proximity", "to": "video transformer", "width": 0.5}, {"from": "agents", "title": "VT fails in binding preferences to specific agents as mentioned in the context.,contextual proximity", "to": "vt", "width": 2.0}, {"from": "agents\u2019 goals", "title": "IRENE is for reasoning about agents\u2019 goals, preferences, and actions,contextual proximity", "to": "irene", "width": 2.5}, {"from": "agents\u2019 preferences", "title": "contextual proximity", "to": "existing models", "width": 0.5}, {"from": "agents\u2019 preferences", "title": "contextual proximity", "to": "graph neural network (gnn)", "width": 0.75}, {"from": "agents\u2019 preferences", "title": "contextual proximity", "to": "intuitive reasoning network (irene)", "width": 0.5}, {"from": "agents\u2019 preferences", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "agents\u2019 preferences", "title": "contextual proximity", "to": "model", "width": 0.75}, {"from": "agents\u2019 preferences", "title": "contextual proximity", "to": "reasoning tasks", "width": 0.5}, {"from": "agents\u2019 preferences", "title": "contextual proximity", "to": "video transformer", "width": 0.5}, {"from": "agents\u2019 preferences", "title": "VT is rather successful in modelling agents\u2019 preferences as mentioned in the context.,contextual proximity", "to": "vt", "width": 2.0}, {"from": "ai agents", "title": "contextual proximity", "to": "ai systems", "width": 0.75}, {"from": "ai agents", "title": "contextual proximity", "to": "benchmarks", "width": 0.75}, {"from": "ai agents", "title": "contextual proximity", "to": "developmental disorders", "width": 0.75}, {"from": "ai agents", "title": "contextual proximity", "to": "human-machine collaboration", "width": 0.75}, {"from": "ai agents", "title": "contextual proximity", "to": "humans", "width": 0.75}, {"from": "ai agents", "title": "contextual proximity", "to": "infants", "width": 0.75}, {"from": "ai agents", "title": "contextual proximity", "to": "machine common-sense reasoning", "width": 0.75}, {"from": "ai agents", "title": "contextual proximity", "to": "research", "width": 0.75}, {"from": "ai agents", "title": "contextual proximity", "to": "robotics", "width": 0.75}, {"from": "ai capabilities", "title": "contextual proximity", "to": "baby intuitions benchmark (bib)", "width": 1.0}, {"from": "ai capabilities", "title": "contextual proximity", "to": "test trials", "width": 0.5}, {"from": "ai capabilities", "title": "VoE allows probing AI capabilities by comparing scenarios that humans can differentiate based on those capabilities.,contextual proximity", "to": "voe paradigm", "width": 1.5}, {"from": "ai systems", "title": "New benchmarks have been introduced to assess the general ability of AI systems to reason about unexpected events or situations.", "to": "benchmarks", "width": 1.0}, {"from": "ann-sophia", "title": "contextual proximity", "to": "denisov", "width": 1.0}, {"from": "ann-sophia", "title": "contextual proximity", "to": "ekta sood", "width": 0.75}, {"from": "ann-sophia", "title": "contextual proximity", "to": "hsiu-yu yang", "width": 0.75}, {"from": "ann-sophia", "title": "contextual proximity", "to": "manuel mager", "width": 0.75}, {"from": "architecture", "title": "contextual proximity", "to": "encoded states", "width": 0.5}, {"from": "architecture", "title": "contextual proximity", "to": "grid-world environment", "width": 0.5}, {"from": "architecture", "title": "contextual proximity", "to": "objects", "width": 0.5}, {"from": "architecture", "title": "contextual proximity", "to": "state encoder", "width": 0.5}, {"from": "architecture", "title": "contextual proximity", "to": "sub-tasks", "width": 0.5}, {"from": "architecture", "title": "contextual proximity", "to": "tasks", "width": 0.75}, {"from": "architecture", "title": "contextual proximity", "to": "trials", "width": 0.5}, {"from": "architecture", "title": "The architecture of IRENE, as shown in Figure 2, includes inputs representing entities in a video frame, indicating the components of the architecture.", "to": "irene", "width": 1.0}, {"from": "attention heads", "title": "contextual proximity", "to": "feature fusion module", "width": 0.5}, {"from": "attention heads", "title": "contextual proximity", "to": "gandhi et al.", "width": 0.75}, {"from": "attention heads", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "attention heads", "title": "contextual proximity", "to": "mlp policy", "width": 0.5}, {"from": "attention heads", "title": "contextual proximity", "to": "prediction net", "width": 0.5}, {"from": "attention heads", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "attention heads", "title": "contextual proximity", "to": "state encoder", "width": 0.75}, {"from": "attention heads", "title": "The transformer encoder has four attention heads,contextual proximity", "to": "transformer encoder", "width": 1.5}, {"from": "attention is all you need", "title": "The first concept is the title of a paper published in the Advances in Neural Information Processing Systems, 30.", "to": "advances in neural information processing systems, 30", "width": 1.0}, {"from": "baby intuitions benchmark", "title": "contextual proximity", "to": "colour", "width": 0.5}, {"from": "baby intuitions benchmark", "title": "contextual proximity", "to": "features", "width": 1.0}, {"from": "baby intuitions benchmark", "title": "contextual proximity", "to": "frames", "width": 0.5}, {"from": "baby intuitions benchmark", "title": "contextual proximity", "to": "graphs", "width": 0.75}, {"from": "baby intuitions benchmark", "title": "contextual proximity", "to": "intuitive reasoning network (irene)", "width": 0.75}, {"from": "baby intuitions benchmark", "title": "IRENE achieves new state-of-the-art performance on three out of five tasks on the Baby Intuitions Benchmark.,IRENE sets new state-of-the-art on three out of five tasks on the Baby Intuitions Benchmark,contextual proximity", "to": "irene", "width": 4.25}, {"from": "baby intuitions benchmark", "title": "contextual proximity", "to": "model", "width": 0.5}, {"from": "baby intuitions benchmark", "title": "contextual proximity", "to": "position", "width": 0.5}, {"from": "baby intuitions benchmark", "title": "contextual proximity", "to": "transformer", "width": 0.75}, {"from": "baby intuitions benchmark", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "baby intuitions benchmark (bib)", "title": "BIB is a benchmark for discerning the goals, preferences, and actions of others, as discussed in the Advances in Neural Information Processing Systems.", "to": "discerning the goals, preferences, and actions of others", "width": 1.0}, {"from": "baby intuitions benchmark (bib)", "title": "Gandhi et al. have recently introduced the Baby Intuitions Benchmark (BIB) \u2013 a set of tasks that require an observer model to reason about agents\u2019 goals, preferences, and actions by observing their behavior in a grid-world environment.,contextual proximity", "to": "gandhi et al.", "width": 2.0}, {"from": "baby intuitions benchmark (bib)", "title": "contextual proximity", "to": "initial models", "width": 1.0}, {"from": "baby intuitions benchmark (bib)", "title": "contextual proximity", "to": "machine theory of mind network", "width": 1.0}, {"from": "baby intuitions benchmark (bib)", "title": "contextual proximity", "to": "meta-learning problem", "width": 1.0}, {"from": "baby intuitions benchmark (bib)", "title": "contextual proximity", "to": "models\u2019 ability to predict future actions", "width": 1.0}, {"from": "baby intuitions benchmark (bib)", "title": "contextual proximity", "to": "most recent model (hein and diepold 2022, vt)", "width": 1.0}, {"from": "baby intuitions benchmark (bib)", "title": "contextual proximity", "to": "observer model", "width": 1.0}, {"from": "baby intuitions benchmark (bib)", "title": "contextual proximity", "to": "test trials", "width": 2.0}, {"from": "baby intuitions benchmark (bib)", "title": "contextual proximity", "to": "training and evaluation tasks", "width": 1.0}, {"from": "baby intuitions benchmark (bib)", "title": "contextual proximity", "to": "video", "width": 1.0}, {"from": "baby intuitions benchmark (bib)", "title": "contextual proximity", "to": "voe paradigm", "width": 2.0}, {"from": "bail-largeon 1987", "title": "contextual proximity", "to": "common-sense reasoning", "width": 0.5}, {"from": "bail-largeon 1987", "title": "contextual proximity", "to": "expected events or situations", "width": 0.5}, {"from": "bail-largeon 1987", "title": "contextual proximity", "to": "gandhi et al.", "width": 0.75}, {"from": "bail-largeon 1987", "title": "contextual proximity", "to": "intuitive physics", "width": 1.0}, {"from": "bail-largeon 1987", "title": "contextual proximity", "to": "intuitive psychology", "width": 0.75}, {"from": "bail-largeon 1987", "title": "contextual proximity", "to": "piloto et al.", "width": 0.5}, {"from": "bail-largeon 1987", "title": "contextual proximity", "to": "riochet et al.", "width": 0.5}, {"from": "bail-largeon 1987", "title": "contextual proximity", "to": "shu et al.", "width": 0.5}, {"from": "bail-largeon 1987", "title": "Bail-largeon 1987 and violation of expectation paradigm are related as they are mentioned together in the context.", "to": "violation of expectation paradigm", "width": 1.0}, {"from": "bayesian theory of mind", "title": "contextual proximity", "to": "bib", "width": 0.75}, {"from": "bayesian theory of mind", "title": "contextual proximity", "to": "bipack", "width": 0.5}, {"from": "bayesian theory of mind", "title": "Zhi et al. have evaluated a Bayesian Theory of Mind model with hierarchical priors over agents\u2019 preference and efficiency (HBToM) on the BIB.", "to": "hbtom", "width": 1.0}, {"from": "bc-mlp", "title": "contextual proximity", "to": "feature fusion module", "width": 0.5}, {"from": "bc-mlp", "title": "Gandhi et al. proposed the BC-MLP model,contextual proximity", "to": "gandhi et al.", "width": 1.75}, {"from": "bc-mlp", "title": "IRENE outperforms BC-MLP by 16 % in the Efficiency Irrational Agent task,contextual proximity", "to": "irene", "width": 4.75}, {"from": "bc-mlp", "title": "contextual proximity", "to": "mlp policy", "width": 0.5}, {"from": "bc-mlp", "title": "contextual proximity", "to": "prediction net", "width": 0.5}, {"from": "bc-mlp", "title": "contextual proximity", "to": "relational graphs", "width": 0.5}, {"from": "bc-mlp", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "bc-mlp", "title": "contextual proximity", "to": "state encoder", "width": 0.75}, {"from": "bc-mlp", "title": "contextual proximity", "to": "transformer encoder", "width": 0.5}, {"from": "bc-mlp", "title": "contextual proximity", "to": "video-rnn", "width": 0.5}, {"from": "bc-rnn", "title": "contextual proximity", "to": "feature fusion module", "width": 0.5}, {"from": "bc-rnn", "title": "Gandhi et al. proposed the BC-RNN model,contextual proximity", "to": "gandhi et al.", "width": 1.75}, {"from": "bc-rnn", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "bc-rnn", "title": "contextual proximity", "to": "mlp policy", "width": 0.5}, {"from": "bc-rnn", "title": "contextual proximity", "to": "prediction net", "width": 0.5}, {"from": "bc-rnn", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "bc-rnn", "title": "contextual proximity", "to": "state encoder", "width": 0.75}, {"from": "bc-rnn", "title": "contextual proximity", "to": "transformer encoder", "width": 0.5}, {"from": "behaviour", "title": "The concept of \u0027Behaviour\u0027 is discussed in the context of the book \u0027Mindblindness: An Essay on Autism and Theory of Mind\u0027 by Baron-Cohen, S.", "to": "mindblindness", "width": 1.0}, {"from": "benchmarking progress to infant-level physical reasoning in ai", "title": "The first concept is the title of a paper published in the Transactions on Machine Learning Research.", "to": "transactions on machine learning research", "width": 1.0}, {"from": "benchmarks", "title": "contextual proximity", "to": "bib", "width": 1.5}, {"from": "benchmarks", "title": "contextual proximity", "to": "collaborative agents", "width": 0.75}, {"from": "benchmarks", "title": "contextual proximity", "to": "evaluating language processing", "width": 0.75}, {"from": "benchmarks", "title": "contextual proximity", "to": "general ability of ai systems to reason about unexpected events or situations", "width": 0.75}, {"from": "benchmarks", "title": "contextual proximity", "to": "infants expect other agents to have goals, preferences and engage in instrumental actions", "width": 0.75}, {"from": "benchmarks", "title": "contextual proximity", "to": "intelligent", "width": 0.75}, {"from": "benchmarks", "title": "contextual proximity", "to": "intuitive physics", "width": 0.75}, {"from": "benchmarks", "title": "contextual proximity", "to": "models\u2019 ability to reason about other agents", "width": 0.75}, {"from": "benchmarks", "title": "contextual proximity", "to": "research", "width": 1.75}, {"from": "benchmarks", "title": "contextual proximity", "to": "visual scene understanding", "width": 0.75}, {"from": "bhagavatula, c.", "title": "Authors of the paper \u0027Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning\u0027", "to": "choi, y.", "width": 1.0}, {"from": "bib", "title": "contextual proximity", "to": "bipack", "width": 1.5}, {"from": "bib", "title": "contextual proximity", "to": "collaborative agents", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "encoded states", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "evaluating language processing", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "expectedness", "width": 0.5}, {"from": "bib", "title": "On the BIB, Gandhi et al. have proposed a model based on the Theory of Mind neural network introduced by (Rabinowitz et al. 2018),contextual proximity", "to": "gandhi et al.", "width": 2.0}, {"from": "bib", "title": "contextual proximity", "to": "general ability of ai systems to reason about unexpected events or situations", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "gnn", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "graphsage", "width": 1.0}, {"from": "bib", "title": "contextual proximity", "to": "grid-world environment", "width": 0.5}, {"from": "bib", "title": "HBToM computes a plausibility score by training a set of logistic regression classifiers on a synthetic dataset similar to the BIB evaluation set,contextual proximity", "to": "hbtom", "width": 2.75}, {"from": "bib", "title": "contextual proximity", "to": "hein et al.", "width": 0.5}, {"from": "bib", "title": "Prior work on the BIB has focused on improving performance", "to": "improving performance", "width": 1.0}, {"from": "bib", "title": "contextual proximity", "to": "infants expect other agents to have goals, preferences and engage in instrumental actions", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "intelligent", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "intuitive physics", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "logistic regression classifiers", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "lstm", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "models\u2019 ability to reason about other agents", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "objects", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "research", "width": 1.0}, {"from": "bib", "title": "contextual proximity", "to": "state encoder", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "state-action pairs", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "sub-tasks", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "tasks", "width": 0.75}, {"from": "bib", "title": "contextual proximity", "to": "test trial", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "trials", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "video frames", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "visual scene understanding", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "voe paradigm", "width": 0.5}, {"from": "bib", "title": "contextual proximity", "to": "vt", "width": 1.0}, {"from": "bib task", "title": "IRENE\u0027s performance was compared with BIB Task,contextual proximity", "to": "irene", "width": 2.25}, {"from": "bib task", "title": "contextual proximity", "to": "model", "width": 0.75}, {"from": "bib task", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "bipack", "title": "contextual proximity", "to": "hbtom", "width": 0.5}, {"from": "blocking barriers", "title": "GraphSAGE is key for our model to better understand the role of blocking barriers,contextual proximity", "to": "graphsage", "width": 2.0}, {"from": "blocking barriers", "title": "IRENE does not perform better on all tasks due to a lack of knowledge of blocking barriers,contextual proximity", "to": "irene", "width": 1.5}, {"from": "blocking barriers", "title": "contextual proximity", "to": "lack of accessible and well-maintained benchmarks", "width": 0.5}, {"from": "blocking barriers", "title": "contextual proximity", "to": "lstm", "width": 0.5}, {"from": "blocking obstacles", "title": "contextual proximity", "to": "intuitive reasoning network (irene)", "width": 0.75}, {"from": "blocking obstacles", "title": "IRENE is able to better understand the role of blocking obstacles.,contextual proximity", "to": "irene", "width": 1.75}, {"from": "clevrer: collision events for video representation and reasoning", "title": "The first concept is the title of a paper presented at the International Conference on Learning Representations.", "to": "international conference on learning representations", "width": 1.0}, {"from": "cognitron", "title": "Cognitron is a type of self-organizing multi-layered neural network, as described in the Biological Cybernetics journal.", "to": "self-organizing multi-layered neural network", "width": 1.0}, {"from": "collaborative agents", "title": "contextual proximity", "to": "research", "width": 0.5}, {"from": "collaborative agents", "title": "These are the key concepts mentioned in the given context.", "to": "intelligent", "width": 1.0}, {"from": "colour", "title": "contextual proximity", "to": "context embeddings", "width": 0.5}, {"from": "colour", "title": "contextual proximity", "to": "edges", "width": 0.5}, {"from": "colour", "title": "contextual proximity", "to": "evaluation", "width": 0.5}, {"from": "colour", "title": "The features include colour as one of its components,contextual proximity", "to": "features", "width": 3.0}, {"from": "colour", "title": "contextual proximity", "to": "frames", "width": 1.0}, {"from": "colour", "title": "contextual proximity", "to": "gandhi et al. 2021", "width": 0.5}, {"from": "colour", "title": "contextual proximity", "to": "gnn", "width": 0.5}, {"from": "colour", "title": "contextual proximity", "to": "graph", "width": 0.5}, {"from": "colour", "title": "contextual proximity", "to": "graphs", "width": 1.5}, {"from": "colour", "title": "contextual proximity", "to": "grid-world environment", "width": 0.5}, {"from": "colour", "title": "contextual proximity", "to": "ground truth", "width": 0.5}, {"from": "colour", "title": "contextual proximity", "to": "message passing", "width": 0.5}, {"from": "colour", "title": "contextual proximity", "to": "model", "width": 1.0}, {"from": "colour", "title": "contextual proximity", "to": "normalised between [-1, 1]", "width": 0.5}, {"from": "colour", "title": "contextual proximity", "to": "normalised between [0, 1]", "width": 0.5}, {"from": "colour", "title": "contextual proximity", "to": "objects", "width": 0.5}, {"from": "colour", "title": "contextual proximity", "to": "one-hot vectors", "width": 0.5}, {"from": "colour", "title": "contextual proximity", "to": "position", "width": 1.0}, {"from": "colour", "title": "contextual proximity", "to": "shape", "width": 0.5}, {"from": "colour", "title": "contextual proximity", "to": "spatial relations", "width": 0.5}, {"from": "colour", "title": "contextual proximity", "to": "states", "width": 0.5}, {"from": "colour", "title": "contextual proximity", "to": "training and evaluation on the baby intuitions benchmark", "width": 0.5}, {"from": "colour", "title": "contextual proximity", "to": "transformer", "width": 1.0}, {"from": "colour", "title": "contextual proximity", "to": "type", "width": 1.0}, {"from": "colour", "title": "contextual proximity", "to": "violation of expectation paradigm", "width": 0.5}, {"from": "colour", "title": "contextual proximity", "to": "weights", "width": 0.5}, {"from": "common-sense reasoning", "title": "contextual proximity", "to": "dasgupta et al.", "width": 0.5}, {"from": "common-sense reasoning", "title": "contextual proximity", "to": "expected events or situations", "width": 1.0}, {"from": "common-sense reasoning", "title": "Gandhi et al. and common-sense reasoning are related as they are mentioned together in the context.,contextual proximity", "to": "gandhi et al.", "width": 2.5}, {"from": "common-sense reasoning", "title": "contextual proximity", "to": "intuitive physics", "width": 2.0}, {"from": "common-sense reasoning", "title": "contextual proximity", "to": "intuitive psychology", "width": 1.5}, {"from": "common-sense reasoning", "title": "contextual proximity", "to": "intuitive reasoning network (irene)", "width": 0.75}, {"from": "common-sense reasoning", "title": "contextual proximity", "to": "irene", "width": 0.75}, {"from": "common-sense reasoning", "title": "contextual proximity", "to": "piloto et al.", "width": 1.0}, {"from": "common-sense reasoning", "title": "contextual proximity", "to": "riochet et al.", "width": 1.0}, {"from": "common-sense reasoning", "title": "Shu et al. and common-sense reasoning are related as they are mentioned together in the context.,contextual proximity", "to": "shu et al.", "width": 2.0}, {"from": "common-sense reasoning", "title": "contextual proximity", "to": "violation of expectation paradigm", "width": 0.5}, {"from": "common-sense reasoning", "title": "contextual proximity", "to": "weihs et al.", "width": 0.5}, {"from": "common-sense reasoning", "title": "Studies in developmental cognitive science have demonstrated common-sense reasoning abilities.", "to": "developmental cognitive science", "width": 1.0}, {"from": "common-sense reasoning benchmarks", "title": "IRENE achieves new state-of-the-art performance on three out of five BIB reasoning tasks, which are common-sense reasoning benchmarks,contextual proximity", "to": "irene", "width": 3.5}, {"from": "common-sense reasoning benchmarks", "title": "contextual proximity", "to": "performance", "width": 0.5}, {"from": "common-sense reasoning benchmarks", "title": "contextual proximity", "to": "training tasks", "width": 0.5}, {"from": "common-sense reasoning tasks", "title": "contextual proximity", "to": "encoded states", "width": 0.5}, {"from": "common-sense reasoning tasks", "title": "contextual proximity", "to": "grid-world environment", "width": 0.5}, {"from": "common-sense reasoning tasks", "title": "contextual proximity", "to": "objects", "width": 0.5}, {"from": "common-sense reasoning tasks", "title": "contextual proximity", "to": "state encoder", "width": 0.5}, {"from": "common-sense reasoning tasks", "title": "contextual proximity", "to": "sub-tasks", "width": 0.5}, {"from": "common-sense reasoning tasks", "title": "The benchmark proposes five common-sense reasoning tasks derived from research on infant intuitive psychology, indicating that these tasks are part of the common-sense reasoning domain.,contextual proximity", "to": "tasks", "width": 1.75}, {"from": "common-sense reasoning tasks", "title": "contextual proximity", "to": "trials", "width": 0.5}, {"from": "commonsense psychology", "title": "Commonsense psychology is studied in the context of human infants and machines", "to": "human infants and machines", "width": 1.0}, {"from": "comparing intuitions", "title": "The comparison of intuitions about agents\u0027 goals, preferences, and actions in human infants is a topic of interest in the given context.", "to": "agents\u2019 goals, preferences and actions in human infants", "width": 1.0}, {"from": "complementary basic concepts", "title": "To fully solve the BIB tasks, it may also be necessary to learn complementary basic concepts, such as from intuitive physics,contextual proximity", "to": "irene", "width": 1.5}, {"from": "complementary basic concepts", "title": "contextual proximity", "to": "lack of accessible and well-maintained benchmarks", "width": 0.5}, {"from": "complex tasks", "title": "IRENE has proven to be effective in handling complex tasks that cannot be tackled by heuristics alone.,contextual proximity", "to": "irene", "width": 2.0}, {"from": "complex tasks", "title": "contextual proximity", "to": "models", "width": 0.5}, {"from": "constantin ruhdorfer", "title": "contextual proximity", "to": "denisov", "width": 1.0}, {"from": "constantin ruhdorfer", "title": "contextual proximity", "to": "ekta sood", "width": 0.75}, {"from": "constantin ruhdorfer", "title": "contextual proximity", "to": "hsiu-yu yang", "width": 0.75}, {"from": "constantin ruhdorfer", "title": "contextual proximity", "to": "manuel mager", "width": 0.75}, {"from": "constantin ruhdorfer", "title": "Both are mentioned in the same sentence, indicating a potential relationship", "to": "m\u00a8uller", "width": 1.0}, {"from": "context embedding", "title": "The context encoder outputs a single context embedding obtained by computing the mean of the eight familiarisation trial representations.,contextual proximity", "to": "context encoder", "width": 2.0}, {"from": "context embedding", "title": "contextual proximity", "to": "ctx token", "width": 1.0}, {"from": "context embedding", "title": "contextual proximity", "to": "edges", "width": 0.75}, {"from": "context embedding", "title": "contextual proximity", "to": "edges eij", "width": 0.75}, {"from": "context embedding", "title": "contextual proximity", "to": "embedding vector", "width": 1.5}, {"from": "context embedding", "title": "contextual proximity", "to": "encoded states", "width": 1.25}, {"from": "context embedding", "title": "The encoder produces a context embedding as the mean of the CXT embedding vectors of each familiarisation trial.,contextual proximity", "to": "encoder", "width": 1.75}, {"from": "context embedding", "title": "contextual proximity", "to": "gnn", "width": 0.5}, {"from": "context embedding", "title": "contextual proximity", "to": "graph-sage layers", "width": 0.75}, {"from": "context embedding", "title": "contextual proximity", "to": "grid-world environment", "width": 0.5}, {"from": "context embedding", "title": "contextual proximity", "to": "lstm aggregation", "width": 0.75}, {"from": "context embedding", "title": "contextual proximity", "to": "message passing", "width": 0.5}, {"from": "context embedding", "title": "contextual proximity", "to": "mlp", "width": 0.5}, {"from": "context embedding", "title": "contextual proximity", "to": "mlp policy", "width": 0.5}, {"from": "context embedding", "title": "contextual proximity", "to": "node embeddings", "width": 0.5}, {"from": "context embedding", "title": "contextual proximity", "to": "objects", "width": 0.5}, {"from": "context embedding", "title": "contextual proximity", "to": "relationships", "width": 0.75}, {"from": "context embedding", "title": "contextual proximity", "to": "rnn", "width": 0.5}, {"from": "context embedding", "title": "contextual proximity", "to": "spatial relationships", "width": 0.75}, {"from": "context embedding", "title": "state embedding is concatenated to the context embedding,contextual proximity", "to": "state embedding", "width": 2.0}, {"from": "context embedding", "title": "contextual proximity", "to": "state encoder", "width": 1.75}, {"from": "context embedding", "title": "contextual proximity", "to": "sub-tasks", "width": 0.5}, {"from": "context embedding", "title": "contextual proximity", "to": "tasks", "width": 0.75}, {"from": "context embedding", "title": "contextual proximity", "to": "test frame graph", "width": 0.5}, {"from": "context embedding", "title": "contextual proximity", "to": "test state", "width": 0.75}, {"from": "context embedding", "title": "contextual proximity", "to": "transformer encoder", "width": 0.75}, {"from": "context embedding", "title": "contextual proximity", "to": "trial representations", "width": 0.75}, {"from": "context embedding", "title": "contextual proximity", "to": "trials", "width": 0.5}, {"from": "context embedding", "title": "The transformer context encoder produces a context embedding as the mean of the CXT embedding vectors, showing the output relationship between the components in the context encoder.", "to": "transformer context encoder", "width": 1.0}, {"from": "context embeddings", "title": "contextual proximity", "to": "features", "width": 1.0}, {"from": "context embeddings", "title": "contextual proximity", "to": "frames", "width": 0.5}, {"from": "context embeddings", "title": "contextual proximity", "to": "graphs", "width": 0.75}, {"from": "context embeddings", "title": "contextual proximity", "to": "model", "width": 0.5}, {"from": "context embeddings", "title": "contextual proximity", "to": "position", "width": 0.5}, {"from": "context embeddings", "title": "The transformer is used to generate context embeddings,contextual proximity", "to": "transformer", "width": 1.5}, {"from": "context embeddings", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "context encoder", "title": "contextual proximity", "to": "ctx token", "width": 0.5}, {"from": "context encoder", "title": "contextual proximity", "to": "embedding vector", "width": 0.5}, {"from": "context encoder", "title": "contextual proximity", "to": "encoded states", "width": 0.75}, {"from": "context encoder", "title": "contextual proximity", "to": "feature fusion module", "width": 0.75}, {"from": "context encoder", "title": "contextual proximity", "to": "gnn", "width": 0.5}, {"from": "context encoder", "title": "contextual proximity", "to": "grid-world environment", "width": 0.5}, {"from": "context encoder", "title": "contextual proximity", "to": "local directional relations", "width": 1.0}, {"from": "context encoder", "title": "contextual proximity", "to": "node embeddings", "width": 0.5}, {"from": "context encoder", "title": "contextual proximity", "to": "node features", "width": 1.0}, {"from": "context encoder", "title": "contextual proximity", "to": "objects", "width": 0.5}, {"from": "context encoder", "title": "contextual proximity", "to": "prediction net", "width": 0.5}, {"from": "context encoder", "title": "contextual proximity", "to": "remote directional relations", "width": 1.0}, {"from": "context encoder", "title": "contextual proximity", "to": "state encoder", "width": 0.75}, {"from": "context encoder", "title": "contextual proximity", "to": "sub-tasks", "width": 0.5}, {"from": "context encoder", "title": "contextual proximity", "to": "tasks", "width": 0.75}, {"from": "context encoder", "title": "contextual proximity", "to": "trials", "width": 0.5}, {"from": "context encoder", "title": "contextual proximity", "to": "type, position, color, shape", "width": 0.5}, {"from": "ctx token", "title": "contextual proximity", "to": "embedding vector", "width": 0.5}, {"from": "ctx token", "title": "The encoded states are concatenated to a special learnable CTX token, indicating a connection between the encoded states and the token in the context of IRENE.,contextual proximity", "to": "encoded states", "width": 1.75}, {"from": "ctx token", "title": "contextual proximity", "to": "grid-world environment", "width": 0.5}, {"from": "ctx token", "title": "contextual proximity", "to": "objects", "width": 0.5}, {"from": "ctx token", "title": "contextual proximity", "to": "state encoder", "width": 0.75}, {"from": "ctx token", "title": "contextual proximity", "to": "sub-tasks", "width": 0.5}, {"from": "ctx token", "title": "contextual proximity", "to": "tasks", "width": 0.75}, {"from": "ctx token", "title": "contextual proximity", "to": "trials", "width": 0.5}, {"from": "dasgupta et al.", "title": "contextual proximity", "to": "expected events or situations", "width": 0.5}, {"from": "dasgupta et al.", "title": "contextual proximity", "to": "gandhi et al.", "width": 0.75}, {"from": "dasgupta et al.", "title": "contextual proximity", "to": "intuitive physics", "width": 1.0}, {"from": "dasgupta et al.", "title": "contextual proximity", "to": "intuitive psychology", "width": 0.75}, {"from": "dasgupta et al.", "title": "contextual proximity", "to": "piloto et al.", "width": 0.5}, {"from": "dasgupta et al.", "title": "contextual proximity", "to": "riochet et al.", "width": 0.5}, {"from": "dasgupta et al.", "title": "contextual proximity", "to": "shu et al.", "width": 0.5}, {"from": "deep network learning", "title": "ELUs is a technique used in deep network learning for more accurate results.", "to": "exponential linear units (elus)", "width": 1.0}, {"from": "denisov", "title": "contextual proximity", "to": "ekta sood", "width": 3.0}, {"from": "denisov", "title": "contextual proximity", "to": "hsiu-yu yang", "width": 3.0}, {"from": "denisov", "title": "contextual proximity", "to": "manuel mager", "width": 3.0}, {"from": "denisov", "title": "contextual proximity", "to": "m\u00a8uller", "width": 1.0}, {"from": "designing and creating new benchmarks", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "designing and creating new benchmarks", "title": "contextual proximity", "to": "lack of accessible and well-maintained benchmarks", "width": 0.5}, {"from": "designing and creating new benchmarks", "title": "There is an urgent need for the community to design and create new benchmarks to foster the development of \u201cgeneral neural common-sense reasoners\u201d", "to": "general neural common-sense reasoners", "width": 1.0}, {"from": "deutsche forschungsgemeinschaft", "title": "contextual proximity", "to": "irene", "width": 1.5}, {"from": "deutsche forschungsgemeinschaft", "title": "L. Shi was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany\u2019s Excellence Strategy \u2013 EXC 2075 \u2013 390740016", "to": "l. shi", "width": 1.0}, {"from": "developmental cognitive science", "title": "contextual proximity", "to": "intuitive reasoning network (irene)", "width": 0.75}, {"from": "developmental cognitive science", "title": "contextual proximity", "to": "irene", "width": 0.75}, {"from": "developmental disorders", "title": "Young infants have abilities that can be linked to developmental disorders such as autism.", "to": "infants", "width": 1.0}, {"from": "edges", "title": "contextual proximity", "to": "embedding vector", "width": 0.5}, {"from": "edges", "title": "contextual proximity", "to": "features", "width": 1.0}, {"from": "edges", "title": "contextual proximity", "to": "frames", "width": 0.5}, {"from": "edges", "title": "contextual proximity", "to": "graphs", "width": 1.0}, {"from": "edges", "title": "contextual proximity", "to": "local directional relations", "width": 0.75}, {"from": "edges", "title": "contextual proximity", "to": "model", "width": 0.5}, {"from": "edges", "title": "contextual proximity", "to": "performance", "width": 0.75}, {"from": "edges", "title": "contextual proximity", "to": "position", "width": 0.5}, {"from": "edges", "title": "As different edges represent different relations, \u03d5 is a Relational GNN which uses different weights for each edge type.", "to": "relationships", "width": 1.0}, {"from": "edges", "title": "contextual proximity", "to": "remote relations", "width": 0.5}, {"from": "edges", "title": "Edges represent different spatial relations", "to": "spatial relations", "width": 1.0}, {"from": "edges", "title": "contextual proximity", "to": "state encoder", "width": 0.5}, {"from": "edges", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "edges", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "edges eij", "title": "contextual proximity", "to": "embedding vector", "width": 0.5}, {"from": "edges eij", "title": "Edges eij \u2208 Eij represent spatial relationships and are defined following (Jiang et al. 2021).", "to": "spatial relationships", "width": 1.0}, {"from": "efficiency irrational agent", "title": "IRENE outperforms BC-MLP by 16 % in the Efficiency Irrational Agent task,contextual proximity", "to": "irene", "width": 4.25}, {"from": "efficiency irrational agent", "title": "contextual proximity", "to": "relational graphs", "width": 0.5}, {"from": "efficient", "title": "contextual proximity", "to": "irene", "width": 0.75}, {"from": "efficient", "title": "Rational agents move efficiently towards their goal,contextual proximity", "to": "rational agents", "width": 1.75}, {"from": "efficient", "title": "contextual proximity", "to": "training tasks", "width": 0.5}, {"from": "efficient action", "title": "contextual proximity", "to": "encoded states", "width": 0.5}, {"from": "efficient action", "title": "contextual proximity", "to": "grid-world environment", "width": 0.5}, {"from": "efficient action", "title": "contextual proximity", "to": "instrumental action", "width": 0.5}, {"from": "efficient action", "title": "This results in an improvement in the Efficient Action task of 6.6 % with respect to the previous best model (BC-MLP),contextual proximity", "to": "irene", "width": 4.5}, {"from": "efficient action", "title": "contextual proximity", "to": "objects", "width": 0.5}, {"from": "efficient action", "title": "contextual proximity", "to": "relational graphs", "width": 0.5}, {"from": "efficient action", "title": "contextual proximity", "to": "state encoder", "width": 0.5}, {"from": "efficient action", "title": "contextual proximity", "to": "sub-tasks", "width": 0.5}, {"from": "efficient action", "title": "contextual proximity", "to": "tasks", "width": 0.75}, {"from": "efficient action", "title": "contextual proximity", "to": "trials", "width": 0.5}, {"from": "efficient action task", "title": "The improved score on the Efficient Action task suggests that IRENE can also better model rational agents\u2019 behaviour.,contextual proximity", "to": "irene", "width": 2.0}, {"from": "efficient action task", "title": "contextual proximity", "to": "models", "width": 0.5}, {"from": "ekta sood", "title": "Both are mentioned in the same sentence, indicating a potential relationship,contextual proximity", "to": "hsiu-yu yang", "width": 3.25}, {"from": "ekta sood", "title": "contextual proximity", "to": "manuel mager", "width": 2.25}, {"from": "ekta sood", "title": "contextual proximity", "to": "m\u00a8uller", "width": 0.75}, {"from": "embedding vector", "title": "contextual proximity", "to": "encoded states", "width": 0.5}, {"from": "embedding vector", "title": "contextual proximity", "to": "encoder", "width": 0.5}, {"from": "embedding vector", "title": "contextual proximity", "to": "graph-sage layers", "width": 0.5}, {"from": "embedding vector", "title": "contextual proximity", "to": "lstm aggregation", "width": 0.5}, {"from": "embedding vector", "title": "contextual proximity", "to": "relationships", "width": 0.5}, {"from": "embedding vector", "title": "contextual proximity", "to": "spatial relationships", "width": 0.5}, {"from": "embedding vector", "title": "In the context encoder, the state encoder outputs an embedding vector hij for each frame graph Gij, obtained by applying average pooling to the nodes.,contextual proximity", "to": "state encoder", "width": 1.5}, {"from": "embedding vector", "title": "contextual proximity", "to": "test state", "width": 0.5}, {"from": "embedding vector", "title": "contextual proximity", "to": "transformer encoder", "width": 0.5}, {"from": "embedding vector", "title": "contextual proximity", "to": "trial representations", "width": 0.5}, {"from": "encoded states", "title": "contextual proximity", "to": "feature fusion module", "width": 0.5}, {"from": "encoded states", "title": "contextual proximity", "to": "grid-world environment", "width": 1.0}, {"from": "encoded states", "title": "contextual proximity", "to": "instrumental action", "width": 0.5}, {"from": "encoded states", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "encoded states", "title": "contextual proximity", "to": "objects", "width": 1.0}, {"from": "encoded states", "title": "contextual proximity", "to": "state encoder", "width": 1.25}, {"from": "encoded states", "title": "contextual proximity", "to": "sub-tasks", "width": 1.0}, {"from": "encoded states", "title": "contextual proximity", "to": "tasks", "width": 1.5}, {"from": "encoded states", "title": "contextual proximity", "to": "trajectories", "width": 0.5}, {"from": "encoded states", "title": "contextual proximity", "to": "transformer context encoder", "width": 0.5}, {"from": "encoded states", "title": "contextual proximity", "to": "trials", "width": 1.0}, {"from": "errors", "title": "contextual proximity", "to": "feature fusion module", "width": 0.5}, {"from": "errors", "title": "contextual proximity", "to": "gandhi et al.", "width": 0.75}, {"from": "errors", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "errors", "title": "contextual proximity", "to": "mlp policy", "width": 0.5}, {"from": "errors", "title": "contextual proximity", "to": "prediction net", "width": 0.5}, {"from": "errors", "title": "The errors for all evaluations are reported,contextual proximity", "to": "results", "width": 1.5}, {"from": "errors", "title": "contextual proximity", "to": "state encoder", "width": 0.75}, {"from": "errors", "title": "contextual proximity", "to": "transformer encoder", "width": 0.5}, {"from": "evaluating language processing", "title": "Research has focused on evaluating language processing, indicating a relationship between the two concepts.,contextual proximity", "to": "research", "width": 1.5}, {"from": "evaluation", "title": "contextual proximity", "to": "features", "width": 1.0}, {"from": "evaluation", "title": "contextual proximity", "to": "frames", "width": 0.5}, {"from": "evaluation", "title": "contextual proximity", "to": "graphs", "width": 0.75}, {"from": "evaluation", "title": "contextual proximity", "to": "model", "width": 0.5}, {"from": "evaluation", "title": "contextual proximity", "to": "position", "width": 0.5}, {"from": "evaluation", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "evaluation", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "evaluation", "title": "Evaluation employs a violation of expectation paradigm", "to": "violation of expectation paradigm", "width": 1.0}, {"from": "evaluation performance", "title": "contextual proximity", "to": "graphsage", "width": 1.0}, {"from": "evaluation performance", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "evaluation performance", "title": "contextual proximity", "to": "lstm", "width": 0.5}, {"from": "evaluation performance", "title": "influence of training tasks on evaluation performance,The influence of training tasks on evaluation performance is investigated.,contextual proximity", "to": "training tasks", "width": 3.0}, {"from": "evaluation set", "title": "contextual proximity", "to": "preference task", "width": 0.5}, {"from": "evaluation set", "title": "contextual proximity", "to": "tasks", "width": 0.5}, {"from": "evaluation tasks", "title": "contextual proximity", "to": "feature fusion module", "width": 0.5}, {"from": "evaluation tasks", "title": "contextual proximity", "to": "gandhi et al.", "width": 0.75}, {"from": "evaluation tasks", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "evaluation tasks", "title": "contextual proximity", "to": "mlp policy", "width": 0.5}, {"from": "evaluation tasks", "title": "contextual proximity", "to": "prediction net", "width": 0.5}, {"from": "evaluation tasks", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "evaluation tasks", "title": "contextual proximity", "to": "state encoder", "width": 0.75}, {"from": "evaluation tasks", "title": "contextual proximity", "to": "transformer encoder", "width": 0.5}, {"from": "evaluation tasks", "title": "VoE accuracy scores on all evaluation tasks are compared", "to": "voe accuracy scores", "width": 1.0}, {"from": "evaluation trials", "title": "contextual proximity", "to": "graph generation", "width": 0.5}, {"from": "evaluation trials", "title": "Instrumental Blocking Barrier is related to evaluation trials as the context mentions how the object is confined in evaluation trials, unlike in training trials.,contextual proximity", "to": "instrumental blocking barrier", "width": 1.5}, {"from": "evaluation trials", "title": "contextual proximity", "to": "single-object multiple-agent", "width": 0.5}, {"from": "existing models", "title": "contextual proximity", "to": "generalisation", "width": 0.5}, {"from": "existing models", "title": "contextual proximity", "to": "graph neural network (gnn)", "width": 1.5}, {"from": "existing models", "title": "contextual proximity", "to": "infants\u2019 responses", "width": 0.5}, {"from": "existing models", "title": "contextual proximity", "to": "intuitive reasoning network (irene)", "width": 1.0}, {"from": "existing models", "title": "contextual proximity", "to": "irene", "width": 1.0}, {"from": "existing models", "title": "The model performs particularly well on tasks that existing models struggle with as mentioned in the context.,contextual proximity", "to": "model", "width": 2.5}, {"from": "existing models", "title": "contextual proximity", "to": "neural network", "width": 0.5}, {"from": "existing models", "title": "contextual proximity", "to": "rational agents", "width": 0.5}, {"from": "existing models", "title": "contextual proximity", "to": "reasoning tasks", "width": 1.0}, {"from": "existing models", "title": "contextual proximity", "to": "theory of mind network", "width": 0.5}, {"from": "existing models", "title": "contextual proximity", "to": "training tasks", "width": 0.5}, {"from": "existing models", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "existing models", "title": "contextual proximity", "to": "video frames", "width": 0.5}, {"from": "existing models", "title": "contextual proximity", "to": "video transformer", "width": 1.0}, {"from": "existing models", "title": "contextual proximity", "to": "vt", "width": 2.0}, {"from": "existing models (gandhi et al. 2021; hein and diepold 2022)", "title": "IRENE relies less on heuristics, such as directly moving towards the goal object compared to existing models,contextual proximity", "to": "irene", "width": 1.5}, {"from": "expected events or situations", "title": "Gandhi et al. and expected events or situations are related as they are mentioned together in the context.,contextual proximity", "to": "gandhi et al.", "width": 2.5}, {"from": "expected events or situations", "title": "contextual proximity", "to": "intuitive physics", "width": 2.0}, {"from": "expected events or situations", "title": "contextual proximity", "to": "intuitive psychology", "width": 1.5}, {"from": "expected events or situations", "title": "contextual proximity", "to": "piloto et al.", "width": 1.0}, {"from": "expected events or situations", "title": "Riochet et al. and expected events or situations are related as they are mentioned together in the context.,contextual proximity", "to": "riochet et al.", "width": 2.0}, {"from": "expected events or situations", "title": "contextual proximity", "to": "shu et al.", "width": 1.0}, {"from": "expected events or situations", "title": "contextual proximity", "to": "violation of expectation paradigm", "width": 0.5}, {"from": "expected events or situations", "title": "contextual proximity", "to": "weihs et al.", "width": 0.5}, {"from": "expected outcome", "title": "Expectedness is defined as the observer model\u2019s prediction error: a model is successful if the prediction error on the unexpected outcome is higher than the error on the expected outcome.", "to": "observer model", "width": 1.0}, {"from": "expected outcome", "title": "contextual proximity", "to": "preference task", "width": 0.5}, {"from": "expected outcome", "title": "contextual proximity", "to": "tasks", "width": 0.5}, {"from": "expected trials", "title": "In expected trials, the preferred object is located differently than in familiarisation trials but the agent still moves towards it.,contextual proximity", "to": "preference task", "width": 1.5}, {"from": "expected trials", "title": "contextual proximity", "to": "tasks", "width": 0.5}, {"from": "expectedness", "title": "Gandhi et al. define expectedness in terms of the mean square error between the model prediction and the ground-truth,contextual proximity", "to": "gandhi et al.", "width": 1.5}, {"from": "expectedness", "title": "contextual proximity", "to": "hbtom", "width": 0.5}, {"from": "expectedness", "title": "contextual proximity", "to": "vt", "width": 0.5}, {"from": "familiarisation", "title": "contextual proximity", "to": "preference task", "width": 0.5}, {"from": "familiarisation", "title": "contextual proximity", "to": "tasks", "width": 0.5}, {"from": "familiarisation", "title": "Familiarisation and test trials follow different distributions: expected and unexpected trials are perceptually and conceptually different from familiarisation trials, respectively.", "to": "test trials", "width": 1.0}, {"from": "fast and accurate deep network learning", "title": "The concept of \u0027Fast and Accurate Deep Network Learning\u0027 is related to \u0027Exponential Linear Units\u0027 as discussed in the paper \u0027Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\u0027 by Clevert, D.-A. ; Unterthiner, T.; and Hochreiter, S.", "to": "exponential linear units", "width": 1.0}, {"from": "feature fusion module", "title": "contextual proximity", "to": "gandhi et al.", "width": 1.5}, {"from": "feature fusion module", "title": "contextual proximity", "to": "gnn", "width": 0.75}, {"from": "feature fusion module", "title": "contextual proximity", "to": "graphsage layers", "width": 0.5}, {"from": "feature fusion module", "title": "contextual proximity", "to": "grid-world environment", "width": 0.5}, {"from": "feature fusion module", "title": "contextual proximity", "to": "hein et al.", "width": 0.5}, {"from": "feature fusion module", "title": "contextual proximity", "to": "hidden dimension 96", "width": 0.5}, {"from": "feature fusion module", "title": "contextual proximity", "to": "hidden dimensions 256, 128 and 256", "width": 0.5}, {"from": "feature fusion module", "title": "The feature fusion module is used by IRENE for encoding the node features,contextual proximity", "to": "irene", "width": 2.25}, {"from": "feature fusion module", "title": "contextual proximity", "to": "local directional relations", "width": 0.5}, {"from": "feature fusion module", "title": "contextual proximity", "to": "mean prediction error", "width": 0.5}, {"from": "feature fusion module", "title": "contextual proximity", "to": "mlp policy", "width": 1.0}, {"from": "feature fusion module", "title": "contextual proximity", "to": "node features", "width": 0.5}, {"from": "feature fusion module", "title": "contextual proximity", "to": "objects", "width": 0.5}, {"from": "feature fusion module", "title": "contextual proximity", "to": "output dimension two", "width": 0.5}, {"from": "feature fusion module", "title": "The prediction net uses the same feature fusion module as the context encoder,contextual proximity", "to": "prediction net", "width": 2.25}, {"from": "feature fusion module", "title": "contextual proximity", "to": "remote directional relations", "width": 0.5}, {"from": "feature fusion module", "title": "contextual proximity", "to": "results", "width": 1.0}, {"from": "feature fusion module", "title": "contextual proximity", "to": "stack of six layers", "width": 0.5}, {"from": "feature fusion module", "title": "The state encoder includes a feature fusion module that combines node features, showing a specific relationship within the state encoder component.,contextual proximity", "to": "state encoder", "width": 3.0}, {"from": "feature fusion module", "title": "contextual proximity", "to": "sub-tasks", "width": 0.5}, {"from": "feature fusion module", "title": "contextual proximity", "to": "tasks", "width": 0.75}, {"from": "feature fusion module", "title": "contextual proximity", "to": "transformer encoder", "width": 1.0}, {"from": "feature fusion module", "title": "contextual proximity", "to": "trials", "width": 0.5}, {"from": "feature fusion module", "title": "contextual proximity", "to": "video-rnn", "width": 0.5}, {"from": "feature fusion module", "title": "contextual proximity", "to": "voe accuracy scores", "width": 0.5}, {"from": "feature fusion module", "title": "contextual proximity", "to": "vt model", "width": 0.5}, {"from": "features", "title": "contextual proximity", "to": "frames", "width": 2.0}, {"from": "features", "title": "contextual proximity", "to": "gandhi et al. 2021", "width": 1.0}, {"from": "features", "title": "contextual proximity", "to": "gnn", "width": 1.0}, {"from": "features", "title": "contextual proximity", "to": "graph", "width": 1.0}, {"from": "features", "title": "contextual proximity", "to": "graphs", "width": 3.0}, {"from": "features", "title": "contextual proximity", "to": "grid-world environment", "width": 1.0}, {"from": "features", "title": "contextual proximity", "to": "ground truth", "width": 1.0}, {"from": "features", "title": "contextual proximity", "to": "message passing", "width": 1.0}, {"from": "features", "title": "contextual proximity", "to": "model", "width": 2.0}, {"from": "features", "title": "contextual proximity", "to": "normalised between [-1, 1]", "width": 1.0}, {"from": "features", "title": "contextual proximity", "to": "normalised between [0, 1]", "width": 1.0}, {"from": "features", "title": "contextual proximity", "to": "objects", "width": 1.0}, {"from": "features", "title": "contextual proximity", "to": "one-hot vectors", "width": 1.0}, {"from": "features", "title": "contextual proximity", "to": "position", "width": 2.0}, {"from": "features", "title": "contextual proximity", "to": "shape", "width": 1.0}, {"from": "features", "title": "contextual proximity", "to": "spatial relations", "width": 1.0}, {"from": "features", "title": "contextual proximity", "to": "states", "width": 1.0}, {"from": "features", "title": "contextual proximity", "to": "training and evaluation on the baby intuitions benchmark", "width": 1.0}, {"from": "features", "title": "contextual proximity", "to": "transformer", "width": 2.0}, {"from": "features", "title": "contextual proximity", "to": "type", "width": 2.0}, {"from": "features", "title": "contextual proximity", "to": "violation of expectation paradigm", "width": 1.0}, {"from": "features", "title": "contextual proximity", "to": "weights", "width": 1.0}, {"from": "final scores", "title": "contextual proximity", "to": "local directional relations", "width": 0.75}, {"from": "final scores", "title": "contextual proximity", "to": "performance", "width": 0.75}, {"from": "final scores", "title": "remote relations contribute more to the final scores than local ones,contextual proximity", "to": "remote relations", "width": 1.5}, {"from": "frame", "title": "Graph Generation is related to frame as graphs are built from sampled frames in the context, forming nodes and edges based on the entities present in each frame.,contextual proximity", "to": "graph generation", "width": 1.5}, {"from": "frame", "title": "contextual proximity", "to": "instrumental blocking barrier", "width": 0.5}, {"from": "frame", "title": "contextual proximity", "to": "single-object multiple-agent", "width": 0.5}, {"from": "frames", "title": "contextual proximity", "to": "gandhi et al. 2021", "width": 0.5}, {"from": "frames", "title": "contextual proximity", "to": "gnn", "width": 0.5}, {"from": "frames", "title": "Graphs are built from frames,contextual proximity", "to": "graph", "width": 1.5}, {"from": "frames", "title": "Graphs are built from frames,contextual proximity", "to": "graphs", "width": 2.5}, {"from": "frames", "title": "contextual proximity", "to": "grid-world environment", "width": 0.5}, {"from": "frames", "title": "contextual proximity", "to": "ground truth", "width": 0.5}, {"from": "frames", "title": "contextual proximity", "to": "message passing", "width": 0.5}, {"from": "frames", "title": "contextual proximity", "to": "model", "width": 1.0}, {"from": "frames", "title": "contextual proximity", "to": "normalised between [-1, 1]", "width": 0.5}, {"from": "frames", "title": "contextual proximity", "to": "normalised between [0, 1]", "width": 0.5}, {"from": "frames", "title": "contextual proximity", "to": "objects", "width": 0.5}, {"from": "frames", "title": "contextual proximity", "to": "one-hot vectors", "width": 0.5}, {"from": "frames", "title": "contextual proximity", "to": "position", "width": 1.0}, {"from": "frames", "title": "contextual proximity", "to": "shape", "width": 0.5}, {"from": "frames", "title": "contextual proximity", "to": "spatial relations", "width": 0.5}, {"from": "frames", "title": "contextual proximity", "to": "states", "width": 0.5}, {"from": "frames", "title": "contextual proximity", "to": "training and evaluation on the baby intuitions benchmark", "width": 0.5}, {"from": "frames", "title": "contextual proximity", "to": "transformer", "width": 1.0}, {"from": "frames", "title": "contextual proximity", "to": "type", "width": 1.0}, {"from": "frames", "title": "contextual proximity", "to": "violation of expectation paradigm", "width": 0.5}, {"from": "frames", "title": "contextual proximity", "to": "weights", "width": 0.5}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "gnn", "width": 1.25}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "graphsage layers", "width": 0.75}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "hbtom", "width": 1.0}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "hein et al.", "width": 1.25}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "hidden dimension 96", "width": 0.75}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "hidden dimensions 256, 128 and 256", "width": 0.75}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "intuitive physics", "width": 3.0}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "intuitive psychology", "width": 2.25}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "irene", "width": 1.5}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "logistic regression classifiers", "width": 0.5}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "mean prediction error", "width": 0.75}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "mlp policy", "width": 1.5}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "output dimension two", "width": 0.75}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "piloto et al.", "width": 1.5}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "prediction net", "width": 1.5}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "results", "width": 1.5}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "riochet et al.", "width": 1.5}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "shu et al.", "width": 1.5}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "stack of six layers", "width": 0.75}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "state encoder", "width": 2.25}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "state-action pairs", "width": 0.5}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "test trial", "width": 0.5}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "test trials", "width": 0.5}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "transformer encoder", "width": 1.5}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "video frames", "width": 0.5}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "video-rnn", "width": 0.75}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "violation of expectation paradigm", "width": 0.75}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "voe accuracy scores", "width": 0.75}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "voe paradigm", "width": 1.0}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "vt", "width": 1.0}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "vt model", "width": 0.75}, {"from": "gandhi et al.", "title": "contextual proximity", "to": "weihs et al.", "width": 0.75}, {"from": "gandhi et al. (2021)", "title": "contextual proximity", "to": "irene", "width": 0.75}, {"from": "gandhi et al. (2021)", "title": "contextual proximity", "to": "rational agents", "width": 0.75}, {"from": "gandhi et al. (2021)", "title": "contextual proximity", "to": "training tasks", "width": 0.5}, {"from": "gandhi et al. 2021", "title": "contextual proximity", "to": "graphs", "width": 0.75}, {"from": "gandhi et al. 2021", "title": "contextual proximity", "to": "model", "width": 0.5}, {"from": "gandhi et al. 2021", "title": "contextual proximity", "to": "position", "width": 0.5}, {"from": "gandhi et al. 2021", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "gandhi et al. 2021", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "gandhi et al. 2021", "title": "The training and evaluation is based on the Baby Intuitions Benchmark by Gandhi et al. 2021", "to": "training and evaluation on the baby intuitions benchmark", "width": 1.0}, {"from": "gcn", "title": "GCN substitutes GraphSAGE with GCN layers", "to": "graphsage", "width": 1.0}, {"from": "gcn", "title": "contextual proximity", "to": "irene", "width": 1.25}, {"from": "gcn", "title": "contextual proximity", "to": "model", "width": 0.75}, {"from": "gcn", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "general ability of ai systems to reason about unexpected events or situations", "title": "contextual proximity", "to": "research", "width": 0.5}, {"from": "general neural common-sense reasoners", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "general neural common-sense reasoners", "title": "contextual proximity", "to": "lack of accessible and well-maintained benchmarks", "width": 0.5}, {"from": "generalisation", "title": "contextual proximity", "to": "graph neural network (gnn)", "width": 0.75}, {"from": "generalisation", "title": "contextual proximity", "to": "intuitive reasoning network (irene)", "width": 0.5}, {"from": "generalisation", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "generalisation", "title": "contextual proximity", "to": "model", "width": 0.75}, {"from": "generalisation", "title": "contextual proximity", "to": "reasoning tasks", "width": 0.5}, {"from": "generalisation", "title": "contextual proximity", "to": "video transformer", "width": 0.5}, {"from": "generalisation", "title": "contextual proximity", "to": "vt", "width": 1.0}, {"from": "generalisation", "title": "The choice of training tasks is investigated on generalisation as mentioned in the context.", "to": "training tasks", "width": 1.0}, {"from": "gnn", "title": "contextual proximity", "to": "graphs", "width": 0.75}, {"from": "gnn", "title": "contextual proximity", "to": "hbtom", "width": 0.5}, {"from": "gnn", "title": "contextual proximity", "to": "hein et al.", "width": 0.5}, {"from": "gnn", "title": "IRENE combines a GNN and a transformer to learn rich state and context representations,contextual proximity", "to": "irene", "width": 4.0}, {"from": "gnn", "title": "contextual proximity", "to": "local directional relations", "width": 0.5}, {"from": "gnn", "title": "contextual proximity", "to": "message passing", "width": 0.5}, {"from": "gnn", "title": "contextual proximity", "to": "mlp policy", "width": 0.75}, {"from": "gnn", "title": "contextual proximity", "to": "model", "width": 0.5}, {"from": "gnn", "title": "contextual proximity", "to": "node embeddings", "width": 0.5}, {"from": "gnn", "title": "contextual proximity", "to": "node features", "width": 0.5}, {"from": "gnn", "title": "contextual proximity", "to": "performance", "width": 0.5}, {"from": "gnn", "title": "contextual proximity", "to": "position", "width": 0.5}, {"from": "gnn", "title": "The prediction net uses the same GNN as the feature fusion module used in the context encoder,contextual proximity", "to": "prediction net", "width": 1.75}, {"from": "gnn", "title": "contextual proximity", "to": "remote directional relations", "width": 0.5}, {"from": "gnn", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "gnn", "title": "contextual proximity", "to": "state embedding", "width": 0.5}, {"from": "gnn", "title": "contextual proximity", "to": "state encoder", "width": 1.0}, {"from": "gnn", "title": "A novel method that uses a GNN (Gori, Monfardini,state-action pairs \u03c4i = {(sij, aij)}j=1,...,T , with sij being video frames and T the trial length", "to": "state-action pairs", "width": 1.0}, {"from": "gnn", "title": "contextual proximity", "to": "training tasks", "width": 0.5}, {"from": "gnn", "title": "contextual proximity", "to": "transformer", "width": 0.75}, {"from": "gnn", "title": "contextual proximity", "to": "transformer encoder", "width": 0.5}, {"from": "gnn", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "gnn", "title": "contextual proximity", "to": "vt", "width": 0.5}, {"from": "graph", "title": "contextual proximity", "to": "graphs", "width": 0.75}, {"from": "graph", "title": "contextual proximity", "to": "model", "width": 0.5}, {"from": "graph", "title": "contextual proximity", "to": "position", "width": 0.5}, {"from": "graph", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "graph", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "graph convolutional networks", "title": "Graph Convolutional Networks are used for modeling Relational Data", "to": "relational data", "width": 1.0}, {"from": "graph generation", "title": "contextual proximity", "to": "instrumental blocking barrier", "width": 1.0}, {"from": "graph generation", "title": "contextual proximity", "to": "json file", "width": 0.5}, {"from": "graph generation", "title": "contextual proximity", "to": "navigation preference", "width": 0.5}, {"from": "graph generation", "title": "contextual proximity", "to": "single-object multiple-agent", "width": 1.0}, {"from": "graph generation", "title": "contextual proximity", "to": "training trials", "width": 0.5}, {"from": "graph neural network", "title": "IRENE combines a graph neural network for learning agent and world state representations.,contextual proximity", "to": "intuitive reasoning network (irene)", "width": 1.75}, {"from": "graph neural network", "title": "contextual proximity", "to": "irene", "width": 0.75}, {"from": "graph neural network (gnn)", "title": "contextual proximity", "to": "infants\u2019 responses", "width": 0.75}, {"from": "graph neural network (gnn)", "title": "IRENE uses a graph neural network (GNN) to obtain rich state embeddings as mentioned in the context.,contextual proximity", "to": "intuitive reasoning network (irene)", "width": 2.5}, {"from": "graph neural network (gnn)", "title": "contextual proximity", "to": "irene", "width": 1.5}, {"from": "graph neural network (gnn)", "title": "contextual proximity", "to": "model", "width": 2.25}, {"from": "graph neural network (gnn)", "title": "contextual proximity", "to": "neural network", "width": 0.75}, {"from": "graph neural network (gnn)", "title": "contextual proximity", "to": "rational agents", "width": 0.75}, {"from": "graph neural network (gnn)", "title": "contextual proximity", "to": "reasoning tasks", "width": 1.5}, {"from": "graph neural network (gnn)", "title": "contextual proximity", "to": "theory of mind network", "width": 0.75}, {"from": "graph neural network (gnn)", "title": "contextual proximity", "to": "training tasks", "width": 0.75}, {"from": "graph neural network (gnn)", "title": "contextual proximity", "to": "transformer", "width": 0.75}, {"from": "graph neural network (gnn)", "title": "contextual proximity", "to": "video frames", "width": 0.75}, {"from": "graph neural network (gnn)", "title": "contextual proximity", "to": "video transformer", "width": 1.5}, {"from": "graph neural network (gnn)", "title": "contextual proximity", "to": "vt", "width": 3.0}, {"from": "graph relations", "title": "We trained IRENE on graphs whose edges represent only local or remote directional relations,contextual proximity", "to": "irene", "width": 4.25}, {"from": "graph relations", "title": "contextual proximity", "to": "relational graphs", "width": 0.5}, {"from": "graph-sage layers", "title": "In particular, we use Graph-SAGE layers with LSTM aggregation.", "to": "lstm aggregation", "width": 1.0}, {"from": "graphs", "title": "contextual proximity", "to": "grid-world environment", "width": 0.75}, {"from": "graphs", "title": "contextual proximity", "to": "ground truth", "width": 0.75}, {"from": "graphs", "title": "contextual proximity", "to": "local directional relations", "width": 0.75}, {"from": "graphs", "title": "contextual proximity", "to": "message passing", "width": 0.75}, {"from": "graphs", "title": "contextual proximity", "to": "model", "width": 1.5}, {"from": "graphs", "title": "contextual proximity", "to": "normalised between [-1, 1]", "width": 0.75}, {"from": "graphs", "title": "contextual proximity", "to": "normalised between [0, 1]", "width": 0.75}, {"from": "graphs", "title": "contextual proximity", "to": "objects", "width": 0.75}, {"from": "graphs", "title": "contextual proximity", "to": "one-hot vectors", "width": 0.75}, {"from": "graphs", "title": "contextual proximity", "to": "performance", "width": 0.75}, {"from": "graphs", "title": "contextual proximity", "to": "position", "width": 1.5}, {"from": "graphs", "title": "contextual proximity", "to": "remote relations", "width": 0.5}, {"from": "graphs", "title": "contextual proximity", "to": "shape", "width": 0.75}, {"from": "graphs", "title": "contextual proximity", "to": "spatial relations", "width": 0.75}, {"from": "graphs", "title": "contextual proximity", "to": "states", "width": 0.75}, {"from": "graphs", "title": "contextual proximity", "to": "training and evaluation on the baby intuitions benchmark", "width": 0.75}, {"from": "graphs", "title": "contextual proximity", "to": "transformer", "width": 1.5}, {"from": "graphs", "title": "contextual proximity", "to": "type", "width": 1.5}, {"from": "graphs", "title": "contextual proximity", "to": "violation of expectation paradigm", "width": 0.75}, {"from": "graphs", "title": "contextual proximity", "to": "weights", "width": 0.75}, {"from": "graphs", "title": "IRENE is trained on graphs", "to": "irene", "width": 1.0}, {"from": "graphsage", "title": "contextual proximity", "to": "improving performance", "width": 1.0}, {"from": "graphsage", "title": "contextual proximity", "to": "irene", "width": 2.25}, {"from": "graphsage", "title": "contextual proximity", "to": "local directional relations", "width": 0.75}, {"from": "graphsage", "title": "contextual proximity", "to": "lstm", "width": 2.25}, {"from": "graphsage", "title": "contextual proximity", "to": "model", "width": 0.75}, {"from": "graphsage", "title": "contextual proximity", "to": "obstacles", "width": 1.25}, {"from": "graphsage", "title": "contextual proximity", "to": "performance", "width": 1.75}, {"from": "graphsage", "title": "contextual proximity", "to": "preferences", "width": 1.0}, {"from": "graphsage", "title": "contextual proximity", "to": "rational agents", "width": 0.75}, {"from": "graphsage", "title": "contextual proximity", "to": "remote relations", "width": 0.5}, {"from": "graphsage", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "graphsage", "title": "contextual proximity", "to": "training tasks", "width": 1.5}, {"from": "graphsage", "title": "contextual proximity", "to": "transformer", "width": 1.5}, {"from": "graphsage", "title": "contextual proximity", "to": "transformer encoder", "width": 1.0}, {"from": "graphsage layers", "title": "contextual proximity", "to": "irene", "width": 0.75}, {"from": "graphsage layers", "title": "contextual proximity", "to": "local directional relations", "width": 0.75}, {"from": "graphsage layers", "title": "contextual proximity", "to": "mlp policy", "width": 0.5}, {"from": "graphsage layers", "title": "contextual proximity", "to": "performance", "width": 0.75}, {"from": "graphsage layers", "title": "contextual proximity", "to": "prediction net", "width": 0.5}, {"from": "graphsage layers", "title": "contextual proximity", "to": "remote relations", "width": 0.5}, {"from": "graphsage layers", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "graphsage layers", "title": "The state encoder consists of two GraphSAGE layers for each relation,We replaced the GraphSAGE layers with GCN layers in the state encoder,contextual proximity", "to": "state encoder", "width": 3.0}, {"from": "graphsage layers", "title": "contextual proximity", "to": "transformer encoder", "width": 0.5}, {"from": "grid-world environment", "title": "contextual proximity", "to": "instrumental action", "width": 0.5}, {"from": "grid-world environment", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "grid-world environment", "title": "contextual proximity", "to": "model", "width": 0.5}, {"from": "grid-world environment", "title": "contextual proximity", "to": "objects", "width": 1.25}, {"from": "grid-world environment", "title": "contextual proximity", "to": "position", "width": 0.5}, {"from": "grid-world environment", "title": "contextual proximity", "to": "state encoder", "width": 1.0}, {"from": "grid-world environment", "title": "contextual proximity", "to": "sub-tasks", "width": 1.0}, {"from": "grid-world environment", "title": "contextual proximity", "to": "tasks", "width": 1.5}, {"from": "grid-world environment", "title": "contextual proximity", "to": "trajectories", "width": 0.5}, {"from": "grid-world environment", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "grid-world environment", "title": "contextual proximity", "to": "transformer context encoder", "width": 0.5}, {"from": "grid-world environment", "title": "contextual proximity", "to": "trials", "width": 1.0}, {"from": "grid-world environment", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "ground truth", "title": "The model\u0027s weights are updated based on the ground truth of the test trial,contextual proximity", "to": "model", "width": 1.5}, {"from": "ground truth", "title": "contextual proximity", "to": "position", "width": 0.5}, {"from": "ground truth", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "ground truth", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "hbtom", "title": "contextual proximity", "to": "hein et al.", "width": 0.5}, {"from": "hbtom", "title": "contextual proximity", "to": "logistic regression classifiers", "width": 0.5}, {"from": "hbtom", "title": "contextual proximity", "to": "state-action pairs", "width": 0.5}, {"from": "hbtom", "title": "contextual proximity", "to": "test trial", "width": 0.5}, {"from": "hbtom", "title": "contextual proximity", "to": "video frames", "width": 0.5}, {"from": "hbtom", "title": "contextual proximity", "to": "voe paradigm", "width": 0.5}, {"from": "hbtom", "title": "contextual proximity", "to": "vt", "width": 1.0}, {"from": "hein et al.", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "hein et al.", "title": "contextual proximity", "to": "mlp policy", "width": 0.5}, {"from": "hein et al.", "title": "contextual proximity", "to": "prediction net", "width": 0.5}, {"from": "hein et al.", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "hein et al.", "title": "contextual proximity", "to": "state encoder", "width": 0.75}, {"from": "hein et al.", "title": "contextual proximity", "to": "transformer encoder", "width": 0.5}, {"from": "hein et al.", "title": "contextual proximity", "to": "vt", "width": 0.5}, {"from": "hein et al.", "title": "Hein et al. proposed the VT model", "to": "vt model", "width": 1.0}, {"from": "hein, a.", "title": "Authors of the paper \u0027Comparing Intuitions about Agents\u2019 Goals, Preferences and Actions in Human Infants and Video Transformers\u0027", "to": "diepold, k.", "width": 1.0}, {"from": "hellaswag: can a machine really finish your sentence?", "title": "The first concept is the title of a paper presented in the Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.", "to": "proceedings of the 57th annual meeting of the association for computational linguistics", "width": 1.0}, {"from": "hendrycks, d.", "title": "Authors of the paper \u0027Gaussian Error Linear Units (GELUs)\u0027", "to": "gimpel, k.", "width": 1.0}, {"from": "heuristics", "title": "IRENE relies less on heuristics, such as directly moving towards the goal object.,contextual proximity", "to": "irene", "width": 2.0}, {"from": "heuristics", "title": "contextual proximity", "to": "models", "width": 0.5}, {"from": "hidden dimension 96", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "hidden dimension 96", "title": "contextual proximity", "to": "mlp policy", "width": 0.5}, {"from": "hidden dimension 96", "title": "contextual proximity", "to": "prediction net", "width": 0.5}, {"from": "hidden dimension 96", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "hidden dimension 96", "title": "The state encoder has a hidden dimension of 96,contextual proximity", "to": "state encoder", "width": 1.75}, {"from": "hidden dimension 96", "title": "contextual proximity", "to": "transformer encoder", "width": 0.5}, {"from": "hidden dimensions 256, 128 and 256", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "hidden dimensions 256, 128 and 256", "title": "The MLP policy has hidden dimensions of 256, 128, and 256,contextual proximity", "to": "mlp policy", "width": 1.5}, {"from": "hidden dimensions 256, 128 and 256", "title": "contextual proximity", "to": "prediction net", "width": 0.5}, {"from": "hidden dimensions 256, 128 and 256", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "hidden dimensions 256, 128 and 256", "title": "contextual proximity", "to": "state encoder", "width": 0.75}, {"from": "hidden dimensions 256, 128 and 256", "title": "contextual proximity", "to": "transformer encoder", "width": 0.5}, {"from": "hsiu-yu yang", "title": "contextual proximity", "to": "manuel mager", "width": 2.25}, {"from": "hsiu-yu yang", "title": "contextual proximity", "to": "m\u00a8uller", "width": 0.75}, {"from": "huang, l.", "title": "Authors of the paper \u0027Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning\u0027", "to": "le bras, r.", "width": 1.0}, {"from": "human-robot collaboration", "title": "Commonsense reasoning is essential for effective human-robot collaboration in smart manufacturing contexts.", "to": "commonsense reasoning", "width": 1.0}, {"from": "improving performance", "title": "contextual proximity", "to": "lstm", "width": 0.5}, {"from": "inaccessible goal", "title": "contextual proximity", "to": "inaccessible goal task", "width": 0.75}, {"from": "inaccessible goal", "title": "contextual proximity", "to": "inconsequential barrier", "width": 0.5}, {"from": "inaccessible goal", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "inaccessible goal", "title": "It degrades notably for Multi-Agent, Inaccessible Goal, and Efficient Action in most cases,contextual proximity", "to": "multi-agent", "width": 1.75}, {"from": "inaccessible goal", "title": "contextual proximity", "to": "no barrier", "width": 0.5}, {"from": "inaccessible goal", "title": "contextual proximity", "to": "p", "width": 0.5}, {"from": "inaccessible goal", "title": "contextual proximity", "to": "preference task", "width": 0.5}, {"from": "inaccessible goal", "title": "contextual proximity", "to": "s", "width": 0.75}, {"from": "inaccessible goal task", "title": "contextual proximity", "to": "inconsequential barrier", "width": 1.5}, {"from": "inaccessible goal task", "title": "contextual proximity", "to": "instrumental action", "width": 0.75}, {"from": "inaccessible goal task", "title": "contextual proximity", "to": "instrumental action task", "width": 0.75}, {"from": "inaccessible goal task", "title": "contextual proximity", "to": "irene", "width": 1.5}, {"from": "inaccessible goal task", "title": "contextual proximity", "to": "m", "width": 0.75}, {"from": "inaccessible goal task", "title": "contextual proximity", "to": "multi-agent", "width": 2.25}, {"from": "inaccessible goal task", "title": "contextual proximity", "to": "multi-agent task", "width": 0.75}, {"from": "inaccessible goal task", "title": "contextual proximity", "to": "no barrier", "width": 1.5}, {"from": "inaccessible goal task", "title": "contextual proximity", "to": "p", "width": 1.5}, {"from": "inaccessible goal task", "title": "contextual proximity", "to": "preference task", "width": 1.5}, {"from": "inaccessible goal task", "title": "contextual proximity", "to": "s", "width": 2.25}, {"from": "inconsequential barrier", "title": "contextual proximity", "to": "instrumental action", "width": 0.5}, {"from": "inconsequential barrier", "title": "contextual proximity", "to": "instrumental action task", "width": 0.5}, {"from": "inconsequential barrier", "title": "contextual proximity", "to": "irene", "width": 1.0}, {"from": "inconsequential barrier", "title": "contextual proximity", "to": "m", "width": 0.5}, {"from": "inconsequential barrier", "title": "contextual proximity", "to": "multi-agent", "width": 1.5}, {"from": "inconsequential barrier", "title": "contextual proximity", "to": "multi-agent task", "width": 0.5}, {"from": "inconsequential barrier", "title": "contextual proximity", "to": "no barrier", "width": 1.0}, {"from": "inconsequential barrier", "title": "contextual proximity", "to": "p", "width": 1.0}, {"from": "inconsequential barrier", "title": "contextual proximity", "to": "preference task", "width": 1.0}, {"from": "inconsequential barrier", "title": "Training on S improved scores on the Inconsequential Barrier sub-tasks,contextual proximity", "to": "s", "width": 2.5}, {"from": "inductive representation learning", "title": "Inductive representation learning is applied to large graphs, as discussed in the Advances in Neural Information Processing Systems.", "to": "large graphs", "width": 1.0}, {"from": "infants", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "infants", "title": "our model\u2019s expectations generally align with those of the infants, specifically for the Inaccessible Goal, Efficient Action, Inefficient Action, and Instrumental Action tasks", "to": "our model\u2019s expectations", "width": 1.0}, {"from": "infants expect other agents to have goals, preferences and engage in instrumental actions", "title": "contextual proximity", "to": "research", "width": 0.5}, {"from": "infants\u0027 looking times", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "infants\u0027 looking times", "title": "Comparison between the z-scored means of infants\u2019 looking times as collected by Stojni\u00b4c et al. (2023)", "to": "stojni\u00b4c et al. (2023)", "width": 1.0}, {"from": "infants\u2019 responses", "title": "contextual proximity", "to": "intuitive reasoning network (irene)", "width": 0.5}, {"from": "infants\u2019 responses", "title": "IRENE\u0027s predictions are in line with infants\u2019 responses collected on a subset of the BIB as mentioned in the context.,contextual proximity", "to": "irene", "width": 1.5}, {"from": "infants\u2019 responses", "title": "contextual proximity", "to": "model", "width": 0.75}, {"from": "infants\u2019 responses", "title": "contextual proximity", "to": "reasoning tasks", "width": 0.5}, {"from": "infants\u2019 responses", "title": "contextual proximity", "to": "video transformer", "width": 0.5}, {"from": "infants\u2019 responses", "title": "contextual proximity", "to": "vt", "width": 1.0}, {"from": "initial models", "title": "Initial models are based on the machine theory of mind network.", "to": "machine theory of mind network", "width": 1.0}, {"from": "initial models", "title": "contextual proximity", "to": "test trials", "width": 0.5}, {"from": "initial models", "title": "contextual proximity", "to": "voe paradigm", "width": 0.5}, {"from": "input", "title": "contextual proximity", "to": "irene", "width": 1.25}, {"from": "input", "title": "contextual proximity", "to": "model", "width": 0.75}, {"from": "input", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "input", "title": "Remote takes as input relational graphs", "to": "remote", "width": 1.0}, {"from": "instrumental action", "title": "Our model struggles the most in the Instrumental Action tasks (average 71.5),contextual proximity", "to": "irene", "width": 5.0}, {"from": "instrumental action", "title": "contextual proximity", "to": "multi-agent", "width": 1.0}, {"from": "instrumental action", "title": "contextual proximity", "to": "no barrier", "width": 0.5}, {"from": "instrumental action", "title": "contextual proximity", "to": "objects", "width": 0.5}, {"from": "instrumental action", "title": "contextual proximity", "to": "p", "width": 0.5}, {"from": "instrumental action", "title": "contextual proximity", "to": "preference task", "width": 0.5}, {"from": "instrumental action", "title": "contextual proximity", "to": "relational graphs", "width": 0.5}, {"from": "instrumental action", "title": "The score on the Instrumental Action task increased for selected combinations of training tasks (MPS, PS, MS, S), especially in MS and S,contextual proximity", "to": "s", "width": 1.75}, {"from": "instrumental action", "title": "contextual proximity", "to": "state encoder", "width": 0.5}, {"from": "instrumental action", "title": "contextual proximity", "to": "sub-tasks", "width": 0.5}, {"from": "instrumental action", "title": "contextual proximity", "to": "tasks", "width": 0.75}, {"from": "instrumental action", "title": "contextual proximity", "to": "trials", "width": 0.5}, {"from": "instrumental action task", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "instrumental action task", "title": "contextual proximity", "to": "multi-agent", "width": 0.75}, {"from": "instrumental action task", "title": "contextual proximity", "to": "no barrier", "width": 0.5}, {"from": "instrumental action task", "title": "contextual proximity", "to": "p", "width": 0.5}, {"from": "instrumental action task", "title": "contextual proximity", "to": "preference task", "width": 0.5}, {"from": "instrumental action task", "title": "contextual proximity", "to": "s", "width": 0.75}, {"from": "instrumental action tasks", "title": "contextual proximity", "to": "irene", "width": 1.0}, {"from": "instrumental action tasks", "title": "models that did not learn the role of barriers during training apply a simple heuristic \u2013 i.e. directly moving towards the goal object \u2013 that works on the Instrumental No Barrier and Inconsequential Barrier sub-tasks but not on the more challenging Blocking Barrier sub-task.,contextual proximity", "to": "models", "width": 1.5}, {"from": "instrumental blocking barrier", "title": "IRENE improves by 30 % on the previous best score in the Instrumental Blocking Barrier sub-task,contextual proximity", "to": "irene", "width": 4.25}, {"from": "instrumental blocking barrier", "title": "contextual proximity", "to": "json file", "width": 0.5}, {"from": "instrumental blocking barrier", "title": "contextual proximity", "to": "navigation preference", "width": 0.5}, {"from": "instrumental blocking barrier", "title": "contextual proximity", "to": "relational graphs", "width": 0.5}, {"from": "instrumental blocking barrier", "title": "contextual proximity", "to": "single-object multiple-agent", "width": 1.0}, {"from": "instrumental blocking barrier", "title": "contextual proximity", "to": "training trials", "width": 0.5}, {"from": "intelligent", "title": "contextual proximity", "to": "research", "width": 0.5}, {"from": "intentional stance", "title": "Taking the intentional stance is observed in infants at 12 months of age, according to the Cognition journal.", "to": "12 months of age", "width": 1.0}, {"from": "international conference on machine learning", "title": "contextual proximity", "to": "video transformer network", "width": 0.5}, {"from": "international conference on machine learning", "title": "Presented at", "to": "machine theory of mind", "width": 1.0}, {"from": "international conference on machine learning", "title": "The first concept is the title of a paper presented at the International Conference on Machine Learning.", "to": "rectified linear units improve restricted boltzmann machines", "width": 1.0}, {"from": "intphys: a framework and benchmark for visual intuitive physics reasoning", "title": "Published in", "to": "ieee transactions on pattern analysis and machine intelligence", "width": 1.0}, {"from": "intuitions about support in 4.5-month-old infants", "title": "The first concept is the title of a paper published in Cognition, volume 47, issue 2, pages 121\u2013148.", "to": "cognition, 47(2): 121\u2013148", "width": 1.0}, {"from": "intuitive physics", "title": "Modeling Expectation Violation in Intuitive Physics involves Coarse Probabilistic Object Representations", "to": "coarse probabilistic object representations", "width": 1.0}, {"from": "intuitive physics", "title": "contextual proximity", "to": "intuitive psychology", "width": 3.0}, {"from": "intuitive physics", "title": "Piloto et al. and intuitive physics are related as they are mentioned together in the context.,contextual proximity", "to": "piloto et al.", "width": 3.0}, {"from": "intuitive physics", "title": "contextual proximity", "to": "research", "width": 0.5}, {"from": "intuitive physics", "title": "Riochet et al. and intuitive physics are related as they are mentioned together in the context.,contextual proximity", "to": "riochet et al.", "width": 3.0}, {"from": "intuitive physics", "title": "contextual proximity", "to": "shu et al.", "width": 2.0}, {"from": "intuitive physics", "title": "contextual proximity", "to": "violation of expectation paradigm", "width": 1.0}, {"from": "intuitive physics", "title": "Weihs et al. and intuitive physics are related as they are mentioned together in the context.,contextual proximity", "to": "weihs et al.", "width": 2.0}, {"from": "intuitive physics learning in a deep-learning model inspired by developmental psychology", "title": "Published in", "to": "nature human behaviour", "width": 1.0}, {"from": "intuitive psychology", "title": "contextual proximity", "to": "irene", "width": 0.5}, {"from": "intuitive psychology", "title": "contextual proximity", "to": "lack of accessible and well-maintained benchmarks", "width": 0.5}, {"from": "intuitive psychology", "title": "Piloto et al. and intuitive psychology are related as they are mentioned together in the context.,contextual proximity", "to": "piloto et al.", "width": 2.5}, {"from": "intuitive psychology", "title": "contextual proximity", "to": "riochet et al.", "width": 1.5}, {"from": "intuitive psychology", "title": "Shu et al. and intuitive psychology are related as they are mentioned together in the context.,contextual proximity", "to": "shu et al.", "width": 2.5}, {"from": "intuitive psychology", "title": "contextual proximity", "to": "violation of expectation paradigm", "width": 0.75}, {"from": "intuitive psychology", "title": "contextual proximity", "to": "weihs et al.", "width": 0.75}, {"from": "intuitive reasoning network (irene)", "title": "contextual proximity", "to": "irene", "width": 3.25}, {"from": "intuitive reasoning network (irene)", "title": "contextual proximity", "to": "model", "width": 1.5}, {"from": "intuitive reasoning network (irene)", "title": "contextual proximity", "to": "neural model", "width": 0.75}, {"from": "intuitive reasoning network (irene)", "title": "contextual proximity", "to": "neural network", "width": 0.5}, {"from": "intuitive reasoning network (irene)", "title": "contextual proximity", "to": "rational agents", "width": 0.5}, {"from": "intuitive reasoning network (irene)", "title": "contextual proximity", "to": "reasoning tasks", "width": 1.0}, {"from": "intuitive reasoning network (irene)", "title": "contextual proximity", "to": "theory of mind network", "width": 0.5}, {"from": "intuitive reasoning network (irene)", "title": "contextual proximity", "to": "training tasks", "width": 0.5}, {"from": "intuitive reasoning network (irene)", "title": "contextual proximity", "to": "transformer", "width": 1.25}, {"from": "intuitive reasoning network (irene)", "title": "contextual proximity", "to": "video frames", "width": 0.5}, {"from": "intuitive reasoning network (irene)", "title": "contextual proximity", "to": "video transformer", "width": 1.0}, {"from": "intuitive reasoning network (irene)", "title": "contextual proximity", "to": "vt", "width": 2.0}, {"from": "irene", "title": "contextual proximity", "to": "irrational agent", "width": 0.75}, {"from": "irene", "title": "contextual proximity", "to": "irrational agents", "width": 0.75}, {"from": "irene", "title": "contextual proximity", "to": "knowledge", "width": 2.0}, {"from": "irene", "title": "contextual proximity", "to": "knowledge gained during training", "width": 2.5}, {"from": "irene", "title": "contextual proximity", "to": "l. shi", "width": 1.5}, {"from": "irene", "title": "contextual proximity", "to": "lack of accessible and well-maintained benchmarks", "width": 1.0}, {"from": "irene", "title": "contextual proximity", "to": "local", "width": 4.5}, {"from": "irene", "title": "contextual proximity", "to": "local directional relations", "width": 0.75}, {"from": "irene", "title": "contextual proximity", "to": "lstm", "width": 1.25}, {"from": "irene", "title": "contextual proximity", "to": "lstm limitations", "width": 0.75}, {"from": "irene", "title": "contextual proximity", "to": "m", "width": 0.5}, {"from": "irene", "title": "contextual proximity", "to": "m. bortoletto", "width": 1.5}, {"from": "irene", "title": "contextual proximity", "to": "mean prediction error", "width": 1.75}, {"from": "irene", "title": "contextual proximity", "to": "mlp policy", "width": 1.0}, {"from": "irene", "title": "IRENE achieves state-of-the-art results on five out of nine tasks,contextual proximity", "to": "model", "width": 6.25}, {"from": "irene", "title": "contextual proximity", "to": "models", "width": 2.0}, {"from": "irene", "title": "contextual proximity", "to": "multi-agent", "width": 4.75}, {"from": "irene", "title": "contextual proximity", "to": "multi-agent task", "width": 0.5}, {"from": "irene", "title": "contextual proximity", "to": "neural model", "width": 0.75}, {"from": "irene", "title": "contextual proximity", "to": "neural network", "width": 2.0}, {"from": "irene", "title": "contextual proximity", "to": "no barrier", "width": 1.0}, {"from": "irene", "title": "contextual proximity", "to": "novel model", "width": 2.5}, {"from": "irene", "title": "contextual proximity", "to": "objects", "width": 0.5}, {"from": "irene", "title": "contextual proximity", "to": "our model\u2019s expectations", "width": 0.5}, {"from": "irene", "title": "contextual proximity", "to": "output dimension two", "width": 0.5}, {"from": "irene", "title": "contextual proximity", "to": "p", "width": 1.0}, {"from": "irene", "title": "contextual proximity", "to": "path control", "width": 3.25}, {"from": "irene", "title": "contextual proximity", "to": "path control and time control sub-tasks", "width": 1.0}, {"from": "irene", "title": "contextual proximity", "to": "performance", "width": 5.75}, {"from": "irene", "title": "contextual proximity", "to": "performance scores", "width": 0.5}, {"from": "irene", "title": "contextual proximity", "to": "prediction net", "width": 1.0}, {"from": "irene", "title": "contextual proximity", "to": "preference", "width": 3.25}, {"from": "irene", "title": "contextual proximity", "to": "preference task", "width": 1.0}, {"from": "irene", "title": "contextual proximity", "to": "preferences", "width": 1.5}, {"from": "irene", "title": "contextual proximity", "to": "rational agents", "width": 2.75}, {"from": "irene", "title": "contextual proximity", "to": "reasoning performance", "width": 2.5}, {"from": "irene", "title": "contextual proximity", "to": "reasoning tasks", "width": 1.0}, {"from": "irene", "title": "contextual proximity", "to": "relational graphs", "width": 7.75}, {"from": "irene", "title": "contextual proximity", "to": "remote", "width": 4.5}, {"from": "irene", "title": "contextual proximity", "to": "remote relations", "width": 0.5}, {"from": "irene", "title": "contextual proximity", "to": "results", "width": 3.5}, {"from": "irene", "title": "contextual proximity", "to": "reward hacking", "width": 1.0}, {"from": "irene", "title": "contextual proximity", "to": "s", "width": 1.5}, {"from": "irene", "title": "contextual proximity", "to": "stack of six layers", "width": 0.5}, {"from": "irene", "title": "contextual proximity", "to": "state and context representations", "width": 2.5}, {"from": "irene", "title": "contextual proximity", "to": "state encoder", "width": 2.25}, {"from": "irene", "title": "contextual proximity", "to": "stojni\u00b4c et al. (2023)", "width": 0.5}, {"from": "irene", "title": "contextual proximity", "to": "sub-tasks", "width": 0.5}, {"from": "irene", "title": "contextual proximity", "to": "t-tests", "width": 1.25}, {"from": "irene", "title": "contextual proximity", "to": "table 2", "width": 1.25}, {"from": "irene", "title": "contextual proximity", "to": "tasks", "width": 0.75}, {"from": "irene", "title": "contextual proximity", "to": "the state of the art", "width": 0.5}, {"from": "irene", "title": "contextual proximity", "to": "theory of mind network", "width": 0.5}, {"from": "irene", "title": "contextual proximity", "to": "time control", "width": 3.25}, {"from": "irene", "title": "contextual proximity", "to": "training tasks", "width": 8.5}, {"from": "irene", "title": "contextual proximity", "to": "transformer", "width": 5.75}, {"from": "irene", "title": "contextual proximity", "to": "transformer encoder", "width": 1.0}, {"from": "irene", "title": "contextual proximity", "to": "trials", "width": 0.5}, {"from": "irene", "title": "contextual proximity", "to": "unseen evaluation tasks", "width": 2.5}, {"from": "irene", "title": "contextual proximity", "to": "video frames", "width": 0.5}, {"from": "irene", "title": "contextual proximity", "to": "video transformer", "width": 1.0}, {"from": "irene", "title": "contextual proximity", "to": "video-rnn", "width": 3.75}, {"from": "irene", "title": "contextual proximity", "to": "voe accuracy scores", "width": 0.5}, {"from": "irene", "title": "contextual proximity", "to": "vt", "width": 5.25}, {"from": "irene", "title": "contextual proximity", "to": "vt model", "width": 0.5}, {"from": "irrational agent", "title": "contextual proximity", "to": "rational agents", "width": 0.75}, {"from": "irrational agent", "title": "contextual proximity", "to": "training tasks", "width": 0.5}, {"from": "irrational agents", "title": "Rational agents are distinguished from irrational agents,contextual proximity", "to": "rational agents", "width": 1.75}, {"from": "irrational agents", "title": "contextual proximity", "to": "training tasks", "width": 0.5}, {"from": "jiang, m.", "title": "Authors of the paper \u0027Grid-to-Graph: Flexible Spatial Relational Inductive Biases for Reinforcement Learning\u0027", "to": "rockt\u00a8aschel, t.", "width": 1.0}, {"from": "jiang, z.", "title": "Authors of the paper \u0027Grid-to-Graph: Flexible Spatial Relational Inductive Biases for Reinforcement Learning\u0027", "to": "minervini, p.", "width": 1.0}, {"from": "jones, l.", "title": "Authors of the paper \u0027Attention Is All You Need\u0027", "to": "gomez, a. n.", "width": 1.0}, {"from": "json file", "title": "contextual proximity", "to": "single-object multiple-agent", "width": 0.5}, {"from": "kaiser, \u0142.", "title": "Authors of the paper \u0027Attention Is All You Need\u0027", "to": "polosukhin, i.", "width": 1.0}, {"from": "knowledge", "title": "contextual proximity", "to": "training tasks", "width": 0.75}, {"from": "knowledge gained during training", "title": "contextual proximity", "to": "performance", "width": 0.5}, {"from": "knowledge gained during training", "title": "contextual proximity", "to": "training tasks", "width": 0.5}, {"from": "learning in graph domains", "title": "The new model for learning in graph domains was presented at the IEEE International Joint Conference on Neural Networks.", "to": "model for learning", "width": 1.0}, {"from": "local", "title": "contextual proximity", "to": "model", "width": 0.75}, {"from": "local", "title": "contextual proximity", "to": "relational graphs", "width": 0.75}, {"from": "local", "title": "contextual proximity", "to": "remote", "width": 0.5}, {"from": "local", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "local directional relations", "title": "contextual proximity", "to": "local relations", "width": 0.75}, {"from": "local directional relations", "title": "contextual proximity", "to": "node embeddings", "width": 0.5}, {"from": "local directional relations", "title": "contextual proximity", "to": "node features", "width": 1.0}, {"from": "local directional relations", "title": "contextual proximity", "to": "nodes", "width": 0.75}, {"from": "local directional relations", "title": "contextual proximity", "to": "obstacles", "width": 0.75}, {"from": "local directional relations", "title": "contextual proximity", "to": "performance", "width": 2.25}, {"from": "local directional relations", "title": "contextual proximity", "to": "prediction net", "width": 0.5}, {"from": "local directional relations", "title": "contextual proximity", "to": "remote directional relations", "width": 1.0}, {"from": "local directional relations", "title": "contextual proximity", "to": "remote relations", "width": 1.5}, {"from": "local directional relations", "title": "contextual proximity", "to": "resulting model", "width": 0.75}, {"from": "local directional relations", "title": "contextual proximity", "to": "state encoder", "width": 0.75}, {"from": "local directional relations", "title": "contextual proximity", "to": "type, position, color, shape", "width": 0.5}, {"from": "local directional relations", "title": "contextual proximity", "to": "worse scores", "width": 0.75}, {"from": "local relations", "title": "using only local relations alone leaves many nodes isolated", "to": "nodes", "width": 1.0}, {"from": "local relations", "title": "contextual proximity", "to": "performance", "width": 0.75}, {"from": "local relations", "title": "contextual proximity", "to": "remote relations", "width": 0.5}, {"from": "logistic regression classifiers", "title": "contextual proximity", "to": "vt", "width": 0.5}, {"from": "lstm", "title": "contextual proximity", "to": "model", "width": 0.75}, {"from": "lstm", "title": "contextual proximity", "to": "obstacles", "width": 0.5}, {"from": "lstm", "title": "contextual proximity", "to": "performance", "width": 0.5}, {"from": "lstm", "title": "contextual proximity", "to": "preferences", "width": 0.5}, {"from": "lstm", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "lstm", "title": "contextual proximity", "to": "training tasks", "width": 0.5}, {"from": "lstm", "title": "contextual proximity", "to": "transformer", "width": 0.75}, {"from": "lstm", "title": "transformer encoder was replaced with an LSTM,contextual proximity", "to": "transformer encoder", "width": 1.5}, {"from": "lstm limitations", "title": "contextual proximity", "to": "rational agents", "width": 0.75}, {"from": "lstm limitations", "title": "contextual proximity", "to": "training tasks", "width": 0.5}, {"from": "lstm limitations", "title": "The non-sequential nature of the transformer together with its self-attention mechanism allow it to overcome LSTM limitations", "to": "transformer", "width": 1.0}, {"from": "m", "title": "contextual proximity", "to": "multi-agent", "width": 0.75}, {"from": "m", "title": "When training on M, performance on the Multi-Agent task is considerably worse", "to": "multi-agent task", "width": 1.0}, {"from": "m", "title": "contextual proximity", "to": "no barrier", "width": 0.5}, {"from": "m", "title": "contextual proximity", "to": "p", "width": 0.5}, {"from": "m", "title": "contextual proximity", "to": "preference task", "width": 0.5}, {"from": "m", "title": "contextual proximity", "to": "s", "width": 0.75}, {"from": "machine common-sense reasoning", "title": "Previous research in machine common-sense reasoning has focused on language processing and visual scene understanding.", "to": "research", "width": 1.0}, {"from": "machine theory of mind network", "title": "contextual proximity", "to": "test trials", "width": 0.5}, {"from": "machine theory of mind network", "title": "contextual proximity", "to": "voe paradigm", "width": 0.5}, {"from": "manuel mager", "title": "contextual proximity", "to": "m\u00a8uller", "width": 0.75}, {"from": "mean prediction error", "title": "contextual proximity", "to": "mlp policy", "width": 0.5}, {"from": "mean prediction error", "title": "The model was evaluated using the mean prediction error,contextual proximity", "to": "model", "width": 1.75}, {"from": "mean prediction error", "title": "contextual proximity", "to": "prediction net", "width": 0.5}, {"from": "mean prediction error", "title": "contextual proximity", "to": "results", "width": 1.0}, {"from": "mean prediction error", "title": "contextual proximity", "to": "state encoder", "width": 0.75}, {"from": "mean prediction error", "title": "contextual proximity", "to": "transformer encoder", "width": 0.5}, {"from": "message passing", "title": "contextual proximity", "to": "model", "width": 0.5}, {"from": "message passing", "title": "contextual proximity", "to": "position", "width": 0.5}, {"from": "message passing", "title": "contextual proximity", "to": "state embedding", "width": 0.5}, {"from": "message passing", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "message passing", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "meta-learning problem", "title": "contextual proximity", "to": "test trials", "width": 0.5}, {"from": "meta-learning problem", "title": "contextual proximity", "to": "voe paradigm", "width": 0.5}, {"from": "mlp", "title": "Feature fusion module encodes the node features using linear layers of hidden dimension 96", "to": "rnn", "width": 1.0}, {"from": "mlp", "title": "contextual proximity", "to": "state embedding", "width": 0.5}, {"from": "mlp policy", "title": "contextual proximity", "to": "output dimension two", "width": 0.5}, {"from": "mlp policy", "title": "contextual proximity", "to": "prediction net", "width": 1.0}, {"from": "mlp policy", "title": "contextual proximity", "to": "results", "width": 1.0}, {"from": "mlp policy", "title": "contextual proximity", "to": "stack of six layers", "width": 0.5}, {"from": "mlp policy", "title": "contextual proximity", "to": "state embedding", "width": 0.5}, {"from": "mlp policy", "title": "contextual proximity", "to": "state encoder", "width": 1.75}, {"from": "mlp policy", "title": "contextual proximity", "to": "transformer encoder", "width": 1.0}, {"from": "mlp policy", "title": "contextual proximity", "to": "video-rnn", "width": 0.5}, {"from": "mlp policy", "title": "contextual proximity", "to": "voe accuracy scores", "width": 0.5}, {"from": "mlp policy", "title": "contextual proximity", "to": "vt model", "width": 0.5}, {"from": "model", "title": "contextual proximity", "to": "neural network", "width": 0.75}, {"from": "model", "title": "contextual proximity", "to": "normalised between [-1, 1]", "width": 0.5}, {"from": "model", "title": "contextual proximity", "to": "normalised between [0, 1]", "width": 0.5}, {"from": "model", "title": "contextual proximity", "to": "objects", "width": 0.5}, {"from": "model", "title": "contextual proximity", "to": "one-hot vectors", "width": 0.5}, {"from": "model", "title": "contextual proximity", "to": "position", "width": 1.0}, {"from": "model", "title": "contextual proximity", "to": "rational agents", "width": 0.75}, {"from": "model", "title": "contextual proximity", "to": "reasoning tasks", "width": 1.5}, {"from": "model", "title": "contextual proximity", "to": "relational graphs", "width": 0.75}, {"from": "model", "title": "contextual proximity", "to": "remote", "width": 0.75}, {"from": "model", "title": "contextual proximity", "to": "results", "width": 1.5}, {"from": "model", "title": "contextual proximity", "to": "shape", "width": 0.5}, {"from": "model", "title": "contextual proximity", "to": "spatial relations", "width": 0.5}, {"from": "model", "title": "contextual proximity", "to": "states", "width": 0.5}, {"from": "model", "title": "contextual proximity", "to": "t-tests", "width": 0.75}, {"from": "model", "title": "contextual proximity", "to": "table 2", "width": 0.75}, {"from": "model", "title": "The theory of mind network is mentioned in relation to the most recent model based on a video transformer.,contextual proximity", "to": "theory of mind network", "width": 1.75}, {"from": "model", "title": "contextual proximity", "to": "training and evaluation on the baby intuitions benchmark", "width": 0.5}, {"from": "model", "title": "contextual proximity", "to": "training tasks", "width": 0.75}, {"from": "model", "title": "contextual proximity", "to": "transformer", "width": 2.5}, {"from": "model", "title": "contextual proximity", "to": "type", "width": 1.0}, {"from": "model", "title": "contextual proximity", "to": "video frames", "width": 0.75}, {"from": "model", "title": "contextual proximity", "to": "video transformer", "width": 1.5}, {"from": "model", "title": "contextual proximity", "to": "violation of expectation paradigm", "width": 0.5}, {"from": "model", "title": "contextual proximity", "to": "vt", "width": 3.0}, {"from": "model", "title": "contextual proximity", "to": "weights", "width": 0.5}, {"from": "modeling violation-of-expectation", "title": "Modeling violation-of-expectation is important in understanding physical reasoning across different event categories.", "to": "physical reasoning", "width": 1.0}, {"from": "models", "title": "contextual proximity", "to": "path control and time control sub-tasks", "width": 0.5}, {"from": "models", "title": "contextual proximity", "to": "reward hacking", "width": 0.5}, {"from": "models\u2019 ability to predict future actions", "title": "contextual proximity", "to": "test trials", "width": 0.5}, {"from": "models\u2019 ability to predict future actions", "title": "contextual proximity", "to": "voe paradigm", "width": 0.5}, {"from": "models\u2019 ability to reason about other agents", "title": "contextual proximity", "to": "research", "width": 0.5}, {"from": "most recent model (hein and diepold 2022, vt)", "title": "contextual proximity", "to": "test trials", "width": 0.5}, {"from": "most recent model (hein and diepold 2022, vt)", "title": "The most recent model (Hein and Diepold 2022, VT) is based on a video.", "to": "video", "width": 1.0}, {"from": "most recent model (hein and diepold 2022, vt)", "title": "contextual proximity", "to": "voe paradigm", "width": 0.5}, {"from": "mota, t.", "title": "Authors of the paper \u0027Commonsense Reasoning and Knowledge Acquisition to Guide Deep Learning on Robots\u0027", "to": "sridharan, m.", "width": 1.0}, {"from": "mse", "title": "contextual proximity", "to": "preference task", "width": 0.5}, {"from": "mse", "title": "contextual proximity", "to": "tasks", "width": 0.5}, {"from": "mse", "title": "In practice, the prediction error is quantified by the mean squared error (MSE) with respect to a ground truth (e.g. next frame or action).", "to": "prediction error", "width": 1.0}, {"from": "multi-agent", "title": "contextual proximity", "to": "multi-agent task", "width": 0.75}, {"from": "multi-agent", "title": "contextual proximity", "to": "no barrier", "width": 1.5}, {"from": "multi-agent", "title": "contextual proximity", "to": "p", "width": 1.5}, {"from": "multi-agent", "title": "contextual proximity", "to": "preference task", "width": 1.5}, {"from": "multi-agent", "title": "contextual proximity", "to": "relational graphs", "width": 0.5}, {"from": "multi-agent", "title": "contextual proximity", "to": "s", "width": 2.25}, {"from": "multi-agent task", "title": "contextual proximity", "to": "no barrier", "width": 0.5}, {"from": "multi-agent task", "title": "contextual proximity", "to": "p", "width": 0.5}, {"from": "multi-agent task", "title": "contextual proximity", "to": "preference task", "width": 0.5}, {"from": "multi-agent task", "title": "contextual proximity", "to": "s", "width": 0.75}, {"from": "natural language processing", "title": "The International Joint Conference on Natural Language Processing is related to the field of Natural Language Processing", "to": "international joint conference on natural language processing", "width": 1.0}, {"from": "navigation preference", "title": "contextual proximity", "to": "single-object multiple-agent", "width": 0.5}, {"from": "neural network", "title": "contextual proximity", "to": "reasoning tasks", "width": 0.5}, {"from": "neural network", "title": "contextual proximity", "to": "video transformer", "width": 0.5}, {"from": "neural network", "title": "contextual proximity", "to": "vt", "width": 1.0}, {"from": "no barrier", "title": "contextual proximity", "to": "p", "width": 1.0}, {"from": "no barrier", "title": "contextual proximity", "to": "preference task", "width": 1.0}, {"from": "no barrier", "title": "Training on S improved scores both on the No Barrier sub-task,contextual proximity", "to": "s", "width": 2.5}, {"from": "node embeddings", "title": "contextual proximity", "to": "node features", "width": 0.5}, {"from": "node embeddings", "title": "contextual proximity", "to": "remote directional relations", "width": 0.5}, {"from": "node embeddings", "title": "contextual proximity", "to": "state embedding", "width": 0.5}, {"from": "node features", "title": "contextual proximity", "to": "prediction net", "width": 0.5}, {"from": "node features", "title": "contextual proximity", "to": "remote directional relations", "width": 1.0}, {"from": "node features", "title": "contextual proximity", "to": "type, position, color, shape", "width": 0.5}, {"from": "nodes", "title": "contextual proximity", "to": "performance", "width": 0.75}, {"from": "nodes", "title": "contextual proximity", "to": "remote relations", "width": 0.5}, {"from": "normalised between [-1, 1]", "title": "Position is normalised between [-1, 1],contextual proximity", "to": "position", "width": 1.5}, {"from": "normalised between [-1, 1]", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "normalised between [-1, 1]", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "normalised between [0, 1]", "title": "contextual proximity", "to": "position", "width": 0.5}, {"from": "normalised between [0, 1]", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "normalised between [0, 1]", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "novel model", "title": "contextual proximity", "to": "performance", "width": 0.5}, {"from": "novel model", "title": "contextual proximity", "to": "training tasks", "width": 0.5}, {"from": "objects", "title": "contextual proximity", "to": "position", "width": 0.5}, {"from": "objects", "title": "contextual proximity", "to": "state encoder", "width": 1.0}, {"from": "objects", "title": "contextual proximity", "to": "sub-tasks", "width": 1.0}, {"from": "objects", "title": "contextual proximity", "to": "tasks", "width": 1.5}, {"from": "objects", "title": "contextual proximity", "to": "trajectories", "width": 0.5}, {"from": "objects", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "objects", "title": "contextual proximity", "to": "transformer context encoder", "width": 0.5}, {"from": "objects", "title": "contextual proximity", "to": "trials", "width": 1.0}, {"from": "objects", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "observer", "title": "contextual proximity", "to": "preference task", "width": 0.5}, {"from": "observer", "title": "contextual proximity", "to": "tasks", "width": 0.5}, {"from": "observer", "title": "According to the VoE paradigm, if the observer is more surprised by the unexpected outcome, this means that what they believed or predicted would happen is not in line with what actually occurred.", "to": "voe paradigm", "width": 1.0}, {"from": "observer model", "title": "contextual proximity", "to": "preference task", "width": 0.5}, {"from": "observer model", "title": "contextual proximity", "to": "tasks", "width": 0.5}, {"from": "observer model", "title": "contextual proximity", "to": "test trials", "width": 0.75}, {"from": "observer model", "title": "contextual proximity", "to": "voe paradigm", "width": 0.75}, {"from": "obstacles", "title": "contextual proximity", "to": "performance", "width": 1.0}, {"from": "obstacles", "title": "contextual proximity", "to": "remote relations", "width": 0.5}, {"from": "one-hot vectors", "title": "contextual proximity", "to": "position", "width": 0.5}, {"from": "one-hot vectors", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "one-hot vectors", "title": "Type is represented as one-hot vectors,contextual proximity", "to": "type", "width": 1.5}, {"from": "output dimension two", "title": "contextual proximity", "to": "prediction net", "width": 0.5}, {"from": "output dimension two", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "output dimension two", "title": "contextual proximity", "to": "state encoder", "width": 0.75}, {"from": "output dimension two", "title": "contextual proximity", "to": "transformer encoder", "width": 0.5}, {"from": "p", "title": "contextual proximity", "to": "preference task", "width": 1.0}, {"from": "p", "title": "contextual proximity", "to": "s", "width": 1.5}, {"from": "parmar, n.", "title": "Authors of the paper \u0027Attention Is All You Need\u0027", "to": "uszkoreit, j.", "width": 1.0}, {"from": "path control", "title": "contextual proximity", "to": "relational graphs", "width": 0.5}, {"from": "performance", "title": "contextual proximity", "to": "reasoning performance", "width": 0.5}, {"from": "performance", "title": "using only remote relations, performance is comparable to the original model except for Multi-Agent, which is at chance level,contextual proximity", "to": "remote relations", "width": 2.5}, {"from": "performance", "title": "contextual proximity", "to": "resulting model", "width": 0.75}, {"from": "performance", "title": "contextual proximity", "to": "state and context representations", "width": 0.5}, {"from": "performance", "title": "contextual proximity", "to": "state encoder", "width": 0.75}, {"from": "performance", "title": "The influence of the chosen training tasks on performance is analyzed,contextual proximity", "to": "training tasks", "width": 2.25}, {"from": "performance", "title": "contextual proximity", "to": "transformer", "width": 0.75}, {"from": "performance", "title": "contextual proximity", "to": "unseen evaluation tasks", "width": 0.5}, {"from": "performance", "title": "contextual proximity", "to": "worse scores", "width": 0.75}, {"from": "performance scores", "title": "Training only on a subset of tasks generally leads to a decrease in performance.,contextual proximity", "to": "training tasks", "width": 1.75}, {"from": "piloto et al.", "title": "contextual proximity", "to": "riochet et al.", "width": 1.0}, {"from": "piloto et al.", "title": "contextual proximity", "to": "shu et al.", "width": 1.0}, {"from": "piloto et al.", "title": "contextual proximity", "to": "violation of expectation paradigm", "width": 0.5}, {"from": "piloto et al.", "title": "contextual proximity", "to": "weihs et al.", "width": 0.5}, {"from": "piqa", "title": "The concept of \u0027PIQA\u0027 is related to \u0027Reasoning about Physical Commonsense in Natural Language\u0027 as discussed in the paper \u0027PIQA: Reasoning about Physical Commonsense in Natural Language\u0027 by Bisk, Y.; Zellers, R.; Bras, R. L.; Gao, J.; and Choi, Y.", "to": "physical commonsense in natural language", "width": 1.0}, {"from": "position", "title": "contextual proximity", "to": "shape", "width": 0.5}, {"from": "position", "title": "contextual proximity", "to": "spatial relations", "width": 0.5}, {"from": "position", "title": "contextual proximity", "to": "states", "width": 0.5}, {"from": "position", "title": "contextual proximity", "to": "training and evaluation on the baby intuitions benchmark", "width": 0.5}, {"from": "position", "title": "contextual proximity", "to": "transformer", "width": 1.0}, {"from": "position", "title": "contextual proximity", "to": "type", "width": 1.0}, {"from": "position", "title": "contextual proximity", "to": "violation of expectation paradigm", "width": 0.5}, {"from": "position", "title": "contextual proximity", "to": "weights", "width": 0.5}, {"from": "prediction error", "title": "contextual proximity", "to": "preference task", "width": 0.5}, {"from": "prediction error", "title": "contextual proximity", "to": "tasks", "width": 0.5}, {"from": "prediction net", "title": "contextual proximity", "to": "remote directional relations", "width": 0.5}, {"from": "prediction net", "title": "contextual proximity", "to": "results", "width": 1.0}, {"from": "prediction net", "title": "contextual proximity", "to": "stack of six layers", "width": 0.5}, {"from": "prediction net", "title": "contextual proximity", "to": "state encoder", "width": 1.5}, {"from": "prediction net", "title": "contextual proximity", "to": "transformer encoder", "width": 1.0}, {"from": "prediction net", "title": "contextual proximity", "to": "video-rnn", "width": 0.5}, {"from": "prediction net", "title": "contextual proximity", "to": "voe accuracy scores", "width": 0.5}, {"from": "prediction net", "title": "contextual proximity", "to": "vt model", "width": 0.5}, {"from": "preference", "title": "contextual proximity", "to": "relational graphs", "width": 0.5}, {"from": "preference task", "title": "contextual proximity", "to": "s", "width": 1.5}, {"from": "preference task", "title": "contextual proximity", "to": "tasks", "width": 1.0}, {"from": "preference task", "title": "contextual proximity", "to": "test trials", "width": 0.5}, {"from": "preference task", "title": "contextual proximity", "to": "training set", "width": 0.5}, {"from": "preference task", "title": "contextual proximity", "to": "unexpected trials", "width": 0.5}, {"from": "preference task", "title": "contextual proximity", "to": "voe paradigm", "width": 0.5}, {"from": "probing physics knowledge using tools from developmental psychology", "title": "Published in", "to": "arxiv preprint arxiv:1804.01128", "width": 1.0}, {"from": "rational agents", "title": "contextual proximity", "to": "reasoning tasks", "width": 0.5}, {"from": "rational agents", "title": "contextual proximity", "to": "training tasks", "width": 1.75}, {"from": "rational agents", "title": "contextual proximity", "to": "transformer", "width": 1.0}, {"from": "rational agents", "title": "contextual proximity", "to": "video transformer", "width": 0.5}, {"from": "rational agents", "title": "VT struggles with understanding the role of rational agents in contrast to irrational ones as mentioned in the context.,contextual proximity", "to": "vt", "width": 2.0}, {"from": "reasoning performance", "title": "contextual proximity", "to": "training tasks", "width": 0.5}, {"from": "reasoning tasks", "title": "contextual proximity", "to": "theory of mind network", "width": 0.5}, {"from": "reasoning tasks", "title": "contextual proximity", "to": "training tasks", "width": 0.5}, {"from": "reasoning tasks", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "reasoning tasks", "title": "contextual proximity", "to": "video frames", "width": 0.5}, {"from": "reasoning tasks", "title": "contextual proximity", "to": "video transformer", "width": 1.0}, {"from": "reasoning tasks", "title": "contextual proximity", "to": "vt", "width": 2.0}, {"from": "relational graphs", "title": "Remote takes as input relational graphs with only remote directional relations,contextual proximity", "to": "remote", "width": 1.75}, {"from": "relational graphs", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "relational graphs", "title": "contextual proximity", "to": "time control", "width": 0.5}, {"from": "relational graphs", "title": "contextual proximity", "to": "video-rnn", "width": 0.5}, {"from": "relational graphs", "title": "contextual proximity", "to": "vt", "width": 0.5}, {"from": "remote", "title": "contextual proximity", "to": "results", "width": 0.5}, {"from": "remote directional relations", "title": "contextual proximity", "to": "type, position, color, shape", "width": 0.5}, {"from": "remote relations", "title": "contextual proximity", "to": "resulting model", "width": 0.5}, {"from": "remote relations", "title": "contextual proximity", "to": "state encoder", "width": 0.5}, {"from": "remote relations", "title": "contextual proximity", "to": "worse scores", "width": 0.5}, {"from": "research", "title": "contextual proximity", "to": "visual scene understanding", "width": 0.5}, {"from": "resulting model", "title": "The resulting model achieved considerably worse scores in the Multi-Agent, Inaccessible Goal, Instrumental No Barrier and Blocking Barrier (sub-)tasks", "to": "worse scores", "width": 1.0}, {"from": "results", "title": "contextual proximity", "to": "stack of six layers", "width": 0.5}, {"from": "results", "title": "contextual proximity", "to": "state encoder", "width": 1.5}, {"from": "results", "title": "contextual proximity", "to": "t-tests", "width": 0.5}, {"from": "results", "title": "contextual proximity", "to": "table 2", "width": 0.5}, {"from": "results", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "results", "title": "contextual proximity", "to": "transformer encoder", "width": 1.0}, {"from": "results", "title": "contextual proximity", "to": "video-rnn", "width": 0.5}, {"from": "results", "title": "contextual proximity", "to": "voe accuracy scores", "width": 0.5}, {"from": "results", "title": "contextual proximity", "to": "vt model", "width": 0.5}, {"from": "riochet et al.", "title": "contextual proximity", "to": "shu et al.", "width": 1.0}, {"from": "riochet et al.", "title": "contextual proximity", "to": "violation of expectation paradigm", "width": 0.5}, {"from": "riochet et al.", "title": "contextual proximity", "to": "weihs et al.", "width": 0.5}, {"from": "rnn", "title": "contextual proximity", "to": "state embedding", "width": 0.5}, {"from": "shape", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "shape", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "shu et al.", "title": "contextual proximity", "to": "violation of expectation paradigm", "width": 0.5}, {"from": "shu et al.", "title": "contextual proximity", "to": "weihs et al.", "width": 0.5}, {"from": "simulation", "title": "The concept of \u0027Simulation\u0027 is related to \u0027Physical scene understanding\u0027 as discussed in the paper \u0027Simulation as an engine of physical scene understanding\u0027 by Battaglia, P. W.; Hamrick, J. B.; and Tenenbaum, J. B.", "to": "physical scene understanding", "width": 1.0}, {"from": "single-object multiple-agent", "title": "contextual proximity", "to": "training trials", "width": 0.5}, {"from": "social iqa: commonsense reasoning about social interactions", "title": "Presented at", "to": "proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language pro", "width": 1.0}, {"from": "solving the baby intuitions benchmark with a hierarchically bayesian theory of mind", "title": "The first concept is the title of a paper presented at the Robotics: Science and Systems Workshop on Social Intelligence in Humans and Robots.", "to": "robotics: science and systems workshop on social intelligence in humans and robots", "width": 1.0}, {"from": "spatial relations", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "spatial relations", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "stack of six layers", "title": "contextual proximity", "to": "state encoder", "width": 0.75}, {"from": "stack of six layers", "title": "The transformer encoder consists of a stack of six layers,contextual proximity", "to": "transformer encoder", "width": 1.5}, {"from": "state and context representations", "title": "contextual proximity", "to": "training tasks", "width": 0.5}, {"from": "state embedding", "title": "contextual proximity", "to": "state encoder", "width": 0.5}, {"from": "state embedding", "title": "contextual proximity", "to": "test frame graph", "width": 0.5}, {"from": "state encoder", "title": "contextual proximity", "to": "sub-tasks", "width": 1.0}, {"from": "state encoder", "title": "contextual proximity", "to": "tasks", "width": 1.5}, {"from": "state encoder", "title": "contextual proximity", "to": "trajectories", "width": 0.5}, {"from": "state encoder", "title": "contextual proximity", "to": "transformer context encoder", "width": 0.5}, {"from": "state encoder", "title": "contextual proximity", "to": "transformer encoder", "width": 1.75}, {"from": "state encoder", "title": "contextual proximity", "to": "trials", "width": 1.0}, {"from": "state encoder", "title": "contextual proximity", "to": "video-rnn", "width": 0.75}, {"from": "state encoder", "title": "contextual proximity", "to": "voe accuracy scores", "width": 0.75}, {"from": "state encoder", "title": "contextual proximity", "to": "vt model", "width": 0.75}, {"from": "state encoder", "title": "A test frame graph is encoded by the state encoder", "to": "test frame graph", "width": 1.0}, {"from": "state-action pairs", "title": "contextual proximity", "to": "vt", "width": 0.5}, {"from": "states", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "states", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "sub-tasks", "title": "contextual proximity", "to": "tasks", "width": 1.5}, {"from": "sub-tasks", "title": "contextual proximity", "to": "trajectories", "width": 0.5}, {"from": "sub-tasks", "title": "contextual proximity", "to": "transformer context encoder", "width": 0.5}, {"from": "sub-tasks", "title": "contextual proximity", "to": "trials", "width": 1.0}, {"from": "tasks", "title": "The Efficient Action and Instrumental Action tasks each have three sub-tasks, showing a hierarchical relationship within the tasks mentioned.", "to": "tasks", "width": 1.0}, {"from": "tasks", "title": "contextual proximity", "to": "test trials", "width": 0.5}, {"from": "tasks", "title": "The training set presents four tasks: Single-Object, No-Navigation Preference, Single-Object Multiple-Agent and Agent-Blocked Instrumental Action.,contextual proximity", "to": "training set", "width": 1.5}, {"from": "tasks", "title": "contextual proximity", "to": "trajectories", "width": 0.75}, {"from": "tasks", "title": "contextual proximity", "to": "transformer context encoder", "width": 0.75}, {"from": "tasks", "title": "contextual proximity", "to": "trials", "width": 1.5}, {"from": "tasks", "title": "contextual proximity", "to": "unexpected trials", "width": 0.5}, {"from": "tasks", "title": "contextual proximity", "to": "voe paradigm", "width": 0.5}, {"from": "teachers\u2019 extent of the use of particular task types in mathematics", "title": "The concept of \u0027Teachers\u2019 Extent of the Use of Particular Task Types in Mathematics\u0027 is discussed in the context of the paper \u0027Teachers\u2019 Extent of the Use of Particular Task Types in Mathematics and Choices behind That Use\u0027 by Clarke, D.; and Roche, A.", "to": "mathematics education research group of australasia", "width": 1.0}, {"from": "test trial", "title": "contextual proximity", "to": "vt", "width": 0.5}, {"from": "test trial", "title": "According to the VoE paradigm, a test trial can be consistent with the familiarisation examples (expected outcome) or inconsistent (unexpected outcome)", "to": "voe paradigm", "width": 1.0}, {"from": "test trials", "title": "Training and evaluation tasks differ in their number and setting, and test trials in the training set present only expected outcomes, requiring observer models to generalize to unseen situations by combining different pieces of knowledge gained during training.,contextual proximity", "to": "training and evaluation tasks", "width": 1.5}, {"from": "test trials", "title": "contextual proximity", "to": "video", "width": 0.5}, {"from": "test trials", "title": "The VoE paradigm is focused on pairing test trials that may not differ much in traditional error metrics but differ in terms of human reasoning.,contextual proximity", "to": "voe paradigm", "width": 2.25}, {"from": "theory of mind network", "title": "contextual proximity", "to": "video transformer", "width": 0.5}, {"from": "theory of mind network", "title": "contextual proximity", "to": "vt", "width": 1.0}, {"from": "training and evaluation on the baby intuitions benchmark", "title": "contextual proximity", "to": "transformer", "width": 0.5}, {"from": "training and evaluation on the baby intuitions benchmark", "title": "contextual proximity", "to": "type", "width": 0.5}, {"from": "training and evaluation tasks", "title": "contextual proximity", "to": "voe paradigm", "width": 0.5}, {"from": "training tasks", "title": "contextual proximity", "to": "transformer", "width": 1.5}, {"from": "training tasks", "title": "contextual proximity", "to": "unseen evaluation tasks", "width": 0.5}, {"from": "training tasks", "title": "contextual proximity", "to": "video transformer", "width": 0.5}, {"from": "training tasks", "title": "contextual proximity", "to": "vt", "width": 1.0}, {"from": "trajectories", "title": "Each trial in the BIB framework consists of trajectories, showing a sequential relationship between trials and trajectories.,contextual proximity", "to": "trials", "width": 1.5}, {"from": "transformer", "title": "contextual proximity", "to": "type", "width": 1.0}, {"from": "transformer", "title": "contextual proximity", "to": "video transformer", "width": 0.5}, {"from": "transformer", "title": "contextual proximity", "to": "violation of expectation paradigm", "width": 0.5}, {"from": "transformer", "title": "contextual proximity", "to": "vt", "width": 1.0}, {"from": "transformer", "title": "contextual proximity", "to": "weights", "width": 0.5}, {"from": "transformer context encoder", "title": "contextual proximity", "to": "trials", "width": 0.5}, {"from": "transformer encoder", "title": "The result is input into a transformer encoder and the output CTX\u0027 are taken as trial representations.", "to": "trial representations", "width": 1.0}, {"from": "transformer encoder", "title": "contextual proximity", "to": "video-rnn", "width": 0.5}, {"from": "transformer encoder", "title": "contextual proximity", "to": "voe accuracy scores", "width": 0.5}, {"from": "transformer encoder", "title": "contextual proximity", "to": "vt model", "width": 0.5}, {"from": "type", "title": "contextual proximity", "to": "violation of expectation paradigm", "width": 0.5}, {"from": "type", "title": "contextual proximity", "to": "weights", "width": 0.5}, {"from": "vaswani, a.", "title": "Authors of the paper \u0027Attention Is All You Need\u0027", "to": "shazeer, n.", "width": 1.0}, {"from": "video", "title": "contextual proximity", "to": "voe paradigm", "width": 0.5}, {"from": "video frames", "title": "contextual proximity", "to": "video transformer", "width": 0.5}, {"from": "video frames", "title": "VT encodes frames using a CNN and performs cross- and self-attention over frames,contextual proximity", "to": "vt", "width": 2.5}, {"from": "video transformer", "title": "contextual proximity", "to": "vt", "width": 2.0}, {"from": "video transformer network", "title": "Presented at", "to": "ieee/cvf international conference on computer vision", "width": 1.0}, {"from": "video transformer network", "title": "The first concept is the title of a paper presented in the Proceedings of the conference/workshop.", "to": "proceedings of the", "width": 1.0}, {"from": "voe paradigm", "title": "contextual proximity", "to": "vt", "width": 0.5}, {"from": "winogrande: an adversarial winograd schema challenge at scale", "title": "Published in", "to": "communications of the acm", "width": 1.0}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": true,
        "filter": [
            "physics"
        ]
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "forceAtlas2Based": {
            "avoidOverlap": 0,
            "centralGravity": 0.015,
            "damping": 0.4,
            "gravitationalConstant": -31,
            "springConstant": 0.08,
            "springLength": 100
        },
        "solver": "forceAtlas2Based",
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  
                  // if this network requires displaying the configure window,
                  // put it in its div
                  options.configure["container"] = document.getElementById("config");
                  

                  network = new vis.Network(container, data, options);

                  

                  
                    network.on("selectNode", neighbourhoodHighlight);
                  

                  


                  
                      network.on("stabilizationProgress", function(params) {
                          document.getElementById('loadingBar').removeAttribute("style");
                          var maxWidth = 496;
                          var minWidth = 20;
                          var widthFactor = params.iterations/params.total;
                          var width = Math.max(minWidth,maxWidth * widthFactor);
                          document.getElementById('bar').style.width = width + 'px';
                          document.getElementById('text').innerHTML = Math.round(widthFactor*100) + '%';
                      });
                      network.once("stabilizationIterationsDone", function() {
                          document.getElementById('text').innerHTML = '100%';
                          document.getElementById('bar').style.width = '496px';
                          document.getElementById('loadingBar').style.opacity = 0;
                          // really clean the dom element
                          setTimeout(function () {document.getElementById('loadingBar').style.display = 'none';}, 500);
                      });
                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>